\documentclass{article}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{authblk,url}
\usepackage{amssymb,amsmath,amsthm,twoopt,xargs,mathtools}
\usepackage{times,ifthen}
\usepackage{fancyhdr,xcolor}

\title{}
\date{}

%\author[$\wr$]{Mathis Chagneux}
\author[$\dag$]{XXX}
%\affil[$\wr$]{{\small LTCI, T\'el\'ecom Paris, Institut Polytechnique de Paris, Palaiseau.}}
%\affil[$\dag$]{{\small CMAP, \'Ecole Polytechnique, Institut Polytechnique de Paris, Palaiseau.}}

\lhead{}
\rhead{}

\DeclareUnicodeCharacter{2212}{-}
\usepackage{geometry}
\pagestyle{fancy}



%\newcommand{\E}{\mathbb{E}}
\def\dimX{d}
\def\dimY{m}
%\def\Xset{\mathsf{X}}
\def\Xset{\mathbb{R}^d}
\def\Yset{\mathsf{Y}}
\newcommand{\mk}{\kernel{G}}
\newcommand{\hk}{\kernel{Q}}
\newcommand{\md}[1]{g_{#1}}
\newcommand{\SmoothFigSize}{0.27}

\newcommand{\logllh}[1]{\ell_{#1}}
\newcommand{\llh}[1]{\mathsf{L}_{#1}}
\newcommand{\testf}{\mathsf{h}}

\newcommandx\filtderiv[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\eta_{#2}}
	{\eta_{#2}^\N}
}
\newcommand{\pred}[1]{\pi_{#1}}
\newcommand{\parvec}{\theta}
\newcommand{\parspace}{\Theta}
\newcommand{\tstatletter}{\kernel{T}}
\newcommand{\retrok}{\kernel{D}}
\newcommandx\tstat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\tau_{#2}^{#1}}
}
\newcommandx\tstathat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\widehat{\tau}_{#2}^{#1}}
}
\newcommand{\af}[1]{h_{#1}}
\newcommand{\deriv}{\nabla_{\parvec}}

\newcommand{\kernel}[1]{\mathbf{#1}}
\newcommand{\bmf}[1]{\set{F}(#1)}
\newcommand{\set}[1]{\mathsf{#1}}

\newcommandx{\bk}[2][1=]{
\ifthenelse{\equal{#1}{}}
{\overleftarrow{\kernel{Q}}_{#2}}
{\overleftarrow{\kernel{Q}}_{#2}^{#1}}
}

\newcommandx{\bkhat}[2][1=]{
\ifthenelse{\equal{#1}{}}
{\widehat{\kernel{Q}}_{#2}}
{\widehat{\kernel{Q}}_{#2}^{#1}}
}

\newcommand{\lk}{\kernel{L}}
\newcommand{\idop}{\operatorname{id}}
\newcommand{\hd}[1]{q_{#1}}
\newcommand{\hdhat}[1]{\widehat{q}_{#1}}


\newcommand{\addf}[1]{\termletter_{#1}}
\newcommand{\addfc}[1]{\underline{\termletter}_{#1}}
\newcommand{\adds}[1]{\af{#1}}
\newcommand{\term}[1]{\termletter_{#1}}
\newcommand{\termletter}{\tilde{h}}
\newcommand{\N}{N}
\newcommand{\partpred}[1]{\pi_{#1}^\N}
\newcommand{\tstattil}[2]{\tilde{\tau}_{#2}^{#1}}
\newcommandx{\K}[1][1=]{
\ifthenelse{\equal{#1}{}}{{\kletter}}{{\widetilde{\N}^{#1}}}}
\newcommand{\hkup}{\bar{\varepsilon}}
\newcommand{\bi}[3]{J_{#1}^{(#2, #3)}}
\newcommand{\bihat}[3]{\widehat{J}_{#1}^{(#2, #3)}}

\newcommand{\kletter}{\widetilde{\N}}

\def\sigmaX{\mathcal{X}}
\def\sigmaY{\mathcal{Y}}
\def\1{\mathds{1}}
\def\pE{\mathbb{E}}
\def\pP{\mathbb{P}}
\def\plim{\overset{\pP}{\longrightarrow}}
\def\dlim{\Longrightarrow}
\def\gauss{\mathcal{N}}


\newcommand{\esssup}[2][]
{\ifthenelse{\equal{#1}{}}{\left\| #2 \right\|_\infty}{\left\| #2 \right\|^2_{\infty}}}


\newcommand{\swght}[2]{\ensuremath{\omega_{#1}^{#2}}}

\newtheorem{assumptionA}{\textbf{A}\hspace{-3pt}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\newcommand{\iid}{i.i.d.}

\newcommand{\smwght}[3]{\tilde{\omega}_{#1|#2}^{#3}}
\newcommand{\smwghtfunc}[2]{\tilde{\omega}_{#1|#2}}

\newcommand{\smpart}[3]{\ensuremath{\tilde{\xi}_{#1|#2}^{#3}}}
\def\aux{{\scriptstyle{\mathrm{aux}}}}
\newcommand{\bdm}{\mathsf{TwoFilt}_{bdm}}
\newcommand{\fwt}{\mathsf{TwoFilt}_{fwt}}

\newcommand{\kiss}[3][]
{\ifthenelse{\equal{#1}{}}{r_{#2|#3}}
{\ifthenelse{\equal{#1}{fully}}{r^{\star}_{#2|#3}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2|#3}}{\mathrm{erreur}}}}}

\newcommand{\chunk}[4][]%
{\ifthenelse{\equal{#1}{}}{\ensuremath{{#2}_{#3:#4}}}{\ensuremath{#2^#1}_{#3:#4}}
}

\newcommand{\kissforward}[3][]
{\ifthenelse{\equal{#1}{}}{p_{#2}}
{\ifthenelse{\equal{#1}{fully}}{p^{\star}_{#2}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2}}{\mathrm{erreur}}}}}

\newcommand{\instrpostaux}[1]{\ensuremath{\upsilon_{#1}}}
\newcommandx\post[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\phi_{#2}}
	{\phi_{#2}^\N}
}

\newcommandx\posthat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\widehat{\phi}_{#2}}
	{\widehat{\phi}_{#2}^\N}
}

\newcommand{\adjfunc}[4][]
{\ifthenelse{\equal{#1}{}}{\ifthenelse{\equal{#4}{}}{\vartheta_{#2|#3}}{\vartheta_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{smooth}}{\ifthenelse{\equal{#4}{}}{\tilde{\vartheta}_{#2|#3}}{\tilde{\vartheta}_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{fully}}{\ifthenelse{\equal{#4}{}}{\vartheta^\star_{#2|#3}}{\vartheta^\star_{#2|#3}(#4)}}{\mathrm{erreur}}}}}

\newcommand{\XinitIS}[2][]
{\ifthenelse{\equal{#1}{}}{\ensuremath{\rho_{#2}}}{\ensuremath{\check{\rho}_{#2}}}}
\newcommand{\adjfuncforward}[1]{\vartheta_{#1}}
\newcommand{\rmd}{\ensuremath{\mathrm{d}}}
\newcommand{\eqdef}{\ensuremath{:=}}
\newcommand{\eqsp}{\;}
\newcommand{\ewght}[2]{\ensuremath{\omega_{#1}^{#2}}}
\newcommand{\ewghthat}[2]{\ensuremath{\widehat{\omega}_{#1}^{#2}}}
\newcommand{\epart}[2]{\ensuremath{\xi_{#1}^{#2}}}
\newcommand{\filt}[2][]%
{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\phi_{#2}}}{\ensuremath{\phi_{#1,#2}}}%
}
\newcommand{\Xinit}{\ensuremath{\chi}}
\newcommand{\sumwght}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\Omega_{#2}}}{\ensuremath{\Omega_{#2}^{(#1)}}}}
\newcommand{\sumwghthat}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\widehat{\Omega}_{#2}}}{\ensuremath{\widehat{\Omega}_{#2}^{(#1)}}}}

\newcounter{hypH}
\newenvironment{hypH}{\refstepcounter{hypH}\begin{itemize}
\item[{\bf H\arabic{hypH}}]}{\end{itemize}}

\newcommand{\marginalset}{\mathsf{U}}

\newcommand{\calF}[2]{\mathcal{F}_{#1}^{#2}}
\newcommand{\calG}[2]{\mathcal{G}_{#1}^{#2}}
\newcommand{\Uset}{\mathsf{U}}
\newcommand{\tcalF}[2]{\widetilde{\mathcal{F}}_{#1}^{#2}}
\newcommand{\tcalG}[2]{\widetilde{\mathcal{G}}_{#1}^{#2}}

\newcommand{\kernelmarg}{\mathbf{R}}

\newcommand{\pplim}{\overset{\pP}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\ddlim}{\overset{\mathcal{D}}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\aslim}{\overset{\pP\mathrm{-a.s.}}{ \underset{\N \to \infty}{\longrightarrow}}}

\newcommand{\qg}[1]{\ell_{#1}}
\newcommand{\hatqg}[1]{\mathsf{\ell}_{#1}}

\newcommand{\sfd}{\mathsf{d}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\e}{\text{e}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\frob}{:}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\vois}{\mathcal{V}}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
\section{Related works}

\section{Disentangling based on backward variational decomposition}
In the $\Delta$-SNICA framework, the independent components are generated by a Switching Linear Dynamical System (SLDS), see for instance \cite{}. For each independent component $i \in \{1,\dots,N\}$, consider the following SLDS over some latent vector $y_t^{(i)}$: 
\begin{align}
y_t^{(i)} = B_{u_t}^{(i)} y_{t-1}^{(i)} +  b_{u_t}^{(i)} + \varepsilon_{u_t}^{(i)} \eqsp, \label{eq:sld}
\end{align}
where $u_t:=u_t^{(i)}$ is a state of a first-order hidden Markov chain $(u_t^{(i)})_{t=1:T}$. Crucially, we assume that the independent components at each time-point are the first elements $y_{t,1}^{(i)}$ of $y_t^{(i)} = (y_{t,1}^{(i)}, \dots, y_{t,d}^{(i)})^T$, i.e. $s_t^{(i)} = y_{t,1}^{(i)}$. The rest of the elements in  $y_{t}^{(i)}$ are latent variables modelling hidden dynamics.
The great utility of using such a higher-dimensional latent variable is that this model allows us, for example, as a special case, to consider higher-order ARMA processes, thus modelling each $s_t^{(i)}$ as switching between ARMA processes of an order determined by the dimensionality of $y_t$. The generative model,  can be written as follows.
\begin{align}
    p_\theta(u_1^{(i)}) &= \prod_{k=1}^K (\pi_k^{(i)})^{\delta(u_1^{(i)} = k)} \\
    p_\theta(u_t^{(i)}\mid u_{t-1}^{(i)}) &= \prod_{k=1}^K\prod_{\ell=1}^K (A_{k\ell}^{(i)})^{ \delta(u_t^{(i)}=k)\delta(u_{t-1}^{(i)}=\ell)} \\
    p_\theta( y_1^{(i)}\mid u_1^{(i)}) &= \prod_{k=1}^K\mathcal{N}( y_1^{(i)} ; \bar{ b}_k^{(i)}, \bar{Q}_k^{-1^{(i)}})^{\delta(u_1^{(i)}=k)} \\
    p_\theta( y_t^{(i)}\mid  y_{t-1}^{(i)}, u_t^{(i)}) &= \prod_{k=1}^K \mathcal{N}( y_t^{(i)} ; B_k^{(i)}  y_{t-1}^{(i)} +  b_k^{(i)}, Q_k^{-1^{(i)}})^{\delta(u_t^{(i)}=k)} \\
    p_\theta( x_t\mid  s_t) &= \mathcal{N}( x_t ;  f_\theta(s_t),  R^{-1})
\end{align}
where the superscript $(i)$ again denotes that each independent component $i \in \{1,\dots, N\}$ follows its own switching linear dynamical system. Also, each independent component is part of a higher dimensional latent component $ y_t^{(i)} = (s_t^{(i)}, y_{t, 2}^{(i)},\dots, y_{t, d}^{(i)})$. The mixing function $f_\theta$ and other variables are defined as in the main text. The complete loglikelihood is defined as:
\begin{align}
	\log p_\theta( x_{1:T}^{(1:N)},  y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)}) &= \sum_{t=1}^T \log p_\theta( x_t \mid  s_t) + \sum_{i=1}^N\left(\log p_\theta(u_1^{(i)}) +\log p_\theta( y_1^{(i)} \mid u_1^{(i)}) \right. \nonumber \\
	&\left. \sum_{t=2}^T \log p_\theta(u_t^{(i)}\mid u_{t-1}^{(i)}) + \log p_\theta( y_t^{(i)} \mid  y_{t-1}^{(i)}, u_t^{(i)}) \right) \eqsp.
\end{align}
In variational approaches, instead of trying to design Monte Carlo estimators of such expectations (or equivalently of the conditional distribution of the states given the observations), the conditional law of $(y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})$ given $x_{1:T}^{(1:N)}$ is approximated by choosing a candidate in a parametric family $\{q_\phi\}_{\phi \in \Phi}$, where $\Phi$ is a parameter set. Parameters are then estimated by maximizing the  evidence lower bound (ELBO) defined as:
$$
\mathcal{L}(\parvec,\phi) = \pE_{q_\phi}\left[\log \frac{p_\parvec(x_{1:T}^{(1:N)},  y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}{q_\phi(y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}\right]\eqsp.
$$
In the original $\Delta$-SNICA approach, the marginal likelihood is intractable and hence we instead optimize the ELBO, under the assumption that the posterior factorizes as follows:
\begin{align}
q_\phi( y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)}) = \prod_{i=1}^N q_\phi( y_{1:T}^{(i)}) q_\phi(u_{1:T}^{(i)}). \label{eq:factorization}
\end{align}
Most works in the literature focus on such mean field approximations which means that the family $\{q_\phi\}_{\phi \in \Phi}$ can be written as a product of independent distributions. However, such an assumption fails to hold in standard hidden Markov models which may lead to poor result in the approximation of the posterior distribution of some states given the observations. In this paper, we propose another solution to provide a decomposition of   $q_\phi$ which accounts for the hidden Markov structure of $p_\parvec$.  This leads us to introduce a variational family where each $q_\phi$ is of the form,
\begin{equation}
\label{eq:backward:variational}
q_\phi( y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)}|x_{1:T}^{(1:N)}) =  \prod_{i=1}^N \left\{  q_\phi(y_T^i,u_T^i|x_{1:T}^{i})\prod_{t=0}^{T-1}q_\phi(y_t^i,u_t^i|y_{t+1}^i,u_{t+1}^i,x_{1:t}^{i})\right\}\eqsp.
\end{equation}
In the following, the dependence on $x_{1:T}$ is omitted for simplicity.
\subsection{Backward variational approximation}
Using \eqref{eq:backward:variational}, the ELBO is
\begin{align*}
\mathcal{L}(\parvec,\phi) &= \pE_{q_\phi}\left[\log \frac{p_\parvec(x_{1:T}^{(1:N)},  y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}{q_\phi(y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}\right]\\
&= \pE_{q_\phi}\bigg[\sum_{t=1}^T\log p_\theta( x_t\mid s_t^{(1:N)})\bigg]  + \pE_{q_\phi}\left[\log \frac{p_\parvec(y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}{q_\phi(y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}\right]\\
&= \pE_{q_\phi}\bigg[\sum_{t=1}^T\log p_\theta( x_t\mid s_t^{(1:N)})\bigg]  + \sum_{i=1}^n\left\{\pE_{q_\phi}\left[\log p_\parvec(y_{1}^{(i)}, u_{1}^{(i)})\right] + \sum_{t=1}^{T-1}\pE_{q_\phi}\left[\log p_\parvec(y_{t+1}^{(i)}, u_{t+1}^{(i)}|y_{t}^{(i)}, u_{t}^{(i)})\right]\right\}\\
& \hspace{2cm} -  \sum_{i=1}^n\left\{\pE_{q_\phi}\left[\log q_\phi(y_{T}^{(i)}, u_{T}^{(i)})\right] - \sum_{t=1}^{T-1}\pE_{q_\phi}\left[\log q_\phi(y_{t}^{(i)}, u_{t}^{(i)}|y_{t+1}^{(i)}, u_{t+1}^{(i)})\right]\right\}.
\end{align*}
Here we do not use the online computation of the expectations and we do not compare  directly both backward kernels. Depending on the design of $q_\phi$, it is still maybe possible to obtain a direct computation with a closed-form formula for the ELBO...

\subsection{Online decomposition (maybe for later)}
The ELBO can thus be written as:
\begin{equation}
\label{eq:ELBO:decomposition}
\mathcal{L}(\parvec,\phi)  = \E_{q_\phi}\bigg[\log \frac{p_\theta( x_{1:T},  y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}{q_\phi( y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})} \bigg] = \E_{q_\phi}\left[V_T^{\theta,\phi}( u^{(1:N)}_T,y^{(1:N)}_T)\right]\nonumber
\end{equation}
where
\begin{align}
V_T^{\theta,\phi}( u^{(1:N)}_T,y^{(1:N)}_T) &= \E_{q_\phi}\left[\log \frac{p_\theta( x_{1:T},  y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}{q_\phi( y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}\middle| u^{(1:N)}_T,y^{(1:N)}_T \right]\\
&= \ldots
\end{align}
%\begin{align}
%\mathcal{L}(\parvec,\phi)  &= \E_{q_\phi}\bigg[\log \frac{p_\theta( x_{1:T},  y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})}{q_\phi( y_{1:T}^{(1:N)}, u_{1:T}^{(1:N)})} \bigg] \nonumber\\
%    &= \E_{q_\phi}\bigg[\sum_{t=1}^T\log p( x_t\mid s_t^{(1)}, \ldots,  s_t^{(N)}) + \sum_{i=1}^N\log\frac{p_\theta( y_{1:T}^{(i)}\mid u_{1:T}^{(i)}) p_\theta(u_{1:T}^{(i)})}{q_\phi( y_{1:T}^{(i)},u_{1:T}^{(i)})} \bigg] \nonumber\\
%    &= \E_{q_\phi}\bigg[\sum_{t=1}^T\log p_\theta( x_t\mid s_t^{(1)}, \ldots,  s_t^{(N)})\bigg] + \sum_{i=1}^N\Bigg(-\mathrm{KL}\bigg[q(u_{1:T}^{(i)})\bigg| p(u_{1:T}^{(i)})\bigg] + \mathrm{H}\bigg[q( y_{1:T}^{(i)})\bigg] \nonumber\\    
%    &\qquad+ \E_{q}\bigg[\log p( y_{1:T}^{(i)}\mid u_{1:T}^{(i)})\bigg]\Bigg) \nonumber\\
%    &= \E_{q_\phi}\bigg[\sum_{t=1}^T\log p_\theta( x_t\mid s_t^{(1)}, \ldots,  s_t^{(N)})\bigg] + \sum_{i=1}^N\Bigg(- \mathrm{KL}\bigg[q(u_{1:T}^{(i)})\bigg| p(u_{1:T}^{(i)})\bigg] + \mathrm{H}\bigg[q( s_{1:T}^{(i)})\bigg] \nonumber\\
%    &\qquad+ \E_{q}\bigg[\log p( s_1^{(i)}\mid u_1^{(i)}) \bigg] + \sum_{t=2}^T \E_{q}\bigg[\log p( s_t^{(i)}\mid s_{t-1}^{(i)}, u_t^{(i)}) \bigg]\Bigg) \label{eq:ELBO}
%\end{align}




\end{document}
