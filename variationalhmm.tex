\documentclass{article}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{authblk,url}
\usepackage{amssymb,amsmath,amsthm,twoopt,xargs,mathtools,dsfont}
\usepackage{times,ifthen}
\usepackage{fancyhdr,xcolor}
\usepackage{hyperref}

\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\title{Variational inference based on backward decomposition for hidden Markov models}
\date{}

%\author[$\wr$]{Mathis Chagneux}
\author[$\dag$]{XXX}
%\affil[$\wr$]{{\small LTCI, T\'el\'ecom Paris, Institut Polytechnique de Paris, Palaiseau.}}

\affil[$\dag$]{{\small }}

\lhead{}
\rhead{}

\DeclareUnicodeCharacter{2212}{-}
\usepackage{geometry}
\pagestyle{fancy}


\newcommand{\retrokmodnorm}{\bar{\boldsymbol{\mathcal{L}}}^{\precpar}}
\newcommand{\xarb}{x^\ast}
\newcommand{\precparsp}{\mathcal{E}}
\newcommand{\fk}[2]{\mathbf{F}_{#1 | #2}}
\newcommand{\probmeas}[1]{\mathsf{M}_1(#1)}
\newcommand{\Xfd}{\mathcal{X}}
\newcommand{\uksymbol}{\ell}
\newcommandx{\bkmod}[2][1=]{ 
\ifthenelse{\equal{#1}{}}
{\kernel{B}_{#2}^\precpar}
{\kernel{B}_{#2}^\precpar}
}

\newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
\newcommand{\ukmod}[1]{\mathbf{L}_{#1}^\precpar}
\newcommand{\shiftbwd}{\cev{\shiftsymbol}^{\precpar}}
\newcommand{\shiftfwd}{\vec{\shiftsymbol}^{\, \precpar}}
\newcommand{\shiftsymbol}{\Phi}
\newcommand{\precpar}{\varphi}
\newcommand{\intvect}[2]{\{ #1, #2 \}}
\newcommandx\tstatmod[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter^{\precpar}_{#2}}
	{\tau^{\precpar}_{#2}^{#1}}
}
\newcommand{\noshift}{\shiftsymbol^{\precpar}}
\newcommand{\udlow}{\sigma_-}
\newcommand{\udup}{\sigma_+}
\newcommand{\retrok}{\boldsymbol{\mathcal{L}}}
%\newcommand{\intvect}[2]{\llbracket #1, #2 \rrbracket}
\newcommandx{\bkw}[2][1=]{ 
\ifthenelse{\equal{#1}{}}
{\kernel{B}_{#2}}
{\kernel{B}_{#2}}
}
\newcommand{\retroknorm}{\bar{\boldsymbol{\mathcal{L}}}}
\newcommand{\ud}[1]{\uksymbol_{#1}} 
\newcommand{\nset}{\mathbb{N}}
\newcommand{\nsetpos}{\mathbb{N}_{> 0}}
\newcommand{\1}{\mathbbm{1}} 
\newcommandx{\postmod}[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\phi_{#2}^\varphi}
	{\phi_{#2}^\N}
}
\newcommand{\retrokmod}{\boldsymbol{\mathcal{L}}^\precpar}
\newcommand{\uk}[1]{\mathbf{L}_{#1}}
\newcommand{\tensprod}{\otimes}
\def\dimX{d}
\def\dimY{m}
%\def\Xset{\mathsf{X}}
\newcommand{\Xset}{\mathsf{X}}
\def\Yset{\mathsf{Y}}
\newcommand{\mk}{\kernel{G}}
\newcommand{\hk}{\kernel{Q}}
\newcommand{\md}[1]{g_{#1}}
\newcommand{\SmoothFigSize}{0.27}

\newcommand{\logllh}[1]{\ell_{#1}}
\newcommand{\llh}[1]{\mathsf{L}_{#1}}
\newcommand{\testf}{\mathsf{h}}

\newcommandx\filtderiv[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\eta_{#2}}
	{\eta_{#2}^\N}
}
\newcommand{\pred}[1]{\pi_{#1}}
\newcommand{\parvec}{\theta}
\newcommand{\parspace}{\Theta}
\newcommand{\tstatletter}{\kernel{T}}
%\newcommand{\retrok}{\kernel{D}}
\newcommandx\tstat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\tau_{#2}^{#1}}
}
\newcommandx\tstathat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\widehat{\tau}_{#2}^{#1}}
}
\newcommand{\af}[1]{h_{#1}}
\newcommand{\deriv}{\nabla_{\parvec}}

\newcommand{\kernel}[1]{\mathbf{#1}}
\newcommand{\bmf}[1]{\set{F}(#1)}
\newcommand{\set}[1]{\mathsf{#1}}

\newcommandx{\bk}[2][1=]{
\ifthenelse{\equal{#1}{}}
{\overleftarrow{\kernel{Q}}_{#2}}
{\overleftarrow{\kernel{Q}}_{#2}^{#1}}
}

\newcommandx{\bkhat}[2][1=]{
\ifthenelse{\equal{#1}{}}
{\widehat{\kernel{Q}}_{#2}}
{\widehat{\kernel{Q}}_{#2}^{#1}}
}

\newcommand{\lk}{\kernel{L}}
\newcommand{\idop}{\operatorname{id}}
\newcommand{\hd}[1]{q_{#1}}
\newcommand{\hdhat}[1]{\widehat{q}_{#1}}


\newcommandx{\addf}[2][1=]{
\ifthenelse{\equal{#1}{}}{\termletter_{#2}}{\bar{h}_{#2 | #1}}
}
\newcommand{\addfc}[1]{\underline{\termletter}_{#1}}
\newcommand{\adds}[1]{\af{#1}}
\newcommand{\term}[1]{\termletter_{#1}}
\newcommand{\termletter}{\tilde{h}}
\newcommand{\N}{N}
\newcommand{\partpred}[1]{\pi_{#1}^\N}
\newcommand{\tstattil}[2]{\tilde{\tau}_{#2}^{#1}}
\newcommandx{\K}[1][1=]{
\ifthenelse{\equal{#1}{}}{{\kletter}}{{\widetilde{\N}^{#1}}}}
\newcommand{\hkup}{\bar{\varepsilon}}
\newcommand{\bi}[3]{J_{#1}^{(#2, #3)}}
\newcommand{\bihat}[3]{\widehat{J}_{#1}^{(#2, #3)}}

\newcommand{\kletter}{\widetilde{\N}}

\def\sigmaX{\mathcal{X}}
\def\sigmaY{\mathcal{Y}}
\def\1{\mathds{1}}
\def\pE{\mathbb{E}}
\def\pP{\mathbb{P}}
\def\plim{\overset{\pP}{\longrightarrow}}
\def\dlim{\Longrightarrow}
\def\gauss{\mathcal{N}}


\newcommand{\esssup}[2][]
{\ifthenelse{\equal{#1}{}}{\left\| #2 \right\|_\infty}{\left\| #2 \right\|^2_{\infty}}}


\newcommand{\swght}[2]{\ensuremath{\omega_{#1}^{#2}}}

\newtheorem{assumptionA}{\textbf{A}\hspace{-3pt}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\newcommand{\iid}{i.i.d.}

\newcommand{\smwght}[3]{\tilde{\omega}_{#1|#2}^{#3}}
\newcommand{\smwghtfunc}[2]{\tilde{\omega}_{#1|#2}}

\newcommand{\smpart}[3]{\ensuremath{\tilde{\xi}_{#1|#2}^{#3}}}
\def\aux{{\scriptstyle{\mathrm{aux}}}}
\newcommand{\bdm}{\mathsf{TwoFilt}_{bdm}}
\newcommand{\fwt}{\mathsf{TwoFilt}_{fwt}}

\newcommand{\kiss}[3][]
{\ifthenelse{\equal{#1}{}}{r_{#2|#3}}
{\ifthenelse{\equal{#1}{fully}}{r^{\star}_{#2|#3}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2|#3}}{\mathrm{erreur}}}}}

\newcommand{\chunk}[4][]%
{\ifthenelse{\equal{#1}{}}{\ensuremath{{#2}_{#3:#4}}}{\ensuremath{#2^#1}_{#3:#4}}
}

\newcommand{\kissforward}[3][]
{\ifthenelse{\equal{#1}{}}{p_{#2}}
{\ifthenelse{\equal{#1}{fully}}{p^{\star}_{#2}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2}}{\mathrm{erreur}}}}}

\newcommand{\instrpostaux}[1]{\ensuremath{\upsilon_{#1}}}
\newcommandx\post[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\phi_{#2}}
	{\phi_{#2}^\N}
}

\newcommandx\posthat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\widehat{\phi}_{#2}}
	{\widehat{\phi}_{#2}^\N}
}

\newcommand{\adjfunc}[4][]
{\ifthenelse{\equal{#1}{}}{\ifthenelse{\equal{#4}{}}{\vartheta_{#2|#3}}{\vartheta_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{smooth}}{\ifthenelse{\equal{#4}{}}{\tilde{\vartheta}_{#2|#3}}{\tilde{\vartheta}_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{fully}}{\ifthenelse{\equal{#4}{}}{\vartheta^\star_{#2|#3}}{\vartheta^\star_{#2|#3}(#4)}}{\mathrm{erreur}}}}}

\newcommand{\XinitIS}[2][]
{\ifthenelse{\equal{#1}{}}{\ensuremath{\rho_{#2}}}{\ensuremath{\check{\rho}_{#2}}}}
\newcommand{\adjfuncforward}[1]{\vartheta_{#1}}
\newcommand{\rmd}{\ensuremath{\mathrm{d}}}
\newcommand{\eqdef}{\ensuremath{:=}}
\newcommand{\eqsp}{\;}
\newcommand{\ewght}[2]{\ensuremath{\omega_{#1}^{#2}}}
\newcommand{\ewghthat}[2]{\ensuremath{\widehat{\omega}_{#1}^{#2}}}
\newcommand{\epart}[2]{\ensuremath{\xi_{#1}^{#2}}}
\newcommand{\filt}[2][]%
{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\phi_{#2}}}{\ensuremath{\phi_{#1,#2}}}%
}
\newcommand{\Xinit}{\ensuremath{\chi}}
\newcommand{\sumwght}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\Omega_{#2}}}{\ensuremath{\Omega_{#2}^{(#1)}}}}
\newcommand{\sumwghthat}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\widehat{\Omega}_{#2}}}{\ensuremath{\widehat{\Omega}_{#2}^{(#1)}}}}

\newcounter{hypH}
\newenvironment{hypH}{\refstepcounter{hypH}\begin{itemize}
\item[{\bf H\arabic{hypH}}]}{\end{itemize}}

\newcommand{\marginalset}{\mathsf{U}}

\newcommand{\calF}[2]{\mathcal{F}_{#1}^{#2}}
\newcommand{\calG}[2]{\mathcal{G}_{#1}^{#2}}
\newcommand{\Uset}{\mathsf{U}}
\newcommand{\tcalF}[2]{\widetilde{\mathcal{F}}_{#1}^{#2}}
\newcommand{\tcalG}[2]{\widetilde{\mathcal{G}}_{#1}^{#2}}

\newcommand{\kernelmarg}{\mathbf{R}}

\newcommand{\pplim}{\overset{\pP}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\ddlim}{\overset{\mathcal{D}}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\aslim}{\overset{\pP\mathrm{-a.s.}}{ \underset{\N \to \infty}{\longrightarrow}}}

\newcommand{\qg}[1]{\ell_{#1}}
\newcommand{\hatqg}[1]{\mathsf{\ell}_{#1}}

\newcommand{\sfd}{\mathsf{d}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\e}{\text{e}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\frob}{:}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\vois}{\mathcal{V}}




\newcounter{example}[section]
\newenvironment{example}[1][]{\refstepcounter{example}\par\medskip
   \noindent \textbf{Example~\theexample:} \textit{#1} \text \rmfamily}{\medskip}

\newcommand{\expect}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\gaussian}[2]{\mathcal{N}\left( #1, #2 \right)}
\newcommand{\backward}[1]{\overleftarrow{#1}}
\newcommand{\vbackward}[1]{q_{1:#1}(z_{#1}|z_{#1 + 1})}
\newcommand{\vfilt}[1]{q_{1:#1}(z_{#1})}
\newcommand{\vbackwardparam}[2]{\backward{#1}_{1:#2}^\phi}
\newcommand{\vbackwardmean}[1]{\vbackwardparam{\mu}{#1}}
\newcommand{\vbackwardcov}[1]{\vbackwardparam{\Sigma}{#1}}
\newcommand{\vfiltparam}[2]{#1_{1:#2}^\phi}
\newcommand{\vfiltmean}[1]{\vfiltparam{\mu}{#1}}
\newcommand{\vfiltcov}[1]{\vfiltparam{\Sigma}{#1}}
\newcommand{\inv}[1]{{#1}^{-1}}
\newcommand{\stateprec}{\inv{Q^\theta}}
\newcommand{\vstateprec}{\inv{Q^\phi}}
\newcommand{\quadform}[2]{#1^T #2 #1}


\DeclareMathOperator*{\argmax}{arg\,max} 

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Theoretical perspectives}

Observations $(Y_{t})_{1\leq t \leq T}$, latent variables $(X_{t})_{1\leq t \leq T}$.\\
True (unknown) density distribution $p^{\star}$.\\
Decoding (= model) $p_{\theta}$: joint density of $(X_{t}, Y_{t})_{1\leq t \leq T}$. Write $\theta$ even if non parametric.\\
Encoding $q_{\varphi}$: joint density of $(X_{t})_{1\leq t \leq T}$ given $(Y_{t})_{1\leq t \leq T}$, that is a joint density which is random, and $\sigma(Y_{1},\ldots,Y_{T})$-measurable. May also be non parametric.  The estimators $\widehat{\theta}$ and $\widehat{\phi}$ maximize the ELBO:
$$
{\cal L}(\theta,\phi; T)= \E_{q_{\varphi}} \left[ \log \left( \frac{p_{\theta}(X_{1},\ldots,X_{T},Y_{1},\ldots,Y_{T})}{q_{\varphi}(X_{1},\ldots,X_{T})}\right)\middle\vert Y_{1:T}\right].
$$

\subsection{Independent variables}

We consider that the true distribution, decoder and encoder distributions are defined by:
\begin{align*}
p^{\star}(x_{1},\ldots,x_{T},y_{1},\ldots,y_{T})&=\prod_{t=1}^{T}p^{\star}(x_{t},y_{t})\,,\\
p_{\theta}(x_{1},\ldots,x_{T},y_{1},\ldots,y_{T})&=\prod_{t=1}^{T}p_{\theta}(x_{t},y_{t})\,,\\
q_{\varphi}(x_{1},\ldots,x_{T})&=\prod_{t=1}^{T}q_{\varphi,t}(x_{t})\,.
\end{align*}
Therefore,
$$
\frac{1}{T}{\cal L}(\theta,\phi; T) = \frac{1}{T}\sum_{t=1}^T\log p_\theta(Y_t) + \frac{1}{T}\sum_{t=1}^T\E_{q_{\varphi,t}}\left[\log \frac{p_\theta(X_t|Y_t)}{q_{\varphi,t}(X_t)}\right]\,.
$$
When $q_{\varphi,t}(x_{t})=q_{\varphi}(x_{t}|y_t)$ does not depend on $t$, then ${\cal L}(\theta,\phi; T)/T$ has a a.s. limit:
$$
\lim_{T\mapsto \infty}\frac{1}{T}{\cal L}(\theta,\phi; T) = M_{\infty}(\theta,\phi)=\int p^{\star}(y)\log p_{\theta}(y) dy - \int \log \left(\frac{q_{\varphi}(x\vert y)}{p_{\theta}(x \vert y) } \right)q_{\varphi}(x\vert y)\,.
p^{\star}(y)dxdy
$$
The maximizers of this limit are investigated in \cite{koehler2021variational} in scenarii used in VAE.  The analysis of $(\widehat{\theta},\widehat{\phi})$ as $M-estimators$ with oracle inequalities and applications is in \cite{tang21a}.
If we assume that the $q_{\varphi,t}$ indeed depend on $t$, then if we write for each $\theta$,
$$
\widehat{q}_{\theta,t}=q_{\widehat{\varphi}_{t}(\theta),t}= \underset{{q \text{ of form }q_{\varphi},t}}{\mathrm{argmin}} \int  \left[\log \left(\frac{q(x\vert Y_{t})}{p_{\theta}(x \vert Y_{t}) } \right)\right]q(x\vert Y_{t})dx,
$$
then $\widehat{\theta}$ maximizes
$$
\ell_{T}(\theta)-\sum_{t=1}^{T} D\left(\widehat{q}_{t,\theta} \vert p_{\theta}(\cdot \vert Y_{t})\right),
$$
where $\ell_{T}$ is the log-likelihood, thus something like a penalized likelihood. All of this is  interesting only if such VAE with so many parameters for the encoding   is used for computational convenience or so. If this is the case, then write a theory to investigate questions such as:
\begin{itemize}
\item If the encoder is a good approximating family then the penalization is small; can it be build so that the penalty is $o(T)$ ? 
\item
And then possible to analyse that for large $T$ (or asymptotically) if $p^{\star}$ is in the decoding class,  for large $t$ is close to $p_{\widehat{\theta}}(\cdot \vert Y)$ for $\widehat{\theta}$ having good asymptotic properties ?
\end{itemize}

 
\subsection{State space decoding}
We consider that 

$$p_{\theta}(x_{1},\ldots,x_{T},y_{1},\ldots,y_{T})=p_{\theta}(x_{1})g_{\theta}(y_{1}\vert x_{1})\prod_{t=2}^{T}p_{\theta}(x_{t-1},x_{t})g_{\theta}(y_{t} \vert x_{t}).
$$ 


\subsubsection{Mean-field encoding}
The case where
$$q_{\varphi}(x_{1},\ldots,x_{T})=\prod_{t=1}^{T}q_{\varphi}(x_{t}).$$
Then ${\cal L}(\theta,\phi; T)/T$ has a limit (as soon as under $p^{\star}$ the $(Y_{t})_{t\geq 1}$ satisfy some dependence properties allowing laws of large numbers):
$$
\lim_{T\mapsto \infty}\frac{1}{T}{\cal L}(\theta,\phi; T) = M_{\infty}(\theta,\phi)=\int p^{\star}(y) q_{\varphi}(x_{1}\vert y)q_{\varphi}(x_{2}\vert y)
 \left[\log \left(\frac{p_{\theta}(x_{1},x_{2} ) g_{\theta}(x_{2}\vert y)}{q_{\varphi}(x_{2}\vert y)} \right)\right]
dx_{1}dx_{2}dy
$$
What can be said ?
\begin{itemize}
\item
Write the $M$-estimator  oracle inequality theory. Or at least the asymptotic theory.
\item
Understand the bias (that is: the maximizers of $M_{\infty}$) in interesting examples.
\end{itemize}

\subsubsection{HMM encoding/ Forward / Backward encoding}

Look at Mathis computations. Understand what are the good ways to write encoding so that the $M$-estimation theory can be written down. In \cite{campbell2021online} forward encoding, also with $\varphi$'s varying with $t$ in the examples ....

\clearpage
\newpage

\section{Exact backward decomposition for ICA}

%We now want to frame the multi-object tracking challenge as the downstream task of the more general problem of "disentenglement". In general terms and departing from the specific field of computer vision, we view our observations as random variables that arise from a generating process involving hidden/latent random variables. In disentenglement research, these latent variables are assumed to decompose into independent components (which, in the deep learning community, would be refered to as disentangled "representations"). For multi-object tracking, our goal is to infer latent variables whose independent components are "object-centric" representations, ie. each of them separetely describes an individual object. Many works exist that constrain VAEs to provide disentengled latent variables (e.g. via a special form of regularization, see \cite{betavae}) but \cite{unidentifiability} showed that most of them are actually trying to solve an ill-posed problem which is essentially unidentifiable. On the contrary, the Independant Component Analysis (ICA) litterature provides many identifiability results. In brief, in the nonlinear case, identifiability is only guaranteed if signals exhibit some structure (e.g. temporal dependencies), or if the inference is conditioned on an additionally observed process, which was shown in \cite{iVAE} for the specific case of VAEs. More recently, \cite{hermanni2021} proved identifiability when hidden states follow linear but flexible dynamics via regime switching: this is currently the most general setup that is still backed by identifiability proofs. However, in practice, inference is made with strong assumptions on the variational family. For sequential data, this is in contrast to the most recent works on VAEs, like \cite{campbell2021online}, which introduce flexible variational families that factorize according to the original model. We therefore attempt to merge ideas from these works.


In all that follows, we use $\mathsf{X}$ to refer to a set with elements $\mathsf{x} \in \mathsf{X}$ and $\mathsf{X}^p$ the associated product space.  For any set $\mathsf{X}$ we always assume that we can build a $\sigma$-field $\sigma(\mathsf{X})$ and we define $\mathsf{M}_1(\mathsf{X})$ the space of probability measures on $(\mathsf{X},\sigma(\mathsf{X}))$. Let $(\mathsf{\Omega},\mathcal{F},\mathbb{P})$ a probability space, we denote $\mathsf{F(X)}$ the space of all measurable functions from $(\mathsf{\Omega},\mathcal{F})$ to $(\mathsf{X},\sigma(\mathsf{X}))$. We say that "$x$ is a random variable on $\mathsf{X}$ with distribution $p_x$" if $x \in \mathsf{F(X)}$ and $p_x \in \mathsf{M}_1(\mathsf{X})$ satisfies $p_x = \mathbb{P} \circ x^{-1}$.  We define a process $(x_t)_{t \in \mathsf{T}}$ on $\mathsf{X}$ as a collection of random variables indexed by $\mathsf{T}$ and taking values in $\mathsf{X}$. 

When useful, we will sometimes stress on using sans-serif formatting $\mathsf{x}$ for non random objects as opposed to serif formatting $x$ for random objects. However we will often refer to objects and their expressions interchangeably. For example, if $x$ is a random variable in $\mathsf{X}$, we say "the function $f(x) = \ldots $" to refer to a function with input space $\mathsf{F(X)}$ having a given expression, and if $\mathsf{x} \in \mathsf{X}$, we say "the function $f(\mathsf{x}) = \ldots"$ to refer to a function with input space $\mathsf{X}$ having a given expression.We systematically overload the symbol $p$ by using $p(\rmd x)$ to refer to a distribution $p_x$ and interchangeably with its expression $p_x(\mathsf{\rmd x})$. If $x$ and $y$ are random variables on $\mathsf{X}$ and $\mathsf{Y}$, respectively, we write $p(\rmd y|x)$ for the conditional distribution of $y$ given that $x = \mathsf{x}$. In practise, we assume that all distributions $p(\rmd x)$ on $\mathsf{X}$ are dominated w.r.t some reference measure $\mu_x$ on $\sigma(\mathsf{X})$ and admit a probabiltiy density denoted as $p(x)$ (interchangeably with its expression $p(\mathsf{x})$ depending on the context) defined as the Radon-Nikodym derivative $p(x) = \frac{\rmd p_x}{\rmd \mu_x}$. Given all this, we often mix a distribution with its density and say that "$x$ is a random variable with distribution $p(x) = p(\mathsf{x})$" to mean that $x$ is a random variable on $\mathsf{X}$ admitting a density whose expression is $p(\mathsf{x})$. Similarly, we will refer to "$p(x_t|x_{t-1}) = p(x_t|\mathsf{x}_{t-1})$" when a Markovian process $(x_t)_{t \in \mathsf{T}}$ is defined by a Markov kernel which admits a density expressed by $p(x_t|\mathsf{x}_{t-1})$.



\subsection{Nonlinear ICA with flexible dynamics}\label{nonlinear_ica}

In this section, we introduce the setting of nonlinear ICA and define the models that form the basis of our work. Let $(x_t)_{t \geq 1}$ be a process in an $d_\mathsf{X}$-dimensional space $\mathsf{X}$ and for which we assume that:
$$x_t = f(s_t) + \epsilon_t$$

In the ICA setting, $(s_t)_{t \geq 1}$ is a process in $\mathsf{S}^I$ with $\dim(\mathsf{S}) = d_\mathsf{S}$ and $I \in \mathbb{N}^{*}$. In the nonlinear case, $f$ is a nonlinear mapping from $\mathsf{S}^I$ to $\mathsf{X}$ and is usually supposed to be injective. The central assumption is that the components $(s_t^i)_{i \leq I}$ are mutually independant random variables on $\mathsf{S}$.

In a general setting, these variables can follow their own temporal dynamics. Transitions are defined component-wise and can also be conditioned on another (unobserved) process $(u_t)_{t \geq 1}$ on a space $\mathsf{U}^I$ for which each of the $(u_t^i)_{i \leq I}$ is a first order Markov chain in $\mathsf{U}$ with Markov kernel $p(u_t^i|u_{t-1}^i)$ and initial distribution $p(u_1^i)$. Finally, rich models with complex dependencies are allowed by prescribing the transitions in the augmented state space $\mathsf{H} = \mathsf{S}^D$ instead of $\mathsf{S}$, with $D > 1$. Denoting $(h_t)_{t \geq 1}$ the process in the augmented space, each $(h_t^i)_{t \geq 1}$ is a Markov chain in $\mathsf{H}$ conditonally on $(u_t^i)_{t \geq 1}$ with Markov kernel $p(h_t^i|h_{t-1}^i,u_t^i)$ and initial distribution $p(h_1^i|u_1^i)$.

Denote $z_t = (z_t^i)_{i \leq I} \in \mathsf{Z}^I$ the full latent state at time $t$ defined by $z_t^i = (h_t^i, u_t^i)$, where $\mathsf{Z} = \mathsf{H} \times \mathsf{U}$. Each $(z_t^i)_{i \leq I}$ is also a Markov chain in $\mathsf{Z}$ with Markov kernel defined as $p(z_t^i|z_{t-1}^i) = p(h_t^i,u_t^i|h_{t-1}^i, u_{t-1}^i)$ = $p(h_t^i|h_{t-1}^i,u_t^i)p(u_t^i|u_{t-1}^i)$. With all that, $(z_t)_{t \geq 1}$ is a Markov chain in $\mathsf{Z}^I$ with inital distribution $p(z_1)$ and kernel

\begin{equation}\label{kernel_z}
    p(z_t|z_{t-1}) = \prod_{i=1}^I p(z_t^i|z_{t-1}^i)
\end{equation}


Simply put $(z_t)_{z \geq 1}$ is just a special form of autoregressive process with independant components. The full data model is given by the finite dimensional law $p(x_{1:T}, s_{1:T}, u_{1:T})$ which is a marginal of its counterpart in augmented space $p(x_{1:T}, z_{1:T}) = p(x_{1:T} \vert z_{1:T})p(z_{1:T})$. At each time $t$, $x_t$ only depends on $s_t$, and observations $(x_t)_{t \leq T}$ are conditionally independant given $(s_t)_{t \leq T}$, therefore $p(x_{1:T} \vert z_{1:T}) = \prod_{t=1}^T p(x_t|s_t)$. All in all, the full data model writes


\begin{equation}\label{full_model}
    p(x_{1:T}, z_{1:T}) = p(u_1)p(h_1|u_1) p(x_1|s_1) \prod_{t=2}^T p(x_t|s_t) \prod_{i=1}^I p(h_t^i|h_{t-1}^i,u_t^i)p(u_t^i|u_{t-1}^i)
\end{equation} 

\subsection{Variational inference for linear Gaussian HMMs}\label{linear_gauss}
\subsubsection*{Small recap on variational inference}
Given observation vectors $x_{1:T}$, we seek to compute the posterior distributions $p( z_{1:T}|x_{1:T})$. To cope with scenarios where $d_{\mathsf{S}}$ and/or $D$ may be large, we resort to variational inference and want to compute an approximation $q( z_{1:T}|x_{1:T}) \in \mathcal{Q}$ where $\mathcal{Q}$ is a family of distributions that we need to choose. 
We consider an online setup where observations $x_t$ are provided sequentially. From now on we write $p_{1:t}$ for any distribution conditional on $x_{1:t} = \mathsf{x}_{1:t}$. We assume that the best approximation $q_{1:t}^*(z_{1:t}) \in \mathcal{Q}$ (in Kullback-Leibler sense) of $p_{1:t}( z_{1:t})$ can be approached by finding $\argmax\limits_{q \in \mathcal{Q}} \mathcal{L}_t(q)$ where $\mathcal{L}_t(q) = \expect{q(z_{1:t})}{\log\frac{p(z_{1:t}, \mathsf{x}_{1:t})}{q(z_{1:t})}}$. 
Estimates of this optimum are usually available via gradient-based techniques which require assumptions on $\mathcal{Q}$. 
Denoting $\mathcal{P}$ be the family of distributions in which the true posterior $p_{1:t}$ belongs, a sensible choice is to have $\mathcal{Q}$ that closely resembles $\mathcal{P}$ while making optimization tractable. 
\subsubsection*{A simplified fully linear model}
From section \ref{nonlinear_ica}, we start by restricting $\mathcal{P}$ to a much simpler family. We first assume that there is no switching process and that each of the $h_t^i$ follows separate linear Gaussian dynamics defined by parameters $A^i,a^i$ and a Gaussian noise $\eta^i \sim \gaussian{0}{Q^i}$, (the initial distribution is Gaussian with parameters $a_1^i$ and covariance matrix $Q_1^i$). In this case $z_t^i = h_t^i$ and we stick to using $z$ in the following. We also assume that the observation model is time-independant linear Gaussian with parameters $B$ and $b$ and covariance matrix $R$.
Decomposing $z_t^i = \begin{bmatrix}
    s_t^i \\
    S_t^i
    \end{bmatrix}$ 
with each $S_t^i \in \mathsf{S}^{D-1}$, the model is therefore
$$p(z_1^i) \sim \gaussian{a_1^i}{Q_1^i}, \eqsp p(z_1) = \prod_{i=1}^I p(z_1^i)$$
$$p(z_t^i|z_{t-1}^i) \sim \gaussian{A^i z_{t-1}^i + a^i}{Q^i}, \eqsp p(z_t|z_{t-1}) = \prod_{i=1}^I p(z_t^i|z_{t-1}^i)$$
$$p(x_t|z_t) \sim \gaussian{B s_t + b}{R}$$

Since, $p_{1:t}( z_{1:t}) = \frac{p(\mathsf{x}_{1:t},  z_{1:t})}{\int p(\mathsf{x}_{1:t},  z'_{1:t})}$, $p_{1:t}$ is fully specified given parameters $\Theta = \{B, b, R, (A^i, a_1^i, a^i, Q_1^i, Q^i)_{i \leq I}\}$ of the joint model. 
\subsubsection*{A recursion for the ELBO}
The tower property of expectations is such that: 

\begin{equation}
    \mathcal{L}_t(q_{1:t}) = \expect{q_{1:t}( z_{1:t})}{\log \frac{p(x_{1:t},z_{1:t})}{q_{1:t}(z_{1:t})}} = \expect{q_{1:t}( z_t)}{V_t(z_{t})}
\end{equation}

where we have defined $$V_t(z_t) = \expect{q_{1:t}( z_{1:t-1}|z_t)}{\log \frac{p(x_{1:t},z_{1:t})}{q_{1:t}(z_{1:t})}}$$

We denote the forward kernel $p_{(s)}(z_s|z_{s-1}) = p(z_s,x_s|z_{s-1}, x_{s-1}) = p(x_s|z_s)p(z_s|z_{s-1})$ (using the subscript $_{(s)}$ to avoid the confusion $p(z_s,x_s|z_{s-1}) \neq p(z_s|z_{s-1},x_s)$, together with $"s = s:s"$). Note that there is no dependency on $x_{s-1}$ because of HMM structure of our model, this is not true in the general case. Using this notation the numerator in the $\log$ becomes $\prod_{s=1}^t p_{(s)}( z_s|z_{s-1})$. 

In the true model, the distributions $\left\{p_{1:t}(z_{1:t})\right\}_{t \geq 1}$ are the smoothing distributions, which verify the recursion $p_{1:t}(z_{1:t}) = \tilde{p}_{1:t}(z_t|z_{t-1}) p_{1:t-1}(z_{1:t-1})$ where we have defined $$\tilde{p}_{1:t}(z_t|z_{t-1}) = \frac{p_{(t)}(z_{t}|z_{t-1})}{\int p_{(t)}(z_{t}|z_{t-1})p_{1:t-1}(z_{1:t-1})\rmd z_{1:t}}$$

Suppose we choose a variational family where we can define some $\tilde{q}$ that allows the same decomposition $q_{1:t}(z_{1:t}) = \tilde{q}_{1:t}(z_t|z_{t-1}) q_{1:t-1}(z_{1:t-1})$, then we have a recursion for $V_t$, because
\begin{align*}
V_{t+1}(z_{t+1}) &= \expect{q_{1:t}(z_{1:t}|z_{t+1})}{\log \frac{p(x_{1:t+1},z_{1:t+1})}{q_{1:t+1}(z_{1:t+1})}} \\
                   &= \expect{q_{1:t}(z_{1:t}|z_{t+1})}{\log\frac{p_{(t+1)}(z_{t+1}|z_t)\prod_{s=1}^t p_{(s)}(z_s|z_{s-1})}{\tilde{q}_{1:t+1}(z_{t+1}|z_t)\prod_{s=1}^t \tilde{q}_{1:s}(z_s|z_{s-1})}}\\
                   &= \expect{q_{1:t}(z_{1:t}|z_{t+1})}{\log\frac{p_{(t+1)}(z_{t+1}|z_t)}{\tilde{q}_{1:t+1}(z_{t+1}|z_t)} + \log\frac{\prod_{s=1}^t p_{(s)}(z_s|z_{s-1})}{\prod_{s=1}^t \tilde{q}_{1:s}(z_s|z_{s-1})}} \\
                   &= \expect{q_{1:t}( z_t|z_{t+1})}{\expect{q_{1:t}(z_{1:t-1}|z_t,z_{t+1})}{
                           \log\frac{p_{(t+1)}(z_{t+1}|z_t)}{\tilde{q}_{1:t+1}(z_{t+1}|z_t)} + \log\frac{\prod_{s=1}^t p_{(s)}(z_s|z_{s-1})}{\prod_{s=1}^t \tilde{q}_{1:s}(z_s|z_{s-1})}}}
\end{align*}
where the last equality is the tower rule applied once more. 

Suppose that we choose a variational family such that 
\begin{equation}\label{backward_fact}
    q_{1:t}(z_{1:t}) = \vfilt{t}\prod_{s=1}^{t-1} q_{1:s}(z_s|z_{s+1})\eqsp,
\end{equation}
then and $q_{1:t}(z_{1:t-1}|z_t, z_{t+1}) = q_{1:t}(z_{1:t-1}|z_t)$ so

\begin{equation}\label{v_t_recursion}
    V_{t+1}(z_{t+1}) = \expect{\vbackward{t}}{V_t(z_t) + \log\frac{p_{(t+1)}(z_{t+1}|z_t)}{\tilde{q}_{1:t+1}(z_{t+1}|z_t)}}
\end{equation}

with 

\begin{equation}
    \tilde{q}_{1:t+1}(z_{t+1}|z_t) = \frac{\vbackward{t}\vfilt{t+1}}{\vfilt{t}}
\end{equation}

\subsubsection*{A fully tractacle variational approximation}
This recursion is useful provided that we can compute the expectation of the $\log$ term (decomposed as the $\log$ difference) under the backward distribution $\vbackward{t}$. 

In a general HMM, we know that:

\begin{itemize}
    \item Conditionally on $(x_t)_{t \leq T}$, $(z_t)_{t \leq T}$ is a Markov chain backward in time with initial distribution $p_{1:t}(z_t)$, ie. the true smoothing distribution $p_{1:t}(z_{1:t})$ does factorize as in eq. \ref{backward_fact}.
    \item The backward kernel is related to the filtering distribution via: $$p_{1:t}(z_t|z_{t+1}) = \frac{p(z_{t+1}|z_t)p_{1:t}(z_t)}{\int p(z_{t+1}|z_t)p_{1:t}(z_t)\rmd z_t}$$
\end{itemize} 

In the specific case of our more restricted (linear Gaussian) model, we further know, before detailing the computations, that:

\begin{itemize}
    \item The filtering distribution is a Gaussian $p_{1:t}(z_t) \sim \gaussian{\mu_{1:t}}{\Sigma_{1:t}}$
    \item Since $p(z_{t+1}|z_t)$ is a Gaussian where only the mean depends on $z_t$, the numerator in eq. \ref{v_t_recursion} is a Gaussian in $z_t$ and the integral in the denominator is the expected value of that Gaussian distribution. Therefore the backward kernel is also a Gaussian distribution where only the mean depends on $z_{t+1}$. 
\end{itemize} 

With similar reasonings, $\log p_{(t+1)}(z_{t+1}|z_t)$ and $\log \tilde{p}_{1:t+1}(z_{t+1}|z_t)$ are both quadratic forms in $z_t$. This means that if we choose a variational family where this is also true for $\tilde{q}_{1:t+1}(z_{t+1}|z_t)$, then the expectation in the recursion \ref{v_t_recursion} will admit a closed form which is a quadratic form in $z_{t+1}$.
                                                    
One obvious way to choose a variational family that verifies the previous constraints is to define a variational model which mirrors exactly the true model but with different parameters. We start with this option and define the following distributions. We define $\Phi = \{B^\phi, b^\phi, R^\phi, (A^{\phi,i}, a_1^{\phi,i}, a^{\phi,i}, Q_1^{\phi,i}, Q^{\phi,i})_{i \leq I}\}$ the set of all variational parameters, and the following distributions:

$$q(z_1^i) \sim \gaussian{a_1^{\phi,i}}{Q_1^{\phi,i}}, \eqsp q(z_1) = \prod_{i=1}^I q(z_1^i)$$
$$q(z_t^i|z_{t-1}^i) \sim \gaussian{A^{\phi,i} z_{t-1}^i + a^{\phi,i}}{Q^{\phi,i}}, \eqsp q(z_t|z_{t-1}) = \prod_{i=1}^I q(z_t^i|z_{t-1}^i)$$
$$q(x_t|z_t) \sim \gaussian{B^\phi s_t + b^\phi}{R}$$

To facilitate computations, we define the block objects $M = diag(M^1, \ldots, M^I)$ for all relevant parameters. We denote $M^\theta$ the parameters of the true model. We now explicitely derive the closed forms for the expectation of all terms in $\ref{v_t_recursion}$ w.r.t parameters in $\Theta$ and $\Phi$. 

Since the model of our variational approximation is also a linear Gaussian HMM, we can define: 

\begin{itemize}
    \item The variational filtering distribution as a Gaussian $\vfilt{t} \sim \gaussian{\mu_{1:t}^\phi}{\Sigma_{1:t}^\phi}$
    \item The backward variational distribution as a Gaussian $\vbackward{t} \sim \gaussian{\vbackwardmean{t}(z_{t+1})}{\vbackwardcov{t}}$ where $\vbackwardmean{t}$ is linear in $z_{t+1}$ with $\vbackwardmean{t}(z_{t+1}) = \backward{A}_{1:t}^\phi z_{t+1} + \backward{a}_{1:t}^\phi$, where $\backward{A}_{1:t}^\phi$ and $\backward{a}_{1:t}^\phi$ can be entirely expressed in terms of $\mu_{1:t}^\phi, \Sigma_{1:t}^\phi$ and parameters in $\Phi$ (which is detailed below).
\end{itemize} 

We start with the terms from the true model $\log p_{(t+1)}(z_{t+1}|z_t) = \log p(x_{t+1}|z_{t+1}) + \log p(z_{t+1}|z_t)$. The first term on the right hand side is completely measured in $\ref{v_t_recursion}$ at $t+1$ and is therefore integrated later against the backward kernel at $t+2$. We first deal with the second term:

\begin{align}
    \log p(z_{t+1}|z_t) &= -\frac{1}{2}\left(d_\mathsf{Z} \log(2\pi) + \log |Q^\theta|\right) - \frac{1}{2}\left[\quadform{(z_{t+1} - A^\theta z_t - a^\theta)}{\stateprec}\right]
\end{align}

For a random variable $u \sim p(u)$ with $\expect{p(u)}{u} = \mu$ and $\mathbb{V}_{p(u)}[u] = \Sigma$, the following holds for any $\Lambda$:
$$\expect{p(u)}{u^T \Lambda u} = tr(\Lambda \Sigma) + \mu^T \Sigma \mu$$

We denote $u = u(z_t) = A^\theta z_t + a^\theta - z_{t+1}$ and $\Lambda = \stateprec$, and we have

\begin{align*}
    \expect{\vbackward{t}}{u^T \Lambda u} &= \expect{z_t \sim \gaussian{\vbackwardmean{t}}{\vbackwardcov{t}}}{u(z_t)^T \Lambda u(z_t)} \\
    &=  \expect{u \sim \gaussian{A^\theta \vbackwardmean{t} + a^\theta - z_{t+1}}{A^\theta \vbackwardcov{t} {A^\theta}^T}}{u^T \Lambda u}
\end{align*}


which allows the computation of the whole term $\log p(z_{t+1}|z_t)$ against the backward kernel and leads to

\begin{multline*}
    \expect{\vbackward{t}}{\log p(z_{t+1}|z_t)} = -\frac{1}{2}\{d_\mathsf{Z} \log(2\pi) + \log |Q^\theta| + tr(\stateprec A^\theta \vbackwardcov{t} {A^\theta}^T) \\
     + \quadform{(A^\theta \vbackwardmean{t}(z_{t+1}) + a^\theta - z_{t+1})}{\stateprec}\} 
\end{multline*}

Where we can see clearly that it is still a quadratic form in $z_{t+1}$ if $\vbackwardmean{t}(z_{t+1})$ is linear in $z_{t+1}$

To express the parameters $\vbackwardmean{t}$ and $\vbackwardcov{t}$ of the variational backward in terms of parameters in $\Phi$ and $\vfiltmean{t},\vfiltcov{t}$, we just recall that: 

\begin{equation*}
    \vbackward{t} \propto q(z_{t+1}|z_t) \vfilt{t} \propto e^{-\frac{1}{2}\left[\quadform{\left(z_{t+1} - (A^\phi z_t + a^\phi)\right)}{\vstateprec} + \quadform{(z_t - \vfiltmean{t})}{\inv{\vfiltcov{t}}} \right]}
\end{equation*}

Quadratic forms develop into $\quadform{(u-b)}{\Omega} = \quadform{u}{\Omega} - 2b^T \Omega u +b^T \Omega b$ (for $\Omega$ symmetric), so for any $A,b_1,b_2,\Omega_1, \Omega_2$:

\begin{align*}
\quadform{(Au - b_1)}{\Omega_1} + \quadform{(u-b_2)}{\Omega_2} &= \quadform{u}{(A^T\Omega_1 A + \Omega_2)} - 2\left[b_1^T \Omega_1 A + b_2^T \Omega_2 \right]u + \quadform{b_1}{\Omega_1} + \quadform{b_2}{\Omega_2} \\
    &= \quadform{u}{\Omega}
\end{align*}

With $\Omega = (\quadform{A}{\Omega_1}+ \Omega_2)$ and $b = \inv{\Omega}\left(\Omega_2 b_2 + A^T \Omega_1 b_1 \right)$.

This operation is precisely what appears in the argument of the exponential in the previous equation with $A = A^\phi$, $b_1 = z_{t+1} - a^\phi$, $\Omega_1 = \vstateprec$, $b_2 = \vfiltmean{t}$, $\Omega_2 = \inv{\vfiltcov{t}}$, so we can identify the parameters of $\vbackward{t}$ as 

$$\vbackwardcov{t} = \inv{\left(\quadform{{A^\phi}}{\vstateprec} + \inv{\vfiltcov{t}}\right)}$$

$$\vbackwardmean{t} = \left(\quadform{{A^\phi}}{\vstateprec} + \inv{\vfiltcov{t}}\right)\left(\inv{\vfiltcov{t}}\vfiltmean{t} + {A^\phi}^T \vstateprec (z_{t+1} - a^\phi)\right)$$

Therefore we do have the linearity in $z_{t+1}$ with $$\backward{A}_{1:t}^\phi = \left(\quadform{{A^\phi}}{\vstateprec} + \inv{\vfiltcov{t}}\right) {A^\phi}^T \vstateprec$$ and $$\backward{a}_{1:t}^\phi = \left(\quadform{{A^\phi}}{\vstateprec} + \inv{\vfiltcov{t}}\right)\left(\inv{\vfiltcov{t}}\vfiltmean{t} - {A^\phi}^T \vstateprec a^\phi \right)$$

It remains to compute $\expect{\vbackward{t}}{\log \tilde{q}_{1:t+1}(z_{t+1}|z_t)}$ which develops into
\begin{multline*}
    \expect{\vbackward{t}}{\log \tilde{q}_{1:t+1}(z_{t+1}|z_t)}= \expect{\vbackward{t}}{\log \vbackward{t}} + \expect{\vbackward{t}}{\log \vfilt{t+1}} \\ 
    - \expect{\vbackward{t}}{\log \vfilt{t}}
\end{multline*}

The second term is only measured at $t+2$ while the first and third term are expectations of quadratic forms with known parameters under a Gaussian with known parameters, so we apply again $$\expect{x \sim \gaussian{\mu}{\Sigma}}{\quadform{(Ax +b)}{\Lambda}} = tr(\Lambda A \Sigma A^T) + \quadform{(A\mu + b)}{\Lambda}$$ to these two terms and get

\begin{multline*}
\expect{\vbackward{t}}{\log \tilde{q}_{1:t+1}(z_{t+1}|z_t)} = \log \vfilt{t+1} - d_\mathsf{Z} \log(2\pi) - \frac{1}{2}\{\log |\vbackwardcov{t}| + \log |\vfiltcov{t}| \\ +  tr(\vfiltcov{t} {\vbackwardcov{t}}^T) +  tr(\vbackwardcov{t}{\vbackwardcov{t}}^T) 
 + \quadform{(\vbackwardmean{t} - \vfiltmean{t})}{\vfiltcov{t}} \}
\end{multline*}

The terms at $t+2$ are treated likewise. For the observation model we have

\begin{multline*}
\expect{q_{1:t+1}(z_{t+1}|z_{t+2})}{\log p(x_{t+1}|z_{t+1})} = -\frac{1}{2}\{d_{\mathsf{X}}\log 2\pi + \log |R^\theta| + tr\left(\inv{R^\theta} B^\theta\vbackwardcov{t+1}{B^\theta}^T\right) \\ + \quadform{(B^\theta\vbackwardmean{t+1} + b^\theta - x_{t+1})}{\inv{R^\theta}}\}
\end{multline*} 

And for $\log \vfilt{t+1}$ we have

\begin{multline*}
\expect{q_{1:t+1}(z_{t+1}|z_{t+2})}{\log \vfilt{t+1}} = \frac{1}{2}\{d_{\mathsf{Z}} \log 2\pi + \log |\vfiltcov{t+1}| + tr(\inv{\vfiltcov{t+1}}\vbackwardcov{t+1}) \\ + \quadform{(\vbackwardmean{t+1} - \vfiltmean{t+1})}{\vfiltcov{t+1}}\}\\ 
\end{multline*}


\subsection{Extensions}

In the linear Gaussian case, we were able to derive all results in closed-form without any approximation. For this model, optimal estimation of the smoothing distribution and the model parameters is already avaible via the Kalman smoothing recursions coupled with EM optimization or recursive likelihood maximization. 

Provided that $(z_t,x_t)_{t \geq 1}$ is still a HMM, the backward factorization of the joint smoothing distribution always exists for the true model, and is therefore a theoretically motivated choice for constraining the variational family as in \ref{backward_fact}. Given this factorization, the recursion \ref{v_t_recursion} is always true. Therefore, for more general cases the variational family may be more conviently defined directly via the backward kernel. 

In general, if the true model is not assumed to be linear Gaussian, none of the posterior distributions (joint and marginal smoothing or filtering) are Gaussian, and therefore the backward kernels (related to the filtering distributions) are not Gaussian either. Even if we prescribe Gaussianity for the variational terms, the log terms from the true model are not easily integrated against the variational backward because they are not quadratic forms anymore. 

As explained in introduction, identifiabilty results in ICA only exist for models with linear dynamics (though with the possibility of regimes as in section \ref{nonlinear_ica}) so we do not consider nonlinear transitions for the hidden state. Yet, nonlinearity in the observation model alone still breaks the gaussianity of the posteriors. In this case, particle methods can be used to build discretely-supported unbiased and online approximators of the filtering distribution $p_{1:t}(z_t)$. However they suffer from the curse of dimensionality because the number of particle grows linearly with the $d_{\mathsf{Z}}$, which makes them unsuitable to estimate representions that are expressive enough for complex sources (e.g. visual descriptors of individual objects in computer vision, properties of individual speakers in audio, etc). What's more, even if $d_{\mathsf{Z}}$ is small enough, the support of the joint $p_{1:t}(z_{1:t})$ is increasing in $t$, and therefore particle methods can typically only provide estimators of expectations $\expect{p_{1:t}(z_{1:t})}{h(z_{1:t})}$. In this context, it therefore makes sense to stick to variational inference, which is the only tractable tool to provide an approximation of the full joint smoothing distribution. 

In the VAE litterature, powerful variational families (e.g. normalizing flows) are now frequently considered to obtain expressive posteriors. However, we show in this section that many terms in section \ref{linear_gauss} become intractble even when sticking to Gaussian parameterization, and we provide an overview of the different tools which can be used to approximate the latter. 

\subsubsection*{Replacing the Kalman update with a learnt filtering recursion}
\dots

\subsubsection*{Nonlinear observation model}

We assume a similar transition model $p(z_t|z_{t-1})$ for the hidden states, but observations are nonlinear transformations of the components under Gaussian noise, ie. $$p(x_t|z_t) = p(x_t|s_t) \sim \gaussian{f_\theta(s_t)}{R^\theta}$$. 

As stated above, in this case, $p_{1:t}(z_t|z_{t+1})$ is not Gaussian anymore, however we approximate it with a Gaussian variational backward $\vbackward{t} \sim \gaussian{\vbackwardmean{t}(z_{t+1})}{\vbackwardcov{t}}$ and define the variational joint smoothing distribution via the backward factorization given by equation \ref{backward_fact}. If we specify that $\vbackwardmean{t}(z_{t+1})$ is linear in $z_{t+1}$, then all quantities in the previous section except from the expectation of the observation model are tractable as before. For this latter term, at $t+2$, 

\begin{multline*}
\expect{q_{1:t+1}(z_{t+1}|z_{t+2})}{\log p(x_{t+1}|z_{t+1})} = -\frac{1}{2}\{d_{\mathsf{X}}\log 2\pi + \log |R^\theta| \\
    + \expect{z_{t+1} \sim \gaussian{\vbackwardmean{t+1}}{\vbackwardcov{t+1}}}{\quadform{(f_\theta(z_{t+1}) - x_{t+1})}{\inv{R^\theta}}}\} \\
\end{multline*}

and the expectation that remains admits no closed-form. From \cite{johnson}, the following approximation may be a suitable alternative (where we write the equation at $t$ for readability)

\begin{equation}
    \expect{\vbackward{t}}{\log p(x_t|z_t)} \propto \eqsp \quadform{[z_{t+1} - v(x_t)]}{W(x_t)}
\end{equation}

where 

\clearpage
\newpage

\section{Control of the variational error for additive functionals}
Let $\parvec$ be a parameter lying in a $\Theta\subset \rset^q$ and consider a  \textit{state space model} where the hidden Markov chain  in $\rset^d$ is denoted by $(X_k)_{k\geqslant 0}$. The distribution of $X_0$ has density $\chi$ with respect to the Lebesgue measure and for all $0\leqslant k \leqslant n-1$, the conditional distribution of $X_{k+1} $ given $X_{0:k}$ has density $\hd{k+1;\parvec}(X_{k},\cdot)$, where $a_{u:v}$ is a short-hand notation for $(a_u,\ldots,a_v)$. 
It is assumed that this state  is partially observed  through an observation process $(Y_k)_{0\leqslant k \leqslant n}$ taking values in $\rset^m$. 
For all $0\leqslant k \leqslant n$, the distribution of $Y_k$ given $X_{0:n}$ depends on $X_k$ only and has density $\md{k;\parvec}(X_k,\cdot)$ with respect to the Lebesgue measure. 
In this context, for any pair of indices $0\leqslant k_1 \leqslant k_2 \leqslant n$, we define the \textit{joint smoothing distribution} as the conditional law of $X_{k_1:k_2}$ given $Y_{0:n}$. 
In this framework, the likelihood of the observations $\llh{n,\parvec}(Y_{0:n})$, which is  in general intractable, is
$$
\llh{n,\parvec}(Y_{0:n})  = \int \chi(x_0)\md{0;\parvec}(x_{0},Y_{0})\prod_{k=0}^{n-1}\qg{k;\parvec}(x_{k},x_{k+1})\rmd x_{0:n}\eqsp,
$$
 where, for all $0\leqslant k \leqslant n$ and all $\parvec\in\parspace$,
\begin{equation}
\label{eq:def:elln}
\qg{k;\parvec}(x_{k},x_{k+1}) = \hd{k+1;\parvec}(x_{k}, x_{k+1})\md{k+1;\parvec}(x_{k+1},Y_{k+1})\eqsp.
\end{equation}
In a large variety of situations, the loglikelihood of the observations is not available explicitly and obtaining approximate maximum likelihood estimators is computationally intensive, in particular in high dimensional settings. Standard approaches  aim at computing \textit{smoothing expectations of additive functionals} of the form $\pE \left[\af{0:n}(X_{0:n})\middle | Y_{0:n}\right]$,  where $\af{0:n}$ is an \textit{additive functional}, \textit{i.e.} a function from $\rset^{d \times (n + 1)}$ to $\rset^{d'}$ satisfying:
\begin{equation}
\label{eq:additive:functional}
\af{0:n}: x_{0:n} \mapsto \sum_{k=0}^{n-1}\addf{k}(x_{k},x_{k+1})\eqsp,
\end{equation}
where $\addf{k}:\rset^{d} \times \rset^{d}\to\rset^{d'}$.
Such expectations are the keystones of many common inference problems in state space models.

\begin{example}[EM algorithm.]
\label{ex:em:algorithm}
In the usual case when $\theta$ is unknown, the maximum likelihood estimator is $\widehat \parvec = \mathrm{argmax}_{\parvec\in\parspace}\eqsp\llh{n,\parvec}(Y_{0:n})$. Expectation Maximization based algorithms are appealing solutions to obtain an estimator of $\hat \parvec$.
The pivotal concept of the EM algorithm is that the intermediate quantity defined by
\begin{equation*}
\parvec\mapsto Q(\parvec,\parvec') = \pE_{\parvec'}\left[\sum_{k=0}^{n-1} \log \qg{k;\parvec}(X_{k}, X_{k+1})\middle | Y_{0:n}\right] 
\end{equation*}
may be used as a surrogate for $\llh{n}(\parvec)$ in the maximization procedure,  where $\pE_{\parvec'}$ is the expectation under the joint distribution of the latent states and the observations when the model is parameterized by $\parvec'$. 
\end{example}

\subsection{Variational backward decomposition}
In variational approaches, instead of trying to design Monte Carlo estimators of such expectations (or equivalently of the conditional distribution of the states given the observations), the conditional law of $X_{0:n}$ given $Y_{0:n}$ is approximated by choosing a candidate in a parametric family $\{q_\phi\}_{\phi \in \Phi}$, where $\Phi$ is a parameter set. Parameters are then estimated by maximizing the ELBO defined as:
$$
\mathcal{L}(\parvec,\phi) = \pE_{q_\phi}\left[\log \frac{p_\parvec(X_{0:n},Y_{0:n})}{q_\phi(X_{0:n}|Y_{0:n})}\right]\eqsp.
$$
Most works in the literature focus on mean field approximations which means that the family $\{q_\phi\}_{\phi \in \Phi}$ can be written as a product of independent distributions. However, such an assumption fails to hold in standard hidden Markov models which may lead to poor result in the approximation of the posterior distribution of some states given the observations. In this paper, we propose another solution to provide a decomposition of   $q_\phi$ which accounts for the hidden Markov structure of $p_\parvec$. Under the assumptions of this paper, conditionally on $\{Y_{0:n}\}$, the backward chain $(X_n,\ldots,X_0)$ is a Markov chain with inhomogeneous Markov transition kernels. This leads us to introduce a variational family where each $q_\phi$ is of the form,
$$
q_\varphi: x_{0:n} \mapsto q_{\varphi,n}(x_n|Y_{0:n})\prod_{k=0}^{n-1}q_{\varphi,k}(x_k|x_{k+1},Y_{0:k})\eqsp.
$$
The variational family parameterizes the backward kernel which naturally appears in the decomposition of the law of $X_{0:n}$ given $Y_{0:n}$. 

\subsection{Control of the variational error}
Let $(\Xset_n, \Xfd_n)_{n \in \nset}$ be a sequence of general state spaces and let, for all $n \in \nset$, $\uk{n}$ defined on  $\Xset_n \times \Xfd_{n + 1}$ be bounded kernels in the sense that $\sup_{x \in \Xset_n} \uk{n}(x, \Xset_{n + 1}) < \infty$. We will assume a dominated model where each kernel $\uk{n}$ has a kernel density $\ud{n}$ with respect to some $\sigma$-finite reference measure $\mu_{n + 1}$ on $\Xfd_{n + 1}$. Finally, let $\chi$ be some bounded measure on $\Xfd_0$. In the following, we denote state-space product sets and $\sigma$-fields by $\Xset^n \eqdef \Xset_0 \times \cdots \times \Xset_n$ and $\Xfd^n \eqdef \Xfd_0 \tensprod \cdots \tensprod \Xfd_n$, respectively, and consider probability measures  
\begin{equation} \label{eq:def:post}
\post{0:n}(\rmd x_{0:n}) \propto \chi(\rmd x_0) \prod_{m = 0}^{n - 1} \uk{m}(x_m, \rmd x_{m + 1}), \quad n \in \nset, 
\end{equation}
on these product spaces.\footnote{We will always use the standard convention $\prod_{\varnothing} = 1$, implying that $\post{0} \propto \chi$.} The generality of the model \eqref{eq:def:post} is striking. In the special case where each $\uk{n}$ can be decomposed as $\uk{n}(x_n, \rmd x_{n + 1}) = \md{n}(x_n) \, \hk_n(x_n, \rmd x_{n + 1})$ for some Markov kernel $\hk_n$ and some nonnegative potential function $\md{n}$, \eqref{eq:def:post} yields the \emph{Feynman-Kac path models} \cite{delmoral:2004}, which are applied in a large variety of scientific and engineering disciplines, including statistics, physics, biology, and signal processing. 
Given a sequence $(\addf{n})_{n \in \nset}$ of measurable functions $\addf{n} : \Xset_n \times \Xset_{n + 1} \to \rset$, the aim of this section is to control  the error between expectations of \emph{additive functionals}   
\begin{equation} \label{eq:add:func}
    h_n:  x_{0:n} \mapsto \sum_{m = 0}^{n - 1} \addf{m}(x_m, x_{m + 1})
\end{equation}
under the distribution flow $(\post{0:n})_{n \in \nset}$ and under the variational family.   

For all $n\in\nset$, write $\post{0:n}^{\varphi}$ the joint smoothing distribution provided by the variational family. Then,
$$
 \post{0:n}^{\varphi}(d x_{0:n})= q_{\varphi,n}(dx_n)\prod_{k=0}^{n-1}q_{\varphi,k}(x_{k+1},dx_k)\eqsp.
$$
This joint probability distribution can be written:
$$
 \post{0:n}^{\varphi}(d x_{0:n})= q_{\varphi,n}(dx_0)\prod_{k=0}^{n-1}m_{\varphi,k}(x_{k},dx_{k+1})\eqsp,
$$
where $m_{\varphi,k}(x_{k},dx_{k+1}) =  q_{\varphi,k+1}(dx_{k+1})q_{\varphi,k}(x_{k+1},dx_k)/q_{\varphi,k}(x_k)$.

\begin{hypH}
\label{assum:bias:bound}
There exists a function $c$ such that for all $n \in \mathbb{N}$, $\parvec$, $\varphi$, $h \in \bmf{\Xfd_n \tensprod \Xfd_{n + 1}}$, and $x \in \Xset_n$,   
$$
\left|\int m_{\varphi,n}(x,dx_{n+1}) h (x_{n+1})- \int\uk{n,\parvec}(x,dx_{n+1}) h (x_{n+1})\right| \leq c(\parvec,\varphi) \| h \|_\infty. 
$$
\end{hypH}


\begin{hypH}
\label{assum:strong:mixing}
There exist constants $0 < \udlow < \udup < \infty$ such that for all $k \in \nset$ and $(x_k, x_{k + 1}) \in \Xset_k \times \Xset_{k + 1}$, 
$$
    \udlow \leq \ud{k,\theta}(x_k, x_{k + 1}) \leq \udup
$$ 
and 
$$
    \udlow \leq \inf_{\varphi} m_{\varphi,k}(x_k, x_{k + 1}), \quad \sup_{\varphi} m_{\varphi,k}(x_k, x_{k + 1}) \leq \udup. 
$$ 
\end{hypH}
Under these assumptions, define the mixing rate 
\begin{equation} \label{eq:def:rho}
    \rho \eqdef 1 - \frac{\udlow}{\udup}
\end{equation}
Then for all $n \in \nset$, $\parvec$, $\varphi$, and $h_n$, 
    \begin{align*}
        \big| \postmod{0:n} h_n -  \post{0:n}^{\parvec} h_n \big| 
        &\leq 2 c(\parvec,\varphi) \frac{\udup}{\udlow^2} \sum_{k = 0}^{n - 1} \| \addf{k} \|_\infty \left( \sum_{m = 1}^{n - 1} \rho^{|k - m| - 1} + 1 \right) \\
        &\leq 2 c(\parvec,\varphi) n \frac{\udup}{\udlow^2} \left( 1 + \frac{1}{\rho} + \frac{2}{1 - \rho} \right) \sup_{0 \leq k \leq n - 1} \| \addf{k} \|_\infty\eqsp.
    \end{align*}

\begin{proof}
We preface the proof by a few technical lemmas. For each $m \in \nset$, define the  backward Markov kernel 
\begin{equation} \label{eq:def:backward:kernel}
    \bkw{m}^\parvec(x_{m + 1}, \rmd x_m) \eqdef \frac{\post{m}^\parvec(\rmd x_m) \, \ud{m,\parvec}(x_m, x_{m + 1})}{\post{m}^\parvec[\ud{m,\parvec}(\cdot, x_{m + 1})]} %= \frac{\post{m}^\parvec(\rmd x_m) \, \hd{m;\parvec}(x_{m}, x_{m+1})}{\post{m}^\parvec[\hd{m;\parvec}(\cdot, x_{m+1})]}
\end{equation}
on $\Xset_{m + 1} \times \Xfd_m$. In addition, for each $n \in \nsetpos$, let the Markov kernel   
\begin{equation} \label{eq:def:tstat}
\tstat{n}^\parvec(x_n, \rmd x_{0:n - 1}) \eqdef \prod_{m = 0}^{n - 1} \bkw{m}^\parvec(x_{m + 1}, \rmd x_m)
\end{equation}
on $\Xset_n \times \Xfd^{n - 1}$ denote the joint law of the backward Markov chain induced by the kernels \eqref{eq:def:backward:kernel} when initialised at $x_n \in \Xset_n$.   In the following, the notation $\parvec$ may be dropped when there is no confusion.

Define, for each $n \in \nset$ and $m \in \intvect{0}{n}$, the kernel 
\begin{equation} \label{eq:def:uk:products}
    \uk{m, n}(x_{0:m}', \rmd x_{0:n + 1}) \eqdef \delta_{x_{0:m}'}(\rmd x_{0:m}) \prod_{\ell = m}^n \uk{\ell}(x_\ell, \rmd x_{\ell + 1}) 
\end{equation}
on $\Xset^m \times \Xfd^{n + 1}$. In addition, let $\uk{n, n - 1} = \operatorname{id}$. Note that $\uk{n, n}$ is different from $\uk{n}$ in the sense that the former is defined on $\Xset^n \times \Xfd^{n + 1}$ whereas the latter is defined on $\Xset_n \times \Xfd_{n + 1}$. Define similarly, $\bkw{m}^\varphi(x_{m + 1}, \rmd x_m) = q_{\varphi,m}(x_{m+1},dx_m)$
on $\Xset_{m + 1} \times \Xfd_m$. In addition, define for each $n \in \nsetpos$, the Markov kernel  $\tstat{n}^\varphi(x_n, \rmd x_{0:n - 1}) = \prod_{m = 0}^{n - 1} \bkw{m}^\varphi(x_{m + 1}, \rmd x_m)$ on $\Xset_n \times \Xfd^{n - 1}$ denote the joint law of the backward Markov chain induced by the kernels \eqref{eq:def:backward:kernel} when initialised at $x_n \in \Xset_n$.  In the following, the notation $\parvec$ may be dropped when there is no confusion. 
%\begin{itemize} 
%\item[(i)]
%For all $n \in \nset$ and $h \in \bmf{\Xfd_n \tensprod \Xfd_{n + 1}}$, 
%\begin{equation} \label{eq:reversibility}
%\iint \post{n}^\parvec(\rmd x_n) \, \uk{n,\parvec}(x_n, \rmd x_{n + 1}) \, h(x_n, x_{n + 1}) = \iint \post{n}^\parvec \uk{n,\parvec}(\rmd x_{n + 1}) \, \bkw{n}^\parvec(x_{n + 1}, \rmd x_n) \, h(x_n, x_{n + 1}). 
%\end{equation}
%\item[(ii)]
%For all $n \in \nsetpos$ and $h \in \bmf{\Xfd^n}$, 
%$
%\post{0:n}^\parvec h = \post{n}^\parvec \tstat{n}^\parvec h. 
%$
%\end{itemize}
 For each $n \in \nset$ and $m \in \intvect{0}{n}$, define the kernel  
\begin{equation} \label{eq:def:retrokmod}
    \retrokmod_{m, n}(x_m', \rmd x_{0:n}) \eqdef \delta_{x_m'}(\rmd x_m) \, \tstatmod{m}(x_m, \rmd x_{0:m - 1}) \prod_{\ell = m}^{n - 1} \uk{\ell}(x_\ell, \rmd x_{\ell + 1}), 
\end{equation}
on $\Xset_m \times \Xfd^n$. 

\begin{lemma} \label{lem:retro:prospective:id}
For all $m \in \intvect{0}{n}$ and $h \in \bmf{\Xfd^n}$, 
\begin{equation} \label{eq:retro:prospective:id}
\postmod{0:m} \uk{m, n - 1} h = \postmod{m} \retrokmod_{m, n} h.  
\end{equation}
\end{lemma}
The following probability measures play a key role in the following: for $h \in \bmf{\Xfd_m}$, 
\begin{align}
\noshift_{m, n} h &\eqdef \frac{\postmod{m} (h \times \retrokmod_{m, n} \1_{\Xset^n})}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}}, \nonumber \\
\shiftfwd_{m, n} h &\eqdef \frac{\postmod{m} (h \times \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}}, \nonumber \\
\shiftbwd_{m, n} h &\eqdef \frac{\postmod{m - 1} \uk{m - 1}(h \times \retrokmod_{m, n} \1_{\Xset^n})}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}.  \nonumber 
\end{align}
In addition, for each $k \in \intvect{0}{n - 1}$, let 
\begin{equation} \label{eq:def:addf}
\addf[n]{k} : \Xset^n \ni x_{0:n} \mapsto \addf{k}(x_k, x_{k + 1})  
\end{equation}
denote the extension of $\addf{k}$ to $\Xset^n$. 

\begin{lemma} \label{lemma:three:identities}
Let $n \in \nset$ and $m \in \intvect{1}{n}$. Then the following holds true for all $k \in \intvect{0}{n - 1}$. 
\begin{itemize}
\item[(i)]  
$
\displaystyle \noshift_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right) = \frac{\postmod{m} \retrokmod_{m, n} \addf[n]{k}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}}$ \quad for $m \in \intvect{1}{n}$, 
\item[(ii)]  
$
\displaystyle \shiftfwd_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right) = \frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{k}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}}
$ \quad for $m \in \intvect{k + 1}{n}$, and 
\item[(iii)] 
$
\displaystyle \shiftbwd_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right) = \frac{\postmod{m - 1} \retrokmod_{m - 1, n} \addf[n]{k}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}
$ \quad for all $m \in \intvect{1}{k}$. 
\end{itemize}
\end{lemma}

\begin{proof}
The identity (i) follows straightforwardly by the definition of $\noshift_{m, n}$. We hence turn to (ii), which is established by first noting that for all $m \in \intvect{k + 1}{n}$, 
$$
\frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} 
= \bkmod[\postmod{m - 1}]{m - 1} \cdots \bkmod[\postmod{k + 1}]{k + 1} (\bkmod[\postmod{k}]{k} \addf{k}).  
$$
Now, 
\begin{align}
\shiftfwd_{m, n} h 
&= \iint \frac{\postmod{m}(\rmd x_m) \, \ukmod{m}(x_m, \rmd x_{m + 1}) \, h(x_m) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} \nonumber \\
&= \iint \frac{\postmod{m} \ukmod{m}(\rmd x_{m + 1}) \, \bkmod[\postmod{m}]{m}(x_{m + 1}, \rmd x_m) \, h(x_m) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} \nonumber \\
&= \frac{\postmod{m + 1} (\bkmod[\postmod{m}]{m} h \times \retrokmod_{m + 1, n} \1_{\Xset^n})}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}}, \nonumber 
\end{align}
we may establish the identity by proceeding like  
\begin{align*}
\lefteqn{\shiftfwd_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right)} \\
&= \iint \frac{\postmod{m + 1}(\rmd x_{m + 1}) \, \bkmod[\postmod{m}]{m}(x_{m + 1}, \rmd x_m) \, \bkmod[\postmod{m - 1}]{m - 1}  
\cdots \bkmod[\postmod{k + 1}]{k + 1}(\bkmod[\postmod{k}]{k} \addf{k})(x_m) 
\retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} \\
&= \frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{k}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}}.  
\end{align*}

Finally, to check (iii), note that for all $m \in \intvect{1}{k}$, 
$$
\uk{m - 1} \retrokmod_{m, n} \addf[n]{k} = \retrokmod_{m - 1, n} \addf[n]{k}. 
$$
Thus, in this case 
$$
\shiftbwd_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right) = \frac{\postmod{m - 1} \uk{m - 1} \retrokmod_{m, n} \addf[n]{k}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}} = \frac{\postmod{m - 1} \retrokmod_{m - 1, n} \addf[n]{k}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}. 
$$
\end{proof}

\begin{lemma} \label{lem:geo:bound}
Assume H\ref{assum:strong:mixing}. Then for all $n \in \nset$, $m \in \intvect{0}{n}$, $k \in \intvect{0}{n - 1}$, and $(\lambda, \lambda') \in \probmeas{\Xfd_m}^2$, 
$$
\left|\frac{\lambda \retrokmod_{m, n} \addf[n]{k}}{\lambda \retrokmod_{m, n} \1_{\Xset^n}} - \frac{\lambda' \retrokmod_{m, n} \addf[n]{k}}{\lambda' \retrokmod_{m, n} \1_{\Xset^n}} \right| \leq \| \addf{k} \|_\infty \rho^{|k - m| - 1}. 
$$
\begin{proof}
First, assume that $m \leq k$; then note that for all $x_m \in \Xset_m$, 
\begin{multline} \label{eq:retrokmodnorm:vs:forward:kernel}
\frac{\retrokmod_{m, n} \addf[n]{k}(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)} - \frac{\retrokmod_{m, n} \addf[n]{k}(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)}
 = \fk{m}{n} \cdots \fk{k - 1}{n} (\fk{k}{n} \addf{k})(x_m) \\ 
- \fk{m}{n} \cdots \fk{k - 1}{n} (\fk{k}{n} \addf{k})(x_m),    
\end{multline}

where we have introduced the \emph{forward kernels}
$$
\fk{m}{n} h (x_m) \eqdef \frac{\uk{m}(h \times \retrokmod_{m  + 1, n} \1_{\Xset^n})(x_m)}{\retrokmod_{m, n} \1_{\Xset^n} (x_m)}, \quad x_m \in \Xset_m, \quad h \in \bmf{\Xfd_{m + 1}}. 
$$
Under H\ref{assum:strong:mixing}, each forward kernel satisfies a global Doeblin condition in the form of the uniform lower bound 
$$
\fk{m}{n} h (x_m) \geq \frac{\udlow}{\udup} \mu_{m, n} h,  
$$
where we have defined the probability measure 
$$
\mu_{m, n} h \eqdef \frac{\mu_{m + 1}(h \times \retrokmod_{m  + 1, n} \1_{\Xset^n})}{\mu_{m + 1} \retrokmod_{m  + 1, n}  \1_{\Xset^n}}, \quad h \in \bmf{\Xfd_{m + 1}}. 
$$
Thus, by standard results for uniformly minorised Markov chains (see, \emph{e.g.}, \cite[Lemma~4.3.13]{Cappe:2005:IHM:1088883}), the Dobrushin coefficient of each $\fk{m}{n}$ is bounded by $\rho = 1 - \udlow / \udup$. Thus, \eqref{eq:retrokmodnorm:vs:forward:kernel} implies that 
\begin{align*}
\left| \frac{\lambda \retrokmod_{m, n} \addf[n]{k}}{\lambda \retrokmod_{m, n} \1_{\Xset^n}} - \frac{\lambda' \retrokmod_{m, n} \addf[n]{k}}{\lambda' \retrokmod_{m, n} \1_{\Xset^n}} \right| 
&= \left| (\lambda_{m, n} - \lambda_{m, n}') \fk{m}{n} \cdots \fk{k - 1}{n} (\fk{k}{n} \addf{k})
\right| \\
&\leq \rho^{k - m} \| \fk{k}{n} \addf{k} \|_\infty \leq \rho^{k - m} \| \addf{k} \|_\infty, 
\end{align*}
where for $h \in \bmf{\Xfd_m}$, 
$$
\lambda_{m, n} h \eqdef \frac{\lambda(h \times \retrokmod_{m, n} \1_{\Xset^n})}{\lambda \retrokmod_{m, n} \1_{\Xset^n}}, \quad 
\lambda_{m, n}' h \eqdef \frac{\lambda'(h \times \retrokmod_{m, n} \1_{\Xset^n})}{\lambda' \retrokmod_{m, n} \1_{\Xset^n}}. 
$$
Now, assume that $m > k$; then note that for all $x_m \in \Xset_m$, 
\begin{multline} \label{eq:retrokmodnorm:vs:backward:kernel}
\frac{\retrokmod_{m, n} \addf[n]{k}(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)} - \frac{\retrokmod_{m, n} \addf[n]{k}(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)} \\
 = \bkmod{m - 1} \cdots \bkmod{k + 1}(\bkmod{k} \addf{k})(x_m) - \bkmod{m - 1} \cdots \bkmod{k + 1}(\bkmod{k} \addf{k})(x_m).     
\end{multline}
Under H\ref{assum:strong:mixing}, also each backward kernel satisfies a Doeblin condition, namely   
$$
\bkmod{m} h (x_{m + 1}) \geq \frac{\udlow}{\udup} \postmod{m} h,  
$$
with the marginal $\postmod{m}$ playing the role as minorising measure. Thus, the backward kernel Dobrushin coefficients are bounded by the same constant $\rho$, implying, via \eqref{eq:retrokmodnorm:vs:backward:kernel}, that 
$$
\left| \frac{\lambda \retrokmod_{m, n} \addf[n]{k}}{\lambda \retrokmod_{m, n} \1_{\Xset^n}} - \frac{\lambda' \retrokmod_{m, n} \addf[n]{k}}{\lambda' \retrokmod_{m, n} \1_{\Xset^n}} \right| 
= \left| (\lambda_{m, n} - \lambda_{m, n}') \bkmod{m - 1} \cdots \bkmod{k + 1}(\bkmod{k} \addf{k}) \right| \leq \rho^{m - k - 1} \| \addf{k} \|_\infty. 
$$
This completes the proof. 
\end{proof}
\end{lemma}

\begin{lemma} \label{lem:diff:bound} 
Assume H\ref{assum:strong:mixing} and H\
ref{assum:bias:bound}. Then the following holds. 
\begin{itemize}
\item[(i)] For all $n \in \nset$, $m \in \intvect{0}{n}$, $\precpar \in \precparsp$, and $h \in \bmf{\Xfd^n}$,   
$$
\left| \noshift_{m, n} h - \shiftbwd_{m, n} h \right| \vee \left| \shiftfwd_{m, n} h - \noshift_{m, n} h \right| \leq 2 c(\theta,\varphi)  \frac{\udup}{\udlow^2} \| h \|_\infty.   
$$
\item[(ii)] For all $n \in \nset$, $m \in \intvect{0}{n - 1}$, and $\precpar \in \precparsp$,
$$
\left| \frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{m}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} - \frac{\postmod{m} \retrokmod_{m, n} \addf[n]{m}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}} \right| \leq 2 c(\theta,\varphi)  \frac{\udup}{\udlow^2} \| h_m \|_\infty. 
$$
\end{itemize}
In both cases, $c(\theta,\varphi)$ is the constant in H
\ref{assum:bias:bound}. 
\end{lemma}

\begin{proof}
We start with (i). Combining the decomposition 
\begin{multline*}
 \noshift_{m, n} h - \shiftbwd_{m, n} h = \varphi_{m, n}^\precpar h \left( \frac{\postmod{m - 1} \uk{m - 1} \retrokmod_{m, n} \1_{\Xset^n} - \postmod{m - 1} \ukmod{m - 1} \retrokmod_{m, n} \1_{\Xset^n}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}} \right) \\
+ \frac{\postmod{m - 1} \ukmod{m - 1}(h \times \retrokmod_{m, n} \1_{\Xset^n}) - \postmod{m - 1} \uk{m - 1}(h \times \retrokmod_{m, n} \1_{\Xset^n})}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}} 
\end{multline*}
with H\ref{assum:bias:bound} provides the bound 
$$
\left| \noshift_{m, n} h - \shiftbwd_{m, n} h \right| \leq 2 c(\theta,\varphi) \frac{\| \retrokmod_{m, n} \1_{\Xset^n} \|_\infty  \| h \|_\infty}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}.
$$
 Now, since for all $x_m \in \Xset_m$,  
\begin{equation*}
\retrokmod_{m, n} \1_{\Xset^n}(x_m) = \int \ud{m}(x_m, x_{m + 1}) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1}) \, \mu_{m + 1}(\rmd x_{m + 1}) \\
\leq \udup \mu_{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n} 
\end{equation*}
and 
\begin{align*}
\lefteqn{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}} \\
&= \iint \postmod{m - 1}(\rmd x_{m - 1}) \, \ud{m - 1}(x_{m - 1}, x_m) \ud{m}(x_m, x_{m + 1}) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1}) \, \mu_m \tensprod \mu_{m + 1}(\rmd x_{m:m + 1}) \\ 
&\geq \udlow^2 \mu_{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}, 
\end{align*}
implying the bound
\begin{equation} \label{eq:retrokmod:ratio:bd}
 \frac{\| \retrokmod_{m, n} \1_{\Xset^n} \|_\infty}{\postmod{m - 1} \uk{m - 1} \retrokmod_{m, n} \1_{\Xset^n}} \leq \frac{\udup}{\udlow^2}, 
\end{equation}
we may conclude that 
$| \noshift_{m, n} h - \shiftbwd_{m, n} h| \leq 2 c(\theta,\varphi) \udup \| h \|_\infty / \udlow^2
$. 
Along similar lines, the second difference can be bounded by the same quantity by writing   
\begin{multline*}
\shiftfwd_{m, n} h - \noshift_{m, n} h = \shiftfwd_{m, n} h \left( \frac{\postmod{m} \uk{m} \retrokmod_{m + 1, n} \1_{\Xset^n} - \postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset_n}}{\postmod{m} \uk{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} \right) \\
+ \frac{\postmod{m} (h \times \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}) - \postmod{m} (h \times \uk{m} \retrokmod_{m + 1, n} \1_{\Xset_n})}{\postmod{m} \uk{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} 
\end{multline*}
and reapplying H\ref{assum:bias:bound} and \eqref{eq:retrokmod:ratio:bd}.  We turn to (ii). By definition \eqref{eq:def:retrokmod},  
$$
\retrokmod_{m + 1, n} \addf[n]{m}(x_{m + 1}) = \int \addf{m}(x_m, x_{m + 1}) \, \bkmod{m}(x_{m + 1}, \rmd x_m) \, \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1});
$$
thus,
\begin{align}
\frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{m}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} &= \iint \frac{\postmod{m} \ukmod{m}(\rmd x_{m + 1}) \, \addf{m}(x_m, x_{m + 1}) \, \bkmod{m}(x_{m + 1}, \rmd x_m) \, \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} \nonumber \\
&= \iint \frac{\postmod{m}(\rmd x_m) \, \ukmod{m}(x_m, \rmd x_{m + 1}) \, \addf{m}(x_m, x_{m + 1}) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}}. \nonumber
\end{align}
We may thus decompose the quantity under consideration as  
\begin{multline*}
\frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{m}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} - \frac{\postmod{m} \retrokmod_{m, n} \addf[n]{m}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}}
= \frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{m}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} \left( \frac{\postmod{m} \uk{m} \retrokmod_{m + 1, n} \1_{\Xset^n} - \postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}} \right) \\
+ \int \frac{\postmod{m}(\rmd x_m) \{ \ukmod{m}(\addf{m} \retrokmod_{m + 1, n} \1_{\Xset^n})(x_m) - \uk{m} (\addf{m} \retrokmod_{m + 1, n} \1_{\Xset^n})(x_m)\}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}} ,  
\end{multline*}
from which (ii) follows, as before, by a combination of H\ref{assum:bias:bound} and \eqref{eq:retrokmod:ratio:bd}.  
\end{proof}



Write 
$$
\postmod{0:n} h_n - \post{0:n} h_n = \sum_{k = 0}^{n - 1} \left( \postmod{0:n} \addf[n]{k} - \post{0:n} \addf[n]{k} \right), 
$$
where each term can be decomposed according to   
\begin{equation*}
\postmod{0:n} \addf[n]{k} - \post{0:n} \addf[n]{k} = 
\sum_{m = 1}^n \left( \frac{\postmod{0:m} \uk{m, n - 1} \addf[n]{k}}{\postmod{0:m} \uk{m, n - 1} \1_{\Xset^n}} - \frac{\postmod{0:m - 1} \uk{m - 1, n - 1} \addf[n]{k}}{\postmod{0:m - 1} \uk{m - 1, n - 1} \1_{\Xset^n}} \right).  
\end{equation*}
In order to bound each term of this decomposition, write, using Lemma~\ref{lem:retro:prospective:id},  
$$
\frac{\postmod{0:m} \uk{m, n - 1} \addf[n]{k}}{\postmod{0:m} \uk{m, n - 1} \1_{\Xset^n}} - \frac{\postmod{0:m - 1} \uk{m - 1, n - 1} \addf[n]{k}}{\postmod{0:m - 1} \uk{m - 1, n - 1} \1_{\Xset^n}} 
= \frac{\postmod{m} \retrokmod_{m, n} \addf[n]{k}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}} - \frac{\postmod{m - 1} \retrokmod_{m - 1, n} \addf[n]{k}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}. 
$$
Now, for all $m \in \intvect{1}{n}$, pick an arbitrary element $\xarb_m \in \Xset_m$ and define the kernel 
\begin{equation} \label{eq:def:norm:objective:func}
\retrokmodnorm_{m, n} h(x_m) \eqdef \frac{\retrokmod_{m, n} h(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)} - \frac{\retrokmod_{m, n} h (\xarb_m)}{\retrokmod_{m, n} \1_{\Xset^n} (\xarb_m)}, \quad x_m \in \Xset_m, \quad h \in \bmf{\Xfd^n}. 
\end{equation}
Combining this definition with Lemma~\ref{lemma:three:identities}, we may express the quantity of interest as 
\begin{multline*}
\postmod{0:n} \addf[n]{k} - \post{0:n} \addf[n]{k} = \sum_{m = 1}^k \left( \noshift_{m, n} \retrokmodnorm_{m, n} \addf[n]{k} - \shiftbwd_{m, n}  \retrokmodnorm_{m, n} \addf[n]{k} \right) \\ 
+ \frac{\postmod{k + 1} \retrokmod_{k + 1, n} \addf[n]{k}}{\postmod{k + 1} \retrokmod_{k + 1, n} \1_{\Xset^n}} - \frac{\postmod{k} \retrokmod_{k, n} \addf[n]{k}}{\postmod{k} \retrokmod_{k, n} \1_{\Xset^n}} + \sum_{m = k + 1}^{n - 1} \left( \shiftfwd_{m, n} \retrokmodnorm_{m, n} \addf[n]{k} - \noshift_{m, n} \retrokmodnorm_{m, n} \addf[n]{k} \right). 
\end{multline*}
Now, applying Lemmas~\ref{lem:geo:bound} and \ref{lem:diff:bound} to the previous decomposition yields
$$
\left| \postmod{0:n} \addf[n]{k} - \post{0:n} \addf[n]{k} \right| \leq 2 c(\theta,\varphi) \frac{\udup }{\udlow^2} \sum_{k = 0}^{n - 1} \| \addf{k} \|_\infty \left( \sum_{m = 1}^{n - 1} \rho^{|k - m| - 1} + 1 \right).  
$$
which was to be established. 


\end{proof}

\clearpage
\newpage

For each $m \in \nset$, define the  backward Markov kernel 
\begin{equation} \label{eq:def:backward:kernel}
    \bkw{m}^\parvec(x_{m + 1}, \rmd x_m) \eqdef \frac{\post{m}^\parvec(\rmd x_m) \, \ud{m,\parvec}(x_m, x_{m + 1})}{\post{m}^\parvec[\ud{m,\parvec}(\cdot, x_{m + 1})]} = \frac{\post{m}^\parvec(\rmd x_m) \, \hd{m;\parvec}(x_{m}, x_{m+1})}{\post{m}^\parvec[\hd{m;\parvec}(\cdot, x_{m+1})]}
\end{equation}
on $\Xset_{m + 1} \times \Xfd_m$. In addition, for each $n \in \nsetpos$, let the Markov kernel   
\begin{equation} \label{eq:def:tstat}
\tstat{n}^\parvec(x_n, \rmd x_{0:n - 1}) \eqdef \prod_{m = 0}^{n - 1} \bkw{m}^\parvec(x_{m + 1}, \rmd x_m)
\end{equation}
on $\Xset_n \times \Xfd^{n - 1}$ denote the joint law of the backward Markov chain induced by the kernels \eqref{eq:def:backward:kernel} when initialised at $x_n \in \Xset_n$. 

\begin{itemize} 
\item[(i)]
For all $n \in \nset$ and $h \in \bmf{\Xfd_n \tensprod \Xfd_{n + 1}}$, 
\begin{equation} \label{eq:reversibility}
\iint \post{n}^\parvec(\rmd x_n) \, \uk{n,\parvec}(x_n, \rmd x_{n + 1}) \, h(x_n, x_{n + 1}) = \iint \post{n}^\parvec \uk{n,\parvec}(\rmd x_{n + 1}) \, \bkw{n}^\parvec(x_{n + 1}, \rmd x_n) \, h(x_n, x_{n + 1}). 
\end{equation}
\item[(ii)]
For all $n \in \nsetpos$ and $h \in \bmf{\Xfd^n}$, 
$
\post{0:n}^\parvec h = \post{n}^\parvec \tstat{n}^\parvec h. 
$
\end{itemize}
In the case of state-space models, the identity (ii) above is a well-known result typically referred to as the \emph{backward decomposition} of the joint-smoothing distribution. Importantly, as noted in \cite{cappe:2009}, the functions $(\tstat{n} h_n)_{n \in \nsetpos}$ can be expressed recursively through  
\begin{equation} \label{eq:forward:smoothing}
\tstat{n + 1}^\parvec h_{n + 1}(x_{n + 1}) = \int (\tstat{n}^\parvec h_n(x_n) + \addf{n}(x_n, x_{n + 1})) \, \bkw{n}^\parvec(x_{n + 1}, \rmd x_n), \quad n \in \nset, 
\end{equation}
with, by convention, $\tstat{0}^\parvec h_0 \equiv 0$. Here the backward kernel $\bkw{n}^\parvec$ depends on the marginal $\post{n}^\parvec$; thus, the recursion is driven by the marginal flow $(\post{n}^\parvec)_{n \in \nset}$, which may again be expressed recursively. However, as these marginals are, as already mentioned, generally intractable, exact computations need typically to be replaced by approximations. 

 For all $n\in\nset$, write $\post{0:n}^{\varphi}$ the joint smoothing distribution provided by the variational family. Then,
$$
\post{0:n}^\parvec h - \post{0:n}^{\varphi} h= \post{n}^\parvec \tstat{n}^\parvec h - \post{0:n}^{\varphi} h =  \post{n}^\parvec \left\{\prod_{m = 0}^{n - 1} \bkw{m}^\parvec\right\} h - q_{\varphi,n}\left\{\prod_{k=0}^{n-1}q_{\varphi,k}\right\} h\eqsp.
$$
Write 
$$
\postmod{0:n} h_n - \post{0:n}^{\varphi} h_n = \sum_{k = 0}^{n - 1} \left( \postmod{0:n} \addf[n]{k} - \post{0:n}^{\varphi} \addf[n]{k} \right), 
$$
where each term can be decomposed according to   
\begin{equation*}
\postmod{0:n} \addf[n]{k} - \post{0:n}^\varphi \addf[n]{k} = 
\sum_{m = 1}^n \left( \frac{\postmod{0:m} \uk{m, n - 1} \addf[n]{k}}{\postmod{0:m} \uk{m, n - 1} \1_{\Xset^n}} - \frac{\postmod{0:m - 1} \uk{m - 1, n - 1} \addf[n]{k}}{\postmod{0:m - 1} \uk{m - 1, n - 1} \1_{\Xset^n}} \right)  
\end{equation*}











\clearpage
\newpage





Define, for each $n \in \nset$ and $0\leq m \leq n$, the kernel  
\begin{equation} \label{eq:def:retrok}
    \retrok_{m, n}^\parvec(x_m', \rmd x_{0:n}) \eqdef \delta_{x_m'}(\rmd x_m) \,  
    \tstat{n}^\parvec(x_m, \rmd x_{0:m - 1})
    \prod_{\ell = m}^{n - 1} \uk{\ell}^\parvec(x_\ell, \rmd x_{\ell + 1})
\end{equation}
on $\Xset_m \times \Xfd^n$ as well as the centered version 
$$
\retroknorm_{m, n}^\parvec h (x_m) \eqdef  \retrok_{m, n}^\parvec(h - \post{0:n}^\parvec h)(x_m) 
$$
on the same space.  For all $0\leq m \leq n$ and $h \in \bmf{\Xfd^n}$, 
\begin{equation} \label{eq:retro:prospective:id}
\postmod{0:m} \uk{m, n - 1}^\parvec h = \postmod{m} \retrokmod_{m, n} h.  
\end{equation}
In addition, for each $0\leq k \leq n - 1$, let 
\begin{equation} \label{eq:def:addf}
\addf[n]{k}: x_{0:n} \mapsto \addf{k}(x_k, x_{k + 1})  
\end{equation}
denote the extension of $\addf{k}$ to $\Xset^n$.  For all $n\in\nset$, write $\post{0:n}^{\varphi}$ the joint smoothing distribution provided by the variational family. Then,
$$
\post{0:n}^\parvec h - \post{0:n}^{\varphi} h= \post{n}^\parvec \tstat{n}^\parvec h - \post{0:n}^{\varphi} h =  \post{n}^\parvec \left\{\prod_{m = 0}^{n - 1} \bkw{m}^\parvec\right\} h - q_{\varphi,n}\left\{\prod_{k=0}^{n-1}q_{\varphi,k}\right\} h\eqsp.
$$
Write 
$$
\postmod{0:n} h_n - \post{0:n}^{\varphi} h_n = \sum_{k = 0}^{n - 1} \left( \postmod{0:n} \addf[n]{k} - \post{0:n}^{\varphi} \addf[n]{k} \right), 
$$
where each term can be decomposed according to   
\begin{equation*}
\postmod{0:n} \addf[n]{k} - \post{0:n}^\varphi \addf[n]{k} = 
\sum_{m = 1}^n \left( \frac{\postmod{0:m} \uk{m, n - 1} \addf[n]{k}}{\postmod{0:m} \uk{m, n - 1} \1_{\Xset^n}} - \frac{\postmod{0:m - 1} \uk{m - 1, n - 1} \addf[n]{k}}{\postmod{0:m - 1} \uk{m - 1, n - 1} \1_{\Xset^n}} \right)  
\end{equation*}

\bibliographystyle{apalike}
\bibliography{variationalhmm}
 
\end{document}
