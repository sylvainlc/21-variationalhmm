\documentclass{article}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{authblk,url}
\usepackage{amssymb,amsmath,amsthm,twoopt,xargs,mathtools,dsfont}
\usepackage{times,ifthen}
\usepackage{fancyhdr,xcolor}
\usepackage{hyperref}

\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\title{Backward variational inference for hidden Markov models}
\date{}

%\author[$\wr$]{Mathis Chagneux}
\author[$\dag$]{XXX}
%\affil[$\wr$]{{\small LTCI, T\'el\'ecom Paris, Institut Polytechnique de Paris, Palaiseau.}}

\affil[$\dag$]{{\small }}

\lhead{}
\rhead{}

\DeclareUnicodeCharacter{2212}{-}
\usepackage{geometry}
\pagestyle{fancy}


\newcommand{\retrokmodnorm}{\bar{\boldsymbol{\mathcal{L}}}^{\precpar}}
\newcommand{\xarb}{x^\ast}
\newcommand{\precparsp}{\mathcal{E}}
\newcommand{\fk}[2]{\mathbf{F}_{#1 | #2}}
\newcommand{\probmeas}[1]{\mathsf{M}_1(#1)}
\newcommand{\Xfd}{\mathcal{X}}
\newcommand{\uksymbol}{\ell}
\newcommandx{\bkmod}[2][1=]{ 
\ifthenelse{\equal{#1}{}}
{\kernel{B}_{#2}^\precpar}
{\kernel{B}_{#2}^\precpar}
}

\newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
\newcommand{\ukmod}[1]{\mathbf{L}_{#1}^\precpar}
\newcommand{\shiftbwd}{\cev{\shiftsymbol}^{\precpar}}
\newcommand{\shiftfwd}{\vec{\shiftsymbol}^{\, \precpar}}
\newcommand{\shiftsymbol}{\Phi}
\newcommand{\precpar}{\varphi}
\newcommand{\intvect}[2]{\{ #1, #2 \}}
\newcommandx\tstatmod[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter^{\precpar}_{#2}}
	{\tau^{\precpar}_{#2}^{#1}}
}
\newcommand{\noshift}{\shiftsymbol^{\precpar}}
\newcommand{\udlow}{\sigma_-}
\newcommand{\udup}{\sigma_+}
\newcommand{\retrok}{\boldsymbol{\mathcal{L}}}
%\newcommand{\intvect}[2]{\llbracket #1, #2 \rrbracket}
\newcommandx{\bkw}[2][1=]{ 
\ifthenelse{\equal{#1}{}}
{\kernel{B}_{#2}}
{\kernel{B}_{#2}}
}
\newcommand{\retroknorm}{\bar{\boldsymbol{\mathcal{L}}}}
\newcommand{\ud}[1]{\uksymbol_{#1}} 
\newcommand{\nset}{\mathbb{N}}
\newcommand{\nsetpos}{\mathbb{N}_{> 0}}
\newcommand{\1}{\mathbbm{1}} 
\newcommandx{\postmod}[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\phi_{#2}^\varphi}
	{\phi_{#2}^\N}
}
\newcommand{\retrokmod}{\boldsymbol{\mathcal{L}}^\precpar}
\newcommand{\uk}[1]{\mathbf{L}_{#1}}
\newcommand{\tensprod}{\otimes}
\def\dimX{d}
\def\dimY{m}
%\def\Xset{\mathsf{X}}
\newcommand{\Xset}{\mathsf{X}}
\def\Yset{\mathsf{Y}}
\newcommand{\mk}{\kernel{G}}
\newcommand{\hk}{\kernel{Q}}
\newcommand{\md}[1]{g_{#1}}
\newcommand{\SmoothFigSize}{0.27}

\newcommand{\logllh}[1]{\ell_{#1}}
\newcommand{\llh}[1]{\mathsf{L}_{#1}}
\newcommand{\testf}{\mathsf{h}}

\newcommandx\filtderiv[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\eta_{#2}}
	{\eta_{#2}^\N}
}
\newcommand{\pred}[1]{\pi_{#1}}
\newcommand{\parvec}{\theta}
\newcommand{\parspace}{\Theta}
\newcommand{\tstatletter}{\kernel{T}}
%\newcommand{\retrok}{\kernel{D}}
\newcommandx\tstat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\tau_{#2}^{#1}}
}
\newcommandx\tstathat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\widehat{\tau}_{#2}^{#1}}
}
\newcommand{\af}[1]{h_{#1}}
\newcommand{\deriv}{\nabla_{\parvec}}

\newcommand{\kernel}[1]{\mathbf{#1}}
\newcommand{\bmf}[1]{\set{F}(#1)}
\newcommand{\set}[1]{\mathsf{#1}}

\newcommandx{\bk}[2][1=]{
\ifthenelse{\equal{#1}{}}
{\overleftarrow{\kernel{Q}}_{#2}}
{\overleftarrow{\kernel{Q}}_{#2}^{#1}}
}

\newcommandx{\bkhat}[2][1=]{
\ifthenelse{\equal{#1}{}}
{\widehat{\kernel{Q}}_{#2}}
{\widehat{\kernel{Q}}_{#2}^{#1}}
}

\newcommand{\lk}{\kernel{L}}
\newcommand{\idop}{\operatorname{id}}
\newcommand{\hd}[1]{q_{#1}}
\newcommand{\hdhat}[1]{\widehat{q}_{#1}}


\newcommandx{\addf}[2][1=]{
\ifthenelse{\equal{#1}{}}{\termletter_{#2}}{\bar{h}_{#2 | #1}}
}
\newcommand{\addfc}[1]{\underline{\termletter}_{#1}}
\newcommand{\adds}[1]{\af{#1}}
\newcommand{\term}[1]{\termletter_{#1}}
\newcommand{\termletter}{\tilde{h}}
\newcommand{\N}{N}
\newcommand{\partpred}[1]{\pi_{#1}^\N}
\newcommand{\tstattil}[2]{\tilde{\tau}_{#2}^{#1}}
\newcommandx{\K}[1][1=]{
\ifthenelse{\equal{#1}{}}{{\kletter}}{{\widetilde{\N}^{#1}}}}
\newcommand{\hkup}{\bar{\varepsilon}}
\newcommand{\bi}[3]{J_{#1}^{(#2, #3)}}
\newcommand{\bihat}[3]{\widehat{J}_{#1}^{(#2, #3)}}

\newcommand{\kletter}{\widetilde{\N}}

\def\sigmaX{\mathcal{X}}
\def\sigmaY{\mathcal{Y}}
\def\1{\mathds{1}}
\def\pE{\mathbb{E}}
\def\pP{\mathbb{P}}
\def\plim{\overset{\pP}{\longrightarrow}}
\def\dlim{\Longrightarrow}
\def\gauss{\mathcal{N}}


\newcommand{\esssup}[2][]
{\ifthenelse{\equal{#1}{}}{\left\| #2 \right\|_\infty}{\left\| #2 \right\|^2_{\infty}}}


\newcommand{\swght}[2]{\ensuremath{\omega_{#1}^{#2}}}

\newtheorem{assumptionA}{\textbf{A}\hspace{-3pt}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\newcommand{\iid}{i.i.d.}

\newcommand{\smwght}[3]{\tilde{\omega}_{#1|#2}^{#3}}
\newcommand{\smwghtfunc}[2]{\tilde{\omega}_{#1|#2}}

\newcommand{\smpart}[3]{\ensuremath{\tilde{\xi}_{#1|#2}^{#3}}}
\def\aux{{\scriptstyle{\mathrm{aux}}}}
\newcommand{\bdm}{\mathsf{TwoFilt}_{bdm}}
\newcommand{\fwt}{\mathsf{TwoFilt}_{fwt}}

\newcommand{\kiss}[3][]
{\ifthenelse{\equal{#1}{}}{r_{#2|#3}}
{\ifthenelse{\equal{#1}{fully}}{r^{\star}_{#2|#3}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2|#3}}{\mathrm{erreur}}}}}

\newcommand{\chunk}[4][]%
{\ifthenelse{\equal{#1}{}}{\ensuremath{{#2}_{#3:#4}}}{\ensuremath{#2^#1}_{#3:#4}}
}

\newcommand{\kissforward}[3][]
{\ifthenelse{\equal{#1}{}}{p_{#2}}
{\ifthenelse{\equal{#1}{fully}}{p^{\star}_{#2}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2}}{\mathrm{erreur}}}}}

\newcommand{\instrpostaux}[1]{\ensuremath{\upsilon_{#1}}}
\newcommandx\post[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\phi_{#2}}
	{\phi_{#2}^\N}
}

\newcommandx\posthat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\widehat{\phi}_{#2}}
	{\widehat{\phi}_{#2}^\N}
}

\newcommand{\adjfunc}[4][]
{\ifthenelse{\equal{#1}{}}{\ifthenelse{\equal{#4}{}}{\vartheta_{#2|#3}}{\vartheta_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{smooth}}{\ifthenelse{\equal{#4}{}}{\tilde{\vartheta}_{#2|#3}}{\tilde{\vartheta}_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{fully}}{\ifthenelse{\equal{#4}{}}{\vartheta^\star_{#2|#3}}{\vartheta^\star_{#2|#3}(#4)}}{\mathrm{erreur}}}}}

\newcommand{\XinitIS}[2][]
{\ifthenelse{\equal{#1}{}}{\ensuremath{\rho_{#2}}}{\ensuremath{\check{\rho}_{#2}}}}
\newcommand{\adjfuncforward}[1]{\vartheta_{#1}}
\newcommand{\rmd}{\ensuremath{\mathrm{d}}}
\newcommand{\eqdef}{\ensuremath{:=}}
\newcommand{\eqsp}{\;}
\newcommand{\ewght}[2]{\ensuremath{\omega_{#1}^{#2}}}
\newcommand{\ewghthat}[2]{\ensuremath{\widehat{\omega}_{#1}^{#2}}}
\newcommand{\epart}[2]{\ensuremath{\xi_{#1}^{#2}}}
\newcommand{\filt}[2][]%
{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\phi_{#2}}}{\ensuremath{\phi_{#1,#2}}}%
}
\newcommand{\Xinit}{\ensuremath{\chi}}
\newcommand{\sumwght}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\Omega_{#2}}}{\ensuremath{\Omega_{#2}^{(#1)}}}}
\newcommand{\sumwghthat}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\widehat{\Omega}_{#2}}}{\ensuremath{\widehat{\Omega}_{#2}^{(#1)}}}}

\newcounter{hypH}
\newenvironment{hypH}{\refstepcounter{hypH}\begin{itemize}
\item[{\bf H\arabic{hypH}}]}{\end{itemize}}

\newcommand{\marginalset}{\mathsf{U}}

\newcommand{\calF}[2]{\mathcal{F}_{#1}^{#2}}
\newcommand{\calG}[2]{\mathcal{G}_{#1}^{#2}}
\newcommand{\Uset}{\mathsf{U}}
\newcommand{\tcalF}[2]{\widetilde{\mathcal{F}}_{#1}^{#2}}
\newcommand{\tcalG}[2]{\widetilde{\mathcal{G}}_{#1}^{#2}}

\newcommand{\kernelmarg}{\mathbf{R}}

\newcommand{\pplim}{\overset{\pP}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\ddlim}{\overset{\mathcal{D}}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\aslim}{\overset{\pP\mathrm{-a.s.}}{ \underset{\N \to \infty}{\longrightarrow}}}

\newcommand{\qg}[1]{\ell_{#1}}
\newcommand{\hatqg}[1]{\mathsf{\ell}_{#1}}

\newcommand{\sfd}{\mathsf{d}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\e}{\text{e}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\frob}{:}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\vois}{\mathcal{V}}




\newcounter{example}[section]
\newenvironment{example}[1][]{\refstepcounter{example}\par\medskip
   \noindent \textbf{Example~\theexample:} \textit{#1} \text \rmfamily}{\medskip}

\newcommand{\expect}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\gaussian}[2]{\mathcal{N}\left( #1, #2 \right)}
\newcommand{\backward}[1]{\overleftarrow{#1}}
\newcommand{\vbackward}[1]{q_{1:#1}(z_{#1}|z_{#1 + 1})}
\newcommand{\vfilt}[1]{q_{1:#1}(z_{#1})}
\newcommand{\vbackwardparam}[2]{\backward{#1}_{1:#2}^\phi}
\newcommand{\vbackwardmean}[1]{\vbackwardparam{\mu}{#1}}
\newcommand{\vbackwardcov}[1]{\vbackwardparam{\Sigma}{#1}}
\newcommand{\vfiltparam}[2]{#1_{1:#2}^\phi}
\newcommand{\vfiltmean}[1]{\vfiltparam{\mu}{#1}}
\newcommand{\vfiltcov}[1]{\vfiltparam{\Sigma}{#1}}
\newcommand{\inv}[1]{{#1}^{-1}}
\newcommand{\stateprec}{\inv{Q^\theta}}
\newcommand{\vstateprec}{\inv{Q^\phi}}
\newcommand{\quadform}[2]{#1^T #2 #1}


\DeclareMathOperator*{\argmax}{arg\,max} 

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Theoretical perspectives}

Observations $(Y_{t})_{1\leq t \leq T}$, latent variables $(X_{t})_{1\leq t \leq T}$.\\
True (unknown) density distribution $p^{\star}$.\\
Decoding (= model) $p_{\theta}$: joint density of $(X_{t}, Y_{t})_{1\leq t \leq T}$. Write $\theta$ even if non parametric.\\
Encoding $q_{\varphi}$: joint density of $(X_{t})_{1\leq t \leq T}$ given $(Y_{t})_{1\leq t \leq T}$, that is a joint density which is random, and $\sigma(Y_{1},\ldots,Y_{T})$-measurable. May also be non parametric.  The estimators $\widehat{\theta}$ and $\widehat{\phi}$ maximize the ELBO:
$$
{\cal L}(\theta,\phi; T)= \E_{q_{\varphi}} \left[ \log \left( \frac{p_{\theta}(X_{1},\ldots,X_{T},Y_{1},\ldots,Y_{T})}{q_{\varphi}(X_{1},\ldots,X_{T})}\right)\middle\vert Y_{1:T}\right].
$$

\subsection{Independent variables}

We consider that the true distribution, decoder and encoder distributions are defined by:
\begin{align*}
p^{\star}(x_{1},\ldots,x_{T},y_{1},\ldots,y_{T})&=\prod_{t=1}^{T}p^{\star}(x_{t},y_{t})\,,\\
p_{\theta}(x_{1},\ldots,x_{T},y_{1},\ldots,y_{T})&=\prod_{t=1}^{T}p_{\theta}(x_{t},y_{t})\,,\\
q_{\varphi}(x_{1},\ldots,x_{T})&=\prod_{t=1}^{T}q_{\varphi,t}(x_{t})\,.
\end{align*}
Therefore,
$$
\frac{1}{T}{\cal L}(\theta,\phi; T) = \frac{1}{T}\sum_{t=1}^T\log p_\theta(Y_t) + \frac{1}{T}\sum_{t=1}^T\E_{q_{\varphi,t}}\left[\log \frac{p_\theta(X_t|Y_t)}{q_{\varphi,t}(X_t)}\right]\,.
$$
When $q_{\varphi,t}(x_{t})=q_{\varphi}(x_{t}|y_t)$ does not depend on $t$, then ${\cal L}(\theta,\phi; T)/T$ has a a.s. limit:
$$
\lim_{T\mapsto \infty}\frac{1}{T}{\cal L}(\theta,\phi; T) = M_{\infty}(\theta,\phi)=\int p^{\star}(y)\log p_{\theta}(y) dy - \int \log \left(\frac{q_{\varphi}(x\vert y)}{p_{\theta}(x \vert y) } \right)q_{\varphi}(x\vert y)\,.
p^{\star}(y)dxdy
$$
The maximizers of this limit are investigated in \cite{koehler2021variational} in scenarii used in VAE.  The analysis of $(\widehat{\theta},\widehat{\phi})$ as $M-estimators$ with oracle inequalities and applications is in \cite{tang21a}.
If we assume that the $q_{\varphi,t}$ indeed depend on $t$, then if we write for each $\theta$,
$$
\widehat{q}_{\theta,t}=q_{\widehat{\varphi}_{t}(\theta),t}= \underset{{q \text{ of form }q_{\varphi},t}}{\mathrm{argmin}} \int  \left[\log \left(\frac{q(x\vert Y_{t})}{p_{\theta}(x \vert Y_{t}) } \right)\right]q(x\vert Y_{t})dx,
$$
then $\widehat{\theta}$ maximizes
$$
\ell_{T}(\theta)-\sum_{t=1}^{T} D\left(\widehat{q}_{t,\theta} \vert p_{\theta}(\cdot \vert Y_{t})\right),
$$
where $\ell_{T}$ is the log-likelihood, thus something like a penalized likelihood. All of this is  interesting only if such VAE with so many parameters for the encoding   is used for computational convenience or so. If this is the case, then write a theory to investigate questions such as:
\begin{itemize}
\item If the encoder is a good approximating family then the penalization is small; can it be build so that the penalty is $o(T)$ ? 
\item
And then possible to analyse that for large $T$ (or asymptotically) if $p^{\star}$ is in the decoding class,  for large $t$ is close to $p_{\widehat{\theta}}(\cdot \vert Y)$ for $\widehat{\theta}$ having good asymptotic properties ?
\end{itemize}

 
\subsection{State space decoding}
We consider that 

$$p_{\theta}(x_{1},\ldots,x_{T},y_{1},\ldots,y_{T})=p_{\theta}(x_{1})g_{\theta}(y_{1}\vert x_{1})\prod_{t=2}^{T}p_{\theta}(x_{t-1},x_{t})g_{\theta}(y_{t} \vert x_{t}).
$$ 


\subsubsection{Mean-field encoding}
The case where
$$q_{\varphi}(x_{1},\ldots,x_{T})=\prod_{t=1}^{T}q_{\varphi}(x_{t}).$$
Then ${\cal L}(\theta,\phi; T)/T$ has a limit (as soon as under $p^{\star}$ the $(Y_{t})_{t\geq 1}$ satisfy some dependence properties allowing laws of large numbers):
$$
\lim_{T\mapsto \infty}\frac{1}{T}{\cal L}(\theta,\phi; T) = M_{\infty}(\theta,\phi)=\int p^{\star}(y) q_{\varphi}(x_{1}\vert y)q_{\varphi}(x_{2}\vert y)
 \left[\log \left(\frac{p_{\theta}(x_{1},x_{2} ) g_{\theta}(x_{2}\vert y)}{q_{\varphi}(x_{2}\vert y)} \right)\right]
dx_{1}dx_{2}dy
$$
What can be said ?
\begin{itemize}
\item
Write the $M$-estimator  oracle inequality theory. Or at least the asymptotic theory.
\item
Understand the bias (that is: the maximizers of $M_{\infty}$) in interesting examples.
\end{itemize}

\subsubsection{HMM encoding/ Forward / Backward encoding}

\begin{itemize}
\item {\bf Parameterization as in Chagneux et al. } Assume that the variational family is parameterized by $\{\phi^T_{\varphi,t},q_{\varphi,t-1|t}\}_{0\leq t\leq T}$ where for all $0\leq t\leq T$, $\phi_{\varphi,t}$ is a probability density on $\rset^d$, and $q_{\varphi,t-1|t}$ a transition kernel on $\rset^d\times \rset^d$. Then, write
$$
q_{\varphi,0:T}(x_{0:T}) = \phi^T_{\varphi,T}(x_T)\prod_{t=1}^Tq_{\varphi,t-1|t}(x_t,x_{t-1})\,.
$$
In this decomposition, $q_{\varphi,t-1|t}$ aims at approximating the true backward kernel of the hidden chain. In this setting, we can write
$$
q_{\varphi,0:T}(x_{0:T}) = \frac{\phi^T_{\varphi,T}(x_T)}{\phi^{T-1}_{\varphi,T-1}(x_{T-1})}q_{\varphi,T-1|T}(x_T,x_{T-1})q_{\varphi,0:T-1}(x_{0:T-1})
$$
and the ELBO writes
\begin{align*}
\mathcal{L}_T(\theta,\varphi) &= \mathbb{E}_{q_{\varphi,0:T}}\left[F_{\theta,\varphi,T}(X_{0:T},Y_{0:T})\right] = \mathbb{E}_{q_{\varphi,0:T}}\left[\log \frac{p_{\theta,0:T}(X_{0:T},Y_{0:T})}{q_{\varphi,0:T}(X_{0:T})}\right]\\
&= \mathbb{E}_{q_{\varphi,0:T}}\left[\log  \frac{p_{\theta,T}(X_{T-1},X_T)g_{\theta,T}(X_T)\phi^{T-1}_{\varphi,T-1}(X_{T-1})}{q_{\varphi,T-1|T}(X_T,X_{T-1})\phi^T_{\varphi,T}(X_T)}\right] + \mathbb{E}_{q_{\varphi,0:T}}\left[F_{\theta,\varphi,T-1}(X_{0:T-1},Y_{0:T-1})\right]\,.
\end{align*}
Let 
$$
\nu_{\varphi,T-1:T}(x_{T-1},x_T) = \phi^T_{\varphi,T}(x_T)q_{\varphi,T-1|T}(x_T,x_{T-1})$$ and 
\begin{align*}
\mu_{\theta,\varphi,T-1:T}(x_{T-1},x_T) &\propto p_{\theta,T}(X_{T-1},X_T)g_{\theta,T}(X_T)\phi^{T-1}_{\varphi,T-1}(X_{T-1}) \\
&= c_{\theta,\varphi,T}p_{\theta,T}(X_{T-1},X_T)g_{\theta,T}(X_T)\phi^{T-1}_{\varphi,T-1}(X_{T-1})\,.
\end{align*}
Then,
\begin{align*}
\mathcal{L}_T(\theta,\varphi) &= - \log c_{\theta,\varphi,T}  + \mathbb{E}_{\nu_{\varphi,T-1:T}}\left[\log \frac{\mu_{\theta,\varphi,T-1:T}}{\nu_{\varphi,T-1:T}}\right]  + \mathbb{E}_{q_{\varphi,0:T}}\left[F_{\theta,\varphi,T-1}(X_{0:T-1},Y_{0:T-1})\right]\,,\\
&= - \log c_{\theta,\varphi,T}  -\mathrm{KL}(\nu_{\varphi,T-1:T}\|\nu_{\varphi,T-1:T})  + \mathbb{E}_{q_{\varphi,0:T}}\left[F_{\theta,\varphi,T-1}(X_{0:T-1},Y_{0:T-1})\right]\,.
\end{align*}
\item {\bf Parameterization using forward kernels and filtering distributions. } Assume that the variational family is parameterized by $\{\phi_{\varphi,t},m_{\varphi,t|t-1}\}_{0\leq t\leq T}$ where for all $0\leq t\leq T$, $\phi_{\varphi,t}$ is a probability density on $\rset^d$, and $m_{\varphi,t|t-1}$ a transition kernel on $\rset^d\times \rset^d$. Then, write
$$
q_{\varphi,0:T}(x_{0:T}) = \phi_{\varphi,T}(x_T)\prod_{t=1}^Tq_{\varphi,t-1|t}(x_t,x_{t-1})\,,
$$
where $q_{\varphi,t-1|t}(x_t,x_{t-1}) \propto \phi_{\varphi,t-1}(x_{t-1})m_{\varphi,t|t-1}(x_{t-1},x_t) = c_{\varphi,t}(x_t)\phi_{\varphi,t-1}(x_{t-1})m_{\varphi,t|t-1}(x_{t-1},x_t)$. In this decomposition, $q_{\varphi,t-1|t}$ aims at approximating the true prior kernel of the hidden chain and $\phi_{\varphi,t}$ the true filtering distribution at time $t$. In this setting, the ELBO writes
\begin{align*}
\mathcal{L}(\theta,\varphi) &= \mathbb{E}_{q_{\varphi,0:T}}\left[\log \frac{p_{\theta,0:T}(X_{0:T},Y_{0:T})}{q_{\varphi,0:T}(X_{0:T})}\right]\\
&= -\sum_{t=1}^T\mathbb{E}_{q_{\varphi,0:T}}\left[\log c_{\varphi,t}(x_t)\right] + \sum_{t=1}^T\mathbb{E}_{q_{\varphi,0:T}}\left[\log \frac{p_\theta(x_{t-1},x_t)g_\theta(y_t|x_t)}{ \phi_{\varphi,t-1}(X_{t-1})m_{\varphi,t|t-1}(X_{t-1},X_t) }\right]\,,
\end{align*}
with $p_\theta(x_{-1},x_0) = \nu_\theta(x_0)$.
\end{itemize}

\subsection*{Limiting ELBO}
\textcolor{red}{A modifier plus tard, on pourra avoir des bornes qui d\'ependent du temps et des $Y_t$. On v\'erifie d'abord que tout passe sous les hypoth\`eses fortes.}
\begin{assumptionA}
\label{assum:transition:hmm}
$\mathrm{inf}_{\theta\in\Theta}\mathrm{inf}_{x,x'\in\Xset} m_\theta(x,x') = \sigma_->0$ and $\mathrm{sup}_{\theta\in\Theta}\mathrm{sup}_{x,x'\in\Xset} m_\theta(x,x') = \sigma_+<\infty$. 
\end{assumptionA}
\begin{assumptionA}
\label{assum:transition:variational}
For all $t\geq 1$, $y_{0:t-1}$, $\mathrm{inf}_{\phi\in\Phi}\mathrm{inf}_{x,x'\in\Xset} q^{y_{0:t-1}}_{\phi,t-1|t}(x,x') = \varpi_->0$ and $\mathrm{sup}_{\phi\in\Phi}\mathrm{sup}_{x,x'\in\Xset} q^{y_{0:t-1}}_{\phi,t-1|t}(x,x') = \varpi_+<\infty$. 
\end{assumptionA}
\textcolor{red}{On peut se passer de A3 a priori}
\begin{assumptionA}
\label{assum:bound:likelihood}
For all $y$, $\mathrm{inf}_{\theta\in\Theta}\mathrm{inf}_{x\in\Xset} g_\theta^y(x) = c_-(y)>0$ and $\mathrm{sup}_{\theta\in\Theta}\mathrm{sup}_{x\in\Xset} g_\theta^y(x)  = c_+(y)<\infty$. 
\end{assumptionA}
For all $t\geq 1$, consider the joint variational distribution
$$
q^{y_{0:t}}_{\varphi,0:t}(x_{0:t}) = \phi^{y_{0:t}}_{\varphi,t}(x_t)\prod_{s=1}^tq^{y_{0:s-1}}_{\varphi,s-1|s}(x_s,x_{s-1})\,,
$$
and, using the backward decomposition of the joint smoothing distribution,
$$
\phi^{y_{0:t}}_{\theta,0:t}(x_{0:t}) = \phi^{y_{0:t}}_{\theta,t}(x_t)\prod_{s=1}^tb^{y_{0:s-1}}_{\theta,s-1|s}(x_s,x_{s-1})\,,
$$
where $\phi^{y_{0:t}}_{\theta,t}$ is the filtering distribution at time $t$, i.e. the distribution of $X_t$  conditionally to $Y_{0:t}$ evaluated at $y_{0:t}$ and $b^{y_{0:s-1}}_{\theta,s-1|s}$ is the backward kernel at time $s$:
\begin{equation}
\label{eq:backwardk}
b^{y_{0:s-1}}_{\theta,s-1|s}(x_s,x_{s-1}) \propto \phi^{y_{0:s-1}}_{\theta,s-1}(x_{s-1})m_\theta(x_{s-1},x_s)\,.
\end{equation}
The ELBO writes
$$
\frac{1}{t}\mathcal{L}^{y_{0:t}}(\theta,\varphi)= \frac{1}{t}\mathbb{E}_{\phi^{y_{0:t}}_{\varphi,t}}\left[\log \frac{\phi^{y_{0:t}}_{\theta,t}(X_{t})}{q^{y_{0:t}}_{\varphi,t}(X_{t})}\right] + \frac{1}{t}\sum_{s=1}^t\mathbb{E}_{q^{y_{0:t}}_{\varphi,0:t}}\left[\log \frac{b^{y_{0:s-1}}_{\theta,s-1|s}(X_{s-1},X_s)}{q^{y_{0:s-1}}_{\varphi,s-1|s}(X_{s-1},X_s)}\right]\,.
$$

\begin{lemma}
\label{lem:forgetting:elbo}
Assume that A\ref{assum:transition:variational} holds. For all bounded measurable functions  $h$, all $\theta\in\Theta$, $\varphi\in\Phi$, all $0\leq s\leq t$, and all $1\leq u\leq s$,
$$
\left| \mathbb{E}_{q^{y_{0:t}}_{\varphi,0:t}}\left[h(X_{u-1},X_u)\right] - \mathbb{E}_{q^{y_{0:s}}_{\varphi,0:s}}\left[h(X_{u-1},X_u)\right]\right|\leq \left(1-\varpi_-/\varpi_+\right)^{s-u}\|h\|_\infty\,.
$$
\end{lemma}

\begin{proof}
Define the probability density function $q_{\varphi,s|t}$ by:
$$
q^{y_{0:t}}_{\varphi,s|t}: x_s \mapsto \int   \phi^{y_{0:t}}_{\varphi,t}(x_t)\prod_{r=s+1}^tq^{y_{0:r-1}}_{\varphi,r-1|r}(x_r,x_{r-1}) \rmd x_{s+1:t}\,,
$$
i.e. $q^{y_{0:t}}_{\varphi,s|t}$ is the marginal density of $X_s$ under $q^{y_{0:t}}_{\varphi,0:t}$. Then,
$$
\mathbb{E}_{q^{y_{0:t}}_{\varphi,0:t}}\left[h(X_{u-1},X_u)\right]  = \int q^{y_{0:t}}_{\varphi,s|t}(x_s)\prod_{r = u+1}^{s}q^{y_{0:r-1}}_{\varphi,r-1|r}(x_r,x_{r-1})  h(x_{u-1},x_u)\prod_{r = 1}^{u}q^{y_{0:r-1}}_{\varphi,r-1|r}(x_r,x_{r-1}) \rmd x_{0:s}\,.
$$
Therefore, under A\ref{assum:transition:variational},
$$
\left| \mathbb{E}_{q^{y_{0:t}}_{\varphi,0:t}}\left[h(X_{u-1},X_u)\right] - \mathbb{E}_{q^{y_{0:s}}_{\varphi,0:s}}\left[h(X_{u-1},X_u)\right]\right|\leq \left(1-\varpi_-/\varpi_+\right)^{s-u}\|\tilde h_{u}\|_\infty\,,
$$
where $\tilde h_{u}: x_{u}\mapsto \int  h(x_{u-1},x_u)\prod_{r = 1}^{u}q^{y_{0:r-1}}_{\varphi,r-1|r}(x_r,x_{r-1}) \rmd x_{0:u-1}$. The proof is completed by noting that
$ \|\tilde h_{u}\|_\infty \leq \|h\|_\infty$.
\end{proof}

\begin{lemma}
\label{lem:bound:filter}
Assume that A\ref{assum:transition:hmm} and A\ref{assum:bound:likelihood} hold. For all $\theta\in\Theta$,  all $t\geq 0$, all $y_{0:t}$,
$$
\sigma_- c_-(y_t)/c_+(y_t)\leq \phi^{y_{0:t}}_{\theta,t}(x_{t})\leq \sigma_+ c_+(y_t)/c_-(y_t)\,.
$$
\end{lemma}
\begin{proof}
At time 0, we have $\phi^{y_{0}}_{\theta,0}(x_{0}) \propto \mu(x_0)g^{y_0}_\theta(x_0)$, so that $\sigma_- c_-(y_t)/c_+(y_t)\leq \phi^{y_{0}}_{\theta,0}(x_{0})\leq \sigma_+ c_+(y_t)/c_-(y_t)$. Similarly, 
$$
\phi^{y_{0:t}}_{\theta,t}(x_{t}) \propto g^{y_t}_\theta(x_t)\int \phi^{y_{0:t-1}}_{\theta,t-1}(x_{t-1})m_\theta(x_{t-1},x_t)\rmd x_{t-1}\,,
$$
so that $\sigma_- c_-(y_t)/c_+(y_t)\leq \phi^{y_{0:t}}_{\theta,t}(x_{t})\leq \sigma_+ c_+(y_t)/c_-(y_t)$.
\end{proof}

By Lemma~\ref{lem:forgetting:elbo} and Lemma~\ref{lem:bound:filter}, using that
$$
\left|\log \frac{b^{y_{0:s-1}}_{\theta,s-1|s}(x_{s-1},x_s)}{q^{y_{0:s-1}}_{\varphi,s-1|s}(x_{s-1},x_s)}\right|\leq \frac{\left|\log b^{y_{0:s-1}}_{\theta,s-1|s}(x_{s-1},x_s) - \log q^{y_{0:s-1}}_{\varphi,s-1|s}(x_{s-1},x_s)\right|}{b^{y_{0:s-1}}_{\theta,s-1|s}(x_{s-1},x_s)\wedge q^{y_{0:s-1}}_{\varphi,s-1|s}(x_{s-1},x_s)}\,,
$$
$\{\mathbb{E}_{q_{\varphi,0:t}}[\log b_{\theta,s-1|s}(X_{s-1},X_s)/q_{\varphi,s-1|s}(X_{s-1},X_s)]\}_{t\geq 1}$ is a uniform Cauchy sequence. Therefore, we can write
$$
\lim_{t\to\infty}\mathbb{E}_{q_{\varphi,0:t}}[\log b_{\theta,s-1|s}(X_{s-1},X_s)/q_{\varphi,s-1|s}(X_{s-1},X_s)] = \delta^{\theta,\varphi}_{\infty,s}(\mathsf{Y}_{0:\infty})\,,
$$  uniformly almost surely.
%$\{\mathbb{E}_{q_{\varphi,0:t}}[\log b_{\theta,s-1|s}(X_{s-1},X_s)/q_{\varphi,s-1|s}(X_{s-1},X_s})]\}$

\begin{lemma}
\label{lem:bound:limit}
Assume that A\ref{assum:transition:variational} holds. For all all $\theta\in\Theta$, $\varphi\in\Phi$, all $s\geq 0$, and all $1\leq u\leq s$,
$$
\left| \delta^{\theta,\varphi}_{\infty,s}(\mathsf{Y}_{0:\infty}) - \mathbb{E}_{q_{\varphi,0:s}}\left[\log \frac{b_{\theta,u-1|u}(X_{u-1},X_u)}{q_{\varphi,u-1|u}(X_{u-1},X_u)}\right] \right|\leq \left(1-\varpi_-/\varpi_+\right)^{s-u}\|h\|_\infty\,.
$$
\end{lemma}
\begin{proof}
By Lemma~\ref{lem:forgetting:elbo},
$$
\left| \mathbb{E}_{q_{\varphi,0:t}}\left[\log \frac{b_{\theta,u-1|u}(X_{u-1},X_u)}{q_{\varphi,u-1|u}(X_{u-1},X_u)}\right] - \mathbb{E}_{q_{\varphi,0:s}}\left[\log \frac{b_{\theta,u-1|u}(X_{u-1},X_u)}{q_{\varphi,u-1|u}(X_{u-1},X_u)}\right]\right|\leq \left(1-\varpi_-/\varpi_+\right)^{s-u}\|h\|_\infty\,.
$$
Then, when $t$ grows to $\infty$, this yields
$$
\left| \delta^{\theta,\varphi}_{\infty,u}(\mathsf{Y}_{0:\infty}) - \mathbb{E}_{q_{\varphi,0:s}}\left[\log \frac{b_{\theta,u-1|u}(X_{u-1},X_u)}{q_{\varphi,u-1|u}(X_{u-1},X_u)}\right]\right|\leq \left(1-\varpi_-/\varpi_+\right)^{s-u}\|h\|_\infty\,.
$$
\end{proof}
Then,
$$
\sum_{s=1}^t\mathrm{sup}_{\theta\in\Theta}\mathrm{sup}_{\phi\in\Phi}\left|\mathbb{E}_{q_{\varphi,0:t}}\left[\log \frac{b_{\theta,s-1|s}(X_{s-1},X_s)}{q_{\varphi,s-1|s}(X_{s-1},X_s)}\right]- \delta^{\theta,\varphi}_{\infty,s}(\mathsf{Y}_{0:\infty})\right| \leq \frac{\varpi_+}{\varpi_-}\,. 
$$
Since $\delta^{\theta,\varphi}_{\infty,s}(\mathsf{Y}_{0:\infty})$ is integrable, the ergodic theorem implies that $\pi$-almost surely and in $\mathrm{L}^1(\pi)$,
$$
\lim_{t\to \infty}\frac{1}{t}\mathcal{L}(\theta,\varphi) = \mathbb{E}\left[\delta^{\theta,\varphi}_{\infty,s}(\mathsf{Y}_{0:\infty})\right]
$$
%\begin{lemma}
%\label{lem:bound:limit}
%For all all $\theta\in\Theta$, $\varphi\in\Phi$, all $s\geq 0$, and all $1\leq u\leq s$,
%$$
%\left| \delta^{\theta,\varphi}_{\infty,s}(\mathsf{Y}_{0:\infty}) - - \mathbb{E}_{q_{\varphi,0:s}}\left[\log \frac{b_{\theta,u-1|u}(X_{u-1},X_u)}{q_{\varphi,u-1|u}(X_{u-1},X_u)}\right] \right|\leq \left(1-\varpi_-/\varpi_+\right)^{s-u}\|h\|_\infty\,.
%$$
%\end{lemma}
%\begin{proof}
%By Lemma~\ref{lem:forgetting:elbo},
%$$
%\left| \mathbb{E}_{q_{\varphi,0:t}}\left[\log \frac{b_{\theta,u-1|u}(X_{u-1},X_u)}{q_{\varphi,u-1|u}(X_{u-1},X_u)}\right] - \mathbb{E}_{q_{\varphi,0:s}}\left[\log \frac{b_{\theta,u-1|u}(X_{u-1},X_u)}{q_{\varphi,u-1|u}(X_{u-1},X_u)}\right]\right|\leq \left(1-\varpi_-/\varpi_+\right)^{s-u}\|h\|_\infty\,.
%$$
%Then, when $t$ grows to $\infty$, this yields
%$$
%\left| \delta^{\theta,\varphi}_{\infty,u}(\mathsf{Y}_{0:\infty}) - \mathbb{E}_{q_{\varphi,0:s}}\left[\log \frac{b_{\theta,u-1|u}(X_{u-1},X_u)}{q_{\varphi,u-1|u}(X_{u-1},X_u)}\right]\right|\leq \left(1-\varpi_-/\varpi_+\right)^{s-u}\|h\|_\infty\,.
%$$
%\end{proof}


\bibliographystyle{apalike}
\bibliography{variationalhmm}
 
\end{document}
