\documentclass{article}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{authblk,url}
\usepackage{amssymb,amsmath,amsthm,twoopt,xargs,mathtools,dsfont}
\usepackage{times,ifthen}
\usepackage{fancyhdr,xcolor}
\usepackage{hyperref}

\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\title{Variational inference based on backward decomposition for hidden Markov models}
\date{}

%\author[$\wr$]{Mathis Chagneux}
\author[$\dag$]{XXX}
%\affil[$\wr$]{{\small LTCI, T\'el\'ecom Paris, Institut Polytechnique de Paris, Palaiseau.}}

\affil[$\dag$]{{\small }}

\lhead{}
\rhead{}

\DeclareUnicodeCharacter{2212}{-}
\usepackage{geometry}
\pagestyle{fancy}


\newcommand{\retrokmodnorm}{\bar{\boldsymbol{\mathcal{L}}}^{\precpar}}
\newcommand{\xarb}{x^\ast}
\newcommand{\precparsp}{\mathcal{E}}
\newcommand{\fk}[2]{\mathbf{F}_{#1 | #2}}
\newcommand{\probmeas}[1]{\mathsf{M}_1(#1)}
\newcommand{\Xfd}{\mathcal{X}}
\newcommand{\uksymbol}{\ell}
\newcommandx{\bkmod}[2][1=]{ 
\ifthenelse{\equal{#1}{}}
{\kernel{B}_{#2}^\precpar}
{\kernel{B}_{#2}^\precpar}
}

\newcommand{\cev}[1]{\reflectbox{\ensuremath{\vec{\reflectbox{\ensuremath{#1}}}}}}
\newcommand{\ukmod}[1]{\mathbf{L}_{#1}^\precpar}
\newcommand{\shiftbwd}{\cev{\shiftsymbol}^{\precpar}}
\newcommand{\shiftfwd}{\vec{\shiftsymbol}^{\, \precpar}}
\newcommand{\shiftsymbol}{\Phi}
\newcommand{\precpar}{\varphi}
\newcommand{\intvect}[2]{\{ #1, #2 \}}
\newcommandx\tstatmod[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter^{\precpar}_{#2}}
	{\tau^{\precpar}_{#2}^{#1}}
}
\newcommand{\noshift}{\shiftsymbol^{\precpar}}
\newcommand{\udlow}{\sigma_-}
\newcommand{\udup}{\sigma_+}
\newcommand{\retrok}{\boldsymbol{\mathcal{L}}}
%\newcommand{\intvect}[2]{\llbracket #1, #2 \rrbracket}
\newcommandx{\bkw}[2][1=]{ 
\ifthenelse{\equal{#1}{}}
{\kernel{B}_{#2}}
{\kernel{B}_{#2}}
}
\newcommand{\retroknorm}{\bar{\boldsymbol{\mathcal{L}}}}
\newcommand{\ud}[1]{\uksymbol_{#1}} 
\newcommand{\nset}{\mathbb{N}}
\newcommand{\nsetpos}{\mathbb{N}_{> 0}}
\newcommand{\1}{\mathbbm{1}} 
\newcommandx{\postmod}[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\phi_{#2}^\varphi}
	{\phi_{#2}^\N}
}
\newcommand{\retrokmod}{\boldsymbol{\mathcal{L}}^\precpar}
\newcommand{\uk}[1]{\mathbf{L}_{#1}}
\newcommand{\tensprod}{\otimes}
\def\dimX{d}
\def\dimY{m}
%\def\Xset{\mathsf{X}}
\newcommand{\Xset}{\mathsf{X}}
\def\Yset{\mathsf{Y}}
\newcommand{\mk}{\kernel{G}}
\newcommand{\hk}{\kernel{Q}}
\newcommand{\md}[1]{g_{#1}}
\newcommand{\SmoothFigSize}{0.27}

\newcommand{\logllh}[1]{\ell_{#1}}
\newcommand{\llh}[1]{\mathsf{L}_{#1}}
\newcommand{\testf}{\mathsf{h}}

\newcommandx\filtderiv[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\eta_{#2}}
	{\eta_{#2}^\N}
}
\newcommand{\pred}[1]{\pi_{#1}}
\newcommand{\parvec}{\theta}
\newcommand{\parspace}{\Theta}
\newcommand{\tstatletter}{\kernel{T}}
%\newcommand{\retrok}{\kernel{D}}
\newcommandx\tstat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\tau_{#2}^{#1}}
}
\newcommandx\tstathat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\tstatletter_{#2}}
	{\widehat{\tau}_{#2}^{#1}}
}
\newcommand{\af}[1]{h_{#1}}
\newcommand{\deriv}{\nabla_{\parvec}}

\newcommand{\kernel}[1]{\mathbf{#1}}
\newcommand{\bmf}[1]{\set{F}(#1)}
\newcommand{\set}[1]{\mathsf{#1}}

\newcommandx{\bk}[2][1=]{
\ifthenelse{\equal{#1}{}}
{\overleftarrow{\kernel{Q}}_{#2}}
{\overleftarrow{\kernel{Q}}_{#2}^{#1}}
}

\newcommandx{\bkhat}[2][1=]{
\ifthenelse{\equal{#1}{}}
{\widehat{\kernel{Q}}_{#2}}
{\widehat{\kernel{Q}}_{#2}^{#1}}
}

\newcommand{\lk}{\kernel{L}}
\newcommand{\idop}{\operatorname{id}}
\newcommand{\hd}[1]{q_{#1}}
\newcommand{\hdhat}[1]{\widehat{q}_{#1}}


\newcommandx{\addf}[2][1=]{
\ifthenelse{\equal{#1}{}}{\termletter_{#2}}{\bar{h}_{#2 | #1}}
}
\newcommand{\addfc}[1]{\underline{\termletter}_{#1}}
\newcommand{\adds}[1]{\af{#1}}
\newcommand{\term}[1]{\termletter_{#1}}
\newcommand{\termletter}{\tilde{h}}
\newcommand{\N}{N}
\newcommand{\partpred}[1]{\pi_{#1}^\N}
\newcommand{\tstattil}[2]{\tilde{\tau}_{#2}^{#1}}
\newcommandx{\K}[1][1=]{
\ifthenelse{\equal{#1}{}}{{\kletter}}{{\widetilde{\N}^{#1}}}}
\newcommand{\hkup}{\bar{\varepsilon}}
\newcommand{\bi}[3]{J_{#1}^{(#2, #3)}}
\newcommand{\bihat}[3]{\widehat{J}_{#1}^{(#2, #3)}}

\newcommand{\kletter}{\widetilde{\N}}

\def\sigmaX{\mathcal{X}}
\def\sigmaY{\mathcal{Y}}
\def\1{\mathds{1}}
\def\pE{\mathbb{E}}
\def\pP{\mathbb{P}}
\def\plim{\overset{\pP}{\longrightarrow}}
\def\dlim{\Longrightarrow}
\def\gauss{\mathcal{N}}


\newcommand{\esssup}[2][]
{\ifthenelse{\equal{#1}{}}{\left\| #2 \right\|_\infty}{\left\| #2 \right\|^2_{\infty}}}


\newcommand{\swght}[2]{\ensuremath{\omega_{#1}^{#2}}}

\newtheorem{assumptionA}{\textbf{A}\hspace{-3pt}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\newcommand{\iid}{i.i.d.}

\newcommand{\smwght}[3]{\tilde{\omega}_{#1|#2}^{#3}}
\newcommand{\smwghtfunc}[2]{\tilde{\omega}_{#1|#2}}

\newcommand{\smpart}[3]{\ensuremath{\tilde{\xi}_{#1|#2}^{#3}}}
\def\aux{{\scriptstyle{\mathrm{aux}}}}
\newcommand{\bdm}{\mathsf{TwoFilt}_{bdm}}
\newcommand{\fwt}{\mathsf{TwoFilt}_{fwt}}

\newcommand{\kiss}[3][]
{\ifthenelse{\equal{#1}{}}{r_{#2|#3}}
{\ifthenelse{\equal{#1}{fully}}{r^{\star}_{#2|#3}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2|#3}}{\mathrm{erreur}}}}}

\newcommand{\chunk}[4][]%
{\ifthenelse{\equal{#1}{}}{\ensuremath{{#2}_{#3:#4}}}{\ensuremath{#2^#1}_{#3:#4}}
}

\newcommand{\kissforward}[3][]
{\ifthenelse{\equal{#1}{}}{p_{#2}}
{\ifthenelse{\equal{#1}{fully}}{p^{\star}_{#2}}
{\ifthenelse{\equal{#1}{smooth}}{\tilde{r}_{#2}}{\mathrm{erreur}}}}}

\newcommand{\instrpostaux}[1]{\ensuremath{\upsilon_{#1}}}
\newcommandx\post[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\phi_{#2}}
	{\phi_{#2}^\N}
}

\newcommandx\posthat[2][1=]{
\ifthenelse{\equal{#1}{}}
	{\widehat{\phi}_{#2}}
	{\widehat{\phi}_{#2}^\N}
}

\newcommand{\adjfunc}[4][]
{\ifthenelse{\equal{#1}{}}{\ifthenelse{\equal{#4}{}}{\vartheta_{#2|#3}}{\vartheta_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{smooth}}{\ifthenelse{\equal{#4}{}}{\tilde{\vartheta}_{#2|#3}}{\tilde{\vartheta}_{#2|#3}(#4)}}
{\ifthenelse{\equal{#1}{fully}}{\ifthenelse{\equal{#4}{}}{\vartheta^\star_{#2|#3}}{\vartheta^\star_{#2|#3}(#4)}}{\mathrm{erreur}}}}}

\newcommand{\XinitIS}[2][]
{\ifthenelse{\equal{#1}{}}{\ensuremath{\rho_{#2}}}{\ensuremath{\check{\rho}_{#2}}}}
\newcommand{\adjfuncforward}[1]{\vartheta_{#1}}
\newcommand{\rmd}{\ensuremath{\mathrm{d}}}
\newcommand{\eqdef}{\ensuremath{:=}}
\newcommand{\eqsp}{\;}
\newcommand{\ewght}[2]{\ensuremath{\omega_{#1}^{#2}}}
\newcommand{\ewghthat}[2]{\ensuremath{\widehat{\omega}_{#1}^{#2}}}
\newcommand{\epart}[2]{\ensuremath{\xi_{#1}^{#2}}}
\newcommand{\filt}[2][]%
{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\phi_{#2}}}{\ensuremath{\phi_{#1,#2}}}%
}
\newcommand{\Xinit}{\ensuremath{\chi}}
\newcommand{\sumwght}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\Omega_{#2}}}{\ensuremath{\Omega_{#2}^{(#1)}}}}
\newcommand{\sumwghthat}[2][]{%
\ifthenelse{\equal{#1}{}}{\ensuremath{\widehat{\Omega}_{#2}}}{\ensuremath{\widehat{\Omega}_{#2}^{(#1)}}}}

\newcounter{hypH}
\newenvironment{hypH}{\refstepcounter{hypH}\begin{itemize}
\item[{\bf H\arabic{hypH}}]}{\end{itemize}}

\newcommand{\marginalset}{\mathsf{U}}

\newcommand{\calF}[2]{\mathcal{F}_{#1}^{#2}}
\newcommand{\calG}[2]{\mathcal{G}_{#1}^{#2}}
\newcommand{\Uset}{\mathsf{U}}
\newcommand{\tcalF}[2]{\widetilde{\mathcal{F}}_{#1}^{#2}}
\newcommand{\tcalG}[2]{\widetilde{\mathcal{G}}_{#1}^{#2}}

\newcommand{\kernelmarg}{\mathbf{R}}

\newcommand{\pplim}{\overset{\pP}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\ddlim}{\overset{\mathcal{D}}{ \underset{\N \to \infty}{\longrightarrow}}}
\newcommand{\aslim}{\overset{\pP\mathrm{-a.s.}}{ \underset{\N \to \infty}{\longrightarrow}}}

\newcommand{\qg}[1]{\ell_{#1}}
\newcommand{\hatqg}[1]{\mathsf{\ell}_{#1}}

\newcommand{\sfd}{\mathsf{d}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\e}{\text{e}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\frob}{:}
\newcommand{\rme}{\mathrm{e}}
\newcommand{\vois}{\mathcal{V}}

\newcounter{example}[section]
\newenvironment{example}[1][]{\refstepcounter{example}\par\medskip
   \noindent \textbf{Example~\theexample:} \textit{#1} \text \rmfamily}{\medskip}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
Let $\parvec$ be a parameter lying in a $\Theta\subset \rset^q$ and consider a  \textit{state space model} where the hidden Markov chain  in $\rset^d$ is denoted by $(X_k)_{k\geqslant 0}$. The distribution of $X_0$ has density $\chi$ with respect to the Lebesgue measure and for all $0\leqslant k \leqslant n-1$, the conditional distribution of $X_{k+1} $ given $X_{0:k}$ has density $\hd{k+1;\parvec}(X_{k},\cdot)$, where $a_{u:v}$ is a short-hand notation for $(a_u,\ldots,a_v)$. 
It is assumed that this state  is partially observed  through an observation process $(Y_k)_{0\leqslant k \leqslant n}$ taking values in $\rset^m$. 
For all $0\leqslant k \leqslant n$, the distribution of $Y_k$ given $X_{0:n}$ depends on $X_k$ only and has density $\md{k;\parvec}(X_k,\cdot)$ with respect to the Lebesgue measure. 
In this context, for any pair of indices $0\leqslant k_1 \leqslant k_2 \leqslant n$, we define the \textit{joint smoothing distribution} as the conditional law of $X_{k_1:k_2}$ given $Y_{0:n}$. 
In this framework, the likelihood of the observations $\llh{n,\parvec}(Y_{0:n})$, which is  in general intractable, is
$$
\llh{n,\parvec}(Y_{0:n})  = \int \chi(x_0)\md{0;\parvec}(x_{0},Y_{0})\prod_{k=0}^{n-1}\qg{k;\parvec}(x_{k},x_{k+1})\rmd x_{0:n}\eqsp,
$$
 where, for all $0\leqslant k \leqslant n$ and all $\parvec\in\parspace$,
\begin{equation}
\label{eq:def:elln}
\qg{k;\parvec}(x_{k},x_{k+1}) = \hd{k+1;\parvec}(x_{k}, x_{k+1})\md{k+1;\parvec}(x_{k+1},Y_{k+1})\eqsp.
\end{equation}
In a large variety of situations, the loglikelihood of the observations is not available explicitly and obtaining approximate maximum likelihood estimators is computationally intensive, in particular in high dimensional settings. Standard approaches  aim at computing \textit{smoothing expectations of additive functionals} of the form $\pE \left[\af{0:n}(X_{0:n})\middle | Y_{0:n}\right]$,  where $\af{0:n}$ is an \textit{additive functional}, \textit{i.e.} a function from $\rset^{d \times (n + 1)}$ to $\rset^{d'}$ satisfying:
\begin{equation}
\label{eq:additive:functional}
\af{0:n}: x_{0:n} \mapsto \sum_{k=0}^{n-1}\addf{k}(x_{k},x_{k+1})\eqsp,
\end{equation}
where $\addf{k}:\rset^{d} \times \rset^{d}\to\rset^{d'}$.
Such expectations are the keystones of many common inference problems in state space models.

\begin{example}[EM algorithm.]
\label{ex:em:algorithm}
In the usual case when $\theta$ is unknown, the maximum likelihood estimator is $\widehat \parvec = \mathrm{argmax}_{\parvec\in\parspace}\eqsp\llh{n,\parvec}(Y_{0:n})$. Expectation Maximization based algorithms are appealing solutions to obtain an estimator of $\hat \parvec$.
The pivotal concept of the EM algorithm is that the intermediate quantity defined by
\begin{equation*}
\parvec\mapsto Q(\parvec,\parvec') = \pE_{\parvec'}\left[\sum_{k=0}^{n-1} \log \qg{k;\parvec}(X_{k}, X_{k+1})\middle | Y_{0:n}\right] 
\end{equation*}
may be used as a surrogate for $\llh{n}(\parvec)$ in the maximization procedure,  where $\pE_{\parvec'}$ is the expectation under the joint distribution of the latent states and the observations when the model is parameterized by $\parvec'$. 
\end{example}

\section{Variational backward decomposition}
In variational approaches, instead of trying to design Monte Carlo estimators of such expectations (or equivalently of the conditional distribution of the states given the observations), the conditional law of $X_{0:n}$ given $Y_{0:n}$ is approximated by choosing a candidate in a parametric family $\{q_\phi\}_{\phi \in \Phi}$, where $\Phi$ is a parameter set. Parameters are then estimated by maximizing the ELBO defined as:
$$
\mathcal{L}(\parvec,\phi) = \pE_{q_\phi}\left[\log \frac{p_\parvec(X_{0:n},Y_{0:n})}{q_\phi(X_{0:n}|Y_{0:n})}\right]\eqsp.
$$
Most works in the literature focus on mean field approximations which means that the family $\{q_\phi\}_{\phi \in \Phi}$ can be written as a product of independent distributions. However, such an assumption fails to hold in standard hidden Markov models which may lead to poor result in the approximation of the posterior distribution of some states given the observations. In this paper, we propose another solution to provide a decomposition of   $q_\phi$ which accounts for the hidden Markov structure of $p_\parvec$. Under the assumptions of this paper, conditionally on $\{Y_{0:n}\}$, the backward chain $(X_n,\ldots,X_0)$ is a Markov chain with inhomogeneous Markov transition kernels. This leads us to introduce a variational family where each $q_\phi$ is of the form,
$$
q_\varphi: x_{0:n} \mapsto q_{\varphi,n}(x_n|Y_{0:n})\prod_{k=0}^{n-1}q_{\varphi,k}(x_k|x_{k+1},Y_{0:k})\eqsp.
$$
The variational family parameterizes the backward kernel which naturally appears in the decomposition of the law of $X_{0:n}$ given $Y_{0:n}$. 

\section{Control of the variational error for additive functionals}
Let $(\Xset_n, \Xfd_n)_{n \in \nset}$ be a sequence of general state spaces and let, for all $n \in \nset$, $\uk{n}$ defined on  $\Xset_n \times \Xfd_{n + 1}$ be bounded kernels in the sense that $\sup_{x \in \Xset_n} \uk{n}(x, \Xset_{n + 1}) < \infty$. We will assume a dominated model where each kernel $\uk{n}$ has a kernel density $\ud{n}$ with respect to some $\sigma$-finite reference measure $\mu_{n + 1}$ on $\Xfd_{n + 1}$. Finally, let $\chi$ be some bounded measure on $\Xfd_0$. In the following, we denote state-space product sets and $\sigma$-fields by $\Xset^n \eqdef \Xset_0 \times \cdots \times \Xset_n$ and $\Xfd^n \eqdef \Xfd_0 \tensprod \cdots \tensprod \Xfd_n$, respectively, and consider probability measures  
\begin{equation} \label{eq:def:post}
\post{0:n}(\rmd x_{0:n}) \propto \chi(\rmd x_0) \prod_{m = 0}^{n - 1} \uk{m}(x_m, \rmd x_{m + 1}), \quad n \in \nset, 
\end{equation}
on these product spaces.\footnote{We will always use the standard convention $\prod_{\varnothing} = 1$, implying that $\post{0} \propto \chi$.} The generality of the model \eqref{eq:def:post} is striking. In the special case where each $\uk{n}$ can be decomposed as $\uk{n}(x_n, \rmd x_{n + 1}) = \md{n}(x_n) \, \hk_n(x_n, \rmd x_{n + 1})$ for some Markov kernel $\hk_n$ and some nonnegative potential function $\md{n}$, \eqref{eq:def:post} yields the \emph{Feynman-Kac path models} \cite{delmoral:2004}, which are applied in a large variety of scientific and engineering disciplines, including statistics, physics, biology, and signal processing. 
Given a sequence $(\addf{n})_{n \in \nset}$ of measurable functions $\addf{n} : \Xset_n \times \Xset_{n + 1} \to \rset$, the aim of this section is to control  the error between expectations of \emph{additive functionals}   
\begin{equation} \label{eq:add:func}
    h_n:  x_{0:n} \mapsto \sum_{m = 0}^{n - 1} \addf{m}(x_m, x_{m + 1})
\end{equation}
under the distribution flow $(\post{0:n})_{n \in \nset}$ and under the variational family.   

For all $n\in\nset$, write $\post{0:n}^{\varphi}$ the joint smoothing distribution provided by the variational family. Then,
$$
 \post{0:n}^{\varphi}(d x_{0:n})= q_{\varphi,n}(dx_n)\prod_{k=0}^{n-1}q_{\varphi,k}(x_{k+1},dx_k)\eqsp.
$$
This joint probability distribution can be written:
$$
 \post{0:n}^{\varphi}(d x_{0:n})= q_{\varphi,n}(dx_0)\prod_{k=0}^{n-1}m_{\varphi,k}(x_{k},dx_{k+1})\eqsp,
$$
where $m_{\varphi,k}(x_{k},dx_{k+1}) =  q_{\varphi,k+1}(dx_{k+1})q_{\varphi,k}(x_{k+1},dx_k)/q_{\varphi,k}(x_k)$.

\begin{hypH}
\label{assum:bias:bound}
There exists a function $c$ such that for all $n \in \mathbb{N}$, $\parvec$, $\varphi$, $h \in \bmf{\Xfd_n \tensprod \Xfd_{n + 1}}$, and $x \in \Xset_n$,   
$$
\left|\int m_{\varphi,n}(x,dx_{n+1}) h (x_{n+1})- \int\uk{n,\parvec}(x,dx_{n+1}) h (x_{n+1})\right| \leq c(\parvec,\varphi) \| h \|_\infty. 
$$
\end{hypH}


\begin{hypH}
\label{assum:strong:mixing}
There exist constants $0 < \udlow < \udup < \infty$ such that for all $k \in \nset$ and $(x_k, x_{k + 1}) \in \Xset_k \times \Xset_{k + 1}$, 
$$
    \udlow \leq \ud{k,\theta}(x_k, x_{k + 1}) \leq \udup
$$ 
and 
$$
    \udlow \leq \inf_{\varphi} m_{\varphi,k}(x_k, x_{k + 1}), \quad \sup_{\varphi} m_{\varphi,k}(x_k, x_{k + 1}) \leq \udup. 
$$ 
\end{hypH}
Under these assumptions, define the mixing rate 
\begin{equation} \label{eq:def:rho}
    \rho \eqdef 1 - \frac{\udlow}{\udup}
\end{equation}
Then for all $n \in \nset$, $\parvec$, $\varphi$, and $h_n$, 
    \begin{align*}
        \big| \postmod{0:n} h_n -  \post{0:n}^{\parvec} h_n \big| 
        &\leq 2 c(\parvec,\varphi) \frac{\udup}{\udlow^2} \sum_{k = 0}^{n - 1} \| \addf{k} \|_\infty \left( \sum_{m = 1}^{n - 1} \rho^{|k - m| - 1} + 1 \right) \\
        &\leq 2 c(\parvec,\varphi) n \frac{\udup}{\udlow^2} \left( 1 + \frac{1}{\rho} + \frac{2}{1 - \rho} \right) \sup_{0 \leq k \leq n - 1} \| \addf{k} \|_\infty\eqsp.
    \end{align*}

\begin{proof}
We preface the proof by a few technical lemmas. For each $m \in \nset$, define the  backward Markov kernel 
\begin{equation} \label{eq:def:backward:kernel}
    \bkw{m}^\parvec(x_{m + 1}, \rmd x_m) \eqdef \frac{\post{m}^\parvec(\rmd x_m) \, \ud{m,\parvec}(x_m, x_{m + 1})}{\post{m}^\parvec[\ud{m,\parvec}(\cdot, x_{m + 1})]} %= \frac{\post{m}^\parvec(\rmd x_m) \, \hd{m;\parvec}(x_{m}, x_{m+1})}{\post{m}^\parvec[\hd{m;\parvec}(\cdot, x_{m+1})]}
\end{equation}
on $\Xset_{m + 1} \times \Xfd_m$. In addition, for each $n \in \nsetpos$, let the Markov kernel   
\begin{equation} \label{eq:def:tstat}
\tstat{n}^\parvec(x_n, \rmd x_{0:n - 1}) \eqdef \prod_{m = 0}^{n - 1} \bkw{m}^\parvec(x_{m + 1}, \rmd x_m)
\end{equation}
on $\Xset_n \times \Xfd^{n - 1}$ denote the joint law of the backward Markov chain induced by the kernels \eqref{eq:def:backward:kernel} when initialised at $x_n \in \Xset_n$.   In the following, the notation $\parvec$ may be dropped when there is no confusion.

Define, for each $n \in \nset$ and $m \in \intvect{0}{n}$, the kernel 
\begin{equation} \label{eq:def:uk:products}
    \uk{m, n}(x_{0:m}', \rmd x_{0:n + 1}) \eqdef \delta_{x_{0:m}'}(\rmd x_{0:m}) \prod_{\ell = m}^n \uk{\ell}(x_\ell, \rmd x_{\ell + 1}) 
\end{equation}
on $\Xset^m \times \Xfd^{n + 1}$. In addition, let $\uk{n, n - 1} = \operatorname{id}$. Note that $\uk{n, n}$ is different from $\uk{n}$ in the sense that the former is defined on $\Xset^n \times \Xfd^{n + 1}$ whereas the latter is defined on $\Xset_n \times \Xfd_{n + 1}$. Define similarly, $\bkw{m}^\varphi(x_{m + 1}, \rmd x_m) = q_{\varphi,m}(x_{m+1},dx_m)$
on $\Xset_{m + 1} \times \Xfd_m$. In addition, define for each $n \in \nsetpos$, the Markov kernel  $\tstat{n}^\varphi(x_n, \rmd x_{0:n - 1}) = \prod_{m = 0}^{n - 1} \bkw{m}^\varphi(x_{m + 1}, \rmd x_m)$ on $\Xset_n \times \Xfd^{n - 1}$ denote the joint law of the backward Markov chain induced by the kernels \eqref{eq:def:backward:kernel} when initialised at $x_n \in \Xset_n$.  In the following, the notation $\parvec$ may be dropped when there is no confusion. 
%\begin{itemize} 
%\item[(i)]
%For all $n \in \nset$ and $h \in \bmf{\Xfd_n \tensprod \Xfd_{n + 1}}$, 
%\begin{equation} \label{eq:reversibility}
%\iint \post{n}^\parvec(\rmd x_n) \, \uk{n,\parvec}(x_n, \rmd x_{n + 1}) \, h(x_n, x_{n + 1}) = \iint \post{n}^\parvec \uk{n,\parvec}(\rmd x_{n + 1}) \, \bkw{n}^\parvec(x_{n + 1}, \rmd x_n) \, h(x_n, x_{n + 1}). 
%\end{equation}
%\item[(ii)]
%For all $n \in \nsetpos$ and $h \in \bmf{\Xfd^n}$, 
%$
%\post{0:n}^\parvec h = \post{n}^\parvec \tstat{n}^\parvec h. 
%$
%\end{itemize}
 For each $n \in \nset$ and $m \in \intvect{0}{n}$, define the kernel  
\begin{equation} \label{eq:def:retrokmod}
    \retrokmod_{m, n}(x_m', \rmd x_{0:n}) \eqdef \delta_{x_m'}(\rmd x_m) \, \tstatmod{m}(x_m, \rmd x_{0:m - 1}) \prod_{\ell = m}^{n - 1} \uk{\ell}(x_\ell, \rmd x_{\ell + 1}), 
\end{equation}
on $\Xset_m \times \Xfd^n$. 

\begin{lemma} \label{lem:retro:prospective:id}
For all $m \in \intvect{0}{n}$ and $h \in \bmf{\Xfd^n}$, 
\begin{equation} \label{eq:retro:prospective:id}
\postmod{0:m} \uk{m, n - 1} h = \postmod{m} \retrokmod_{m, n} h.  
\end{equation}
\end{lemma}
The following probability measures play a key role in the following: for $h \in \bmf{\Xfd_m}$, 
\begin{align}
\noshift_{m, n} h &\eqdef \frac{\postmod{m} (h \times \retrokmod_{m, n} \1_{\Xset^n})}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}}, \nonumber \\
\shiftfwd_{m, n} h &\eqdef \frac{\postmod{m} (h \times \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}}, \nonumber \\
\shiftbwd_{m, n} h &\eqdef \frac{\postmod{m - 1} \uk{m - 1}(h \times \retrokmod_{m, n} \1_{\Xset^n})}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}.  \nonumber 
\end{align}
In addition, for each $k \in \intvect{0}{n - 1}$, let 
\begin{equation} \label{eq:def:addf}
\addf[n]{k} : \Xset^n \ni x_{0:n} \mapsto \addf{k}(x_k, x_{k + 1})  
\end{equation}
denote the extension of $\addf{k}$ to $\Xset^n$. 

\begin{lemma} \label{lemma:three:identities}
Let $n \in \nset$ and $m \in \intvect{1}{n}$. Then the following holds true for all $k \in \intvect{0}{n - 1}$. 
\begin{itemize}
\item[(i)]  
$
\displaystyle \noshift_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right) = \frac{\postmod{m} \retrokmod_{m, n} \addf[n]{k}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}}$ \quad for $m \in \intvect{1}{n}$, 
\item[(ii)]  
$
\displaystyle \shiftfwd_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right) = \frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{k}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}}
$ \quad for $m \in \intvect{k + 1}{n}$, and 
\item[(iii)] 
$
\displaystyle \shiftbwd_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right) = \frac{\postmod{m - 1} \retrokmod_{m - 1, n} \addf[n]{k}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}
$ \quad for all $m \in \intvect{1}{k}$. 
\end{itemize}
\end{lemma}

\begin{proof}
The identity (i) follows straightforwardly by the definition of $\noshift_{m, n}$. We hence turn to (ii), which is established by first noting that for all $m \in \intvect{k + 1}{n}$, 
$$
\frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} 
= \bkmod[\postmod{m - 1}]{m - 1} \cdots \bkmod[\postmod{k + 1}]{k + 1} (\bkmod[\postmod{k}]{k} \addf{k}).  
$$
Now, 
\begin{align}
\shiftfwd_{m, n} h 
&= \iint \frac{\postmod{m}(\rmd x_m) \, \ukmod{m}(x_m, \rmd x_{m + 1}) \, h(x_m) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} \nonumber \\
&= \iint \frac{\postmod{m} \ukmod{m}(\rmd x_{m + 1}) \, \bkmod[\postmod{m}]{m}(x_{m + 1}, \rmd x_m) \, h(x_m) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} \nonumber \\
&= \frac{\postmod{m + 1} (\bkmod[\postmod{m}]{m} h \times \retrokmod_{m + 1, n} \1_{\Xset^n})}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}}, \nonumber 
\end{align}
we may establish the identity by proceeding like  
\begin{align*}
\lefteqn{\shiftfwd_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right)} \\
&= \iint \frac{\postmod{m + 1}(\rmd x_{m + 1}) \, \bkmod[\postmod{m}]{m}(x_{m + 1}, \rmd x_m) \, \bkmod[\postmod{m - 1}]{m - 1}  
\cdots \bkmod[\postmod{k + 1}]{k + 1}(\bkmod[\postmod{k}]{k} \addf{k})(x_m) 
\retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} \\
&= \frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{k}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}}.  
\end{align*}

Finally, to check (iii), note that for all $m \in \intvect{1}{k}$, 
$$
\uk{m - 1} \retrokmod_{m, n} \addf[n]{k} = \retrokmod_{m - 1, n} \addf[n]{k}. 
$$
Thus, in this case 
$$
\shiftbwd_{m, n} \left( \frac{\retrokmod_{m, n} \addf[n]{k}}{\retrokmod_{m, n} \1_{\Xset^n}} \right) = \frac{\postmod{m - 1} \uk{m - 1} \retrokmod_{m, n} \addf[n]{k}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}} = \frac{\postmod{m - 1} \retrokmod_{m - 1, n} \addf[n]{k}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}. 
$$
\end{proof}

\begin{lemma} \label{lem:geo:bound}
Assume H\ref{assum:strong:mixing}. Then for all $n \in \nset$, $m \in \intvect{0}{n}$, $k \in \intvect{0}{n - 1}$, and $(\lambda, \lambda') \in \probmeas{\Xfd_m}^2$, 
$$
\left|\frac{\lambda \retrokmod_{m, n} \addf[n]{k}}{\lambda \retrokmod_{m, n} \1_{\Xset^n}} - \frac{\lambda' \retrokmod_{m, n} \addf[n]{k}}{\lambda' \retrokmod_{m, n} \1_{\Xset^n}} \right| \leq \| \addf{k} \|_\infty \rho^{|k - m| - 1}. 
$$
\begin{proof}
First, assume that $m \leq k$; then note that for all $x_m \in \Xset_m$, 
\begin{multline} \label{eq:retrokmodnorm:vs:forward:kernel}
\frac{\retrokmod_{m, n} \addf[n]{k}(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)} - \frac{\retrokmod_{m, n} \addf[n]{k}(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)}
 = \fk{m}{n} \cdots \fk{k - 1}{n} (\fk{k}{n} \addf{k})(x_m) \\ 
- \fk{m}{n} \cdots \fk{k - 1}{n} (\fk{k}{n} \addf{k})(x_m),    
\end{multline}

where we have introduced the \emph{forward kernels}
$$
\fk{m}{n} h (x_m) \eqdef \frac{\uk{m}(h \times \retrokmod_{m  + 1, n} \1_{\Xset^n})(x_m)}{\retrokmod_{m, n} \1_{\Xset^n} (x_m)}, \quad x_m \in \Xset_m, \quad h \in \bmf{\Xfd_{m + 1}}. 
$$
Under H\ref{assum:strong:mixing}, each forward kernel satisfies a global Doeblin condition in the form of the uniform lower bound 
$$
\fk{m}{n} h (x_m) \geq \frac{\udlow}{\udup} \mu_{m, n} h,  
$$
where we have defined the probability measure 
$$
\mu_{m, n} h \eqdef \frac{\mu_{m + 1}(h \times \retrokmod_{m  + 1, n} \1_{\Xset^n})}{\mu_{m + 1} \retrokmod_{m  + 1, n}  \1_{\Xset^n}}, \quad h \in \bmf{\Xfd_{m + 1}}. 
$$
Thus, by standard results for uniformly minorised Markov chains (see, \emph{e.g.}, \cite[Lemma~4.3.13]{Cappe:2005:IHM:1088883}), the Dobrushin coefficient of each $\fk{m}{n}$ is bounded by $\rho = 1 - \udlow / \udup$. Thus, \eqref{eq:retrokmodnorm:vs:forward:kernel} implies that 
\begin{align*}
\left| \frac{\lambda \retrokmod_{m, n} \addf[n]{k}}{\lambda \retrokmod_{m, n} \1_{\Xset^n}} - \frac{\lambda' \retrokmod_{m, n} \addf[n]{k}}{\lambda' \retrokmod_{m, n} \1_{\Xset^n}} \right| 
&= \left| (\lambda_{m, n} - \lambda_{m, n}') \fk{m}{n} \cdots \fk{k - 1}{n} (\fk{k}{n} \addf{k})
\right| \\
&\leq \rho^{k - m} \| \fk{k}{n} \addf{k} \|_\infty \leq \rho^{k - m} \| \addf{k} \|_\infty, 
\end{align*}
where for $h \in \bmf{\Xfd_m}$, 
$$
\lambda_{m, n} h \eqdef \frac{\lambda(h \times \retrokmod_{m, n} \1_{\Xset^n})}{\lambda \retrokmod_{m, n} \1_{\Xset^n}}, \quad 
\lambda_{m, n}' h \eqdef \frac{\lambda'(h \times \retrokmod_{m, n} \1_{\Xset^n})}{\lambda' \retrokmod_{m, n} \1_{\Xset^n}}. 
$$
Now, assume that $m > k$; then note that for all $x_m \in \Xset_m$, 
\begin{multline} \label{eq:retrokmodnorm:vs:backward:kernel}
\frac{\retrokmod_{m, n} \addf[n]{k}(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)} - \frac{\retrokmod_{m, n} \addf[n]{k}(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)} \\
 = \bkmod{m - 1} \cdots \bkmod{k + 1}(\bkmod{k} \addf{k})(x_m) - \bkmod{m - 1} \cdots \bkmod{k + 1}(\bkmod{k} \addf{k})(x_m).     
\end{multline}
Under H\ref{assum:strong:mixing}, also each backward kernel satisfies a Doeblin condition, namely   
$$
\bkmod{m} h (x_{m + 1}) \geq \frac{\udlow}{\udup} \postmod{m} h,  
$$
with the marginal $\postmod{m}$ playing the role as minorising measure. Thus, the backward kernel Dobrushin coefficients are bounded by the same constant $\rho$, implying, via \eqref{eq:retrokmodnorm:vs:backward:kernel}, that 
$$
\left| \frac{\lambda \retrokmod_{m, n} \addf[n]{k}}{\lambda \retrokmod_{m, n} \1_{\Xset^n}} - \frac{\lambda' \retrokmod_{m, n} \addf[n]{k}}{\lambda' \retrokmod_{m, n} \1_{\Xset^n}} \right| 
= \left| (\lambda_{m, n} - \lambda_{m, n}') \bkmod{m - 1} \cdots \bkmod{k + 1}(\bkmod{k} \addf{k}) \right| \leq \rho^{m - k - 1} \| \addf{k} \|_\infty. 
$$
This completes the proof. 
\end{proof}
\end{lemma}

\begin{lemma} \label{lem:diff:bound} 
Assume H\ref{assum:strong:mixing} and H\
ref{assum:bias:bound}. Then the following holds. 
\begin{itemize}
\item[(i)] For all $n \in \nset$, $m \in \intvect{0}{n}$, $\precpar \in \precparsp$, and $h \in \bmf{\Xfd^n}$,   
$$
\left| \noshift_{m, n} h - \shiftbwd_{m, n} h \right| \vee \left| \shiftfwd_{m, n} h - \noshift_{m, n} h \right| \leq 2 c(\theta,\varphi)  \frac{\udup}{\udlow^2} \| h \|_\infty.   
$$
\item[(ii)] For all $n \in \nset$, $m \in \intvect{0}{n - 1}$, and $\precpar \in \precparsp$,
$$
\left| \frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{m}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} - \frac{\postmod{m} \retrokmod_{m, n} \addf[n]{m}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}} \right| \leq 2 c(\theta,\varphi)  \frac{\udup}{\udlow^2} \| h_m \|_\infty. 
$$
\end{itemize}
In both cases, $c(\theta,\varphi)$ is the constant in H
\ref{assum:bias:bound}. 
\end{lemma}

\begin{proof}
We start with (i). Combining the decomposition 
\begin{multline*}
 \noshift_{m, n} h - \shiftbwd_{m, n} h = \varphi_{m, n}^\precpar h \left( \frac{\postmod{m - 1} \uk{m - 1} \retrokmod_{m, n} \1_{\Xset^n} - \postmod{m - 1} \ukmod{m - 1} \retrokmod_{m, n} \1_{\Xset^n}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}} \right) \\
+ \frac{\postmod{m - 1} \ukmod{m - 1}(h \times \retrokmod_{m, n} \1_{\Xset^n}) - \postmod{m - 1} \uk{m - 1}(h \times \retrokmod_{m, n} \1_{\Xset^n})}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}} 
\end{multline*}
with H\ref{assum:bias:bound} provides the bound 
$$
\left| \noshift_{m, n} h - \shiftbwd_{m, n} h \right| \leq 2 c(\theta,\varphi) \frac{\| \retrokmod_{m, n} \1_{\Xset^n} \|_\infty  \| h \|_\infty}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}.
$$
 Now, since for all $x_m \in \Xset_m$,  
\begin{equation*}
\retrokmod_{m, n} \1_{\Xset^n}(x_m) = \int \ud{m}(x_m, x_{m + 1}) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1}) \, \mu_{m + 1}(\rmd x_{m + 1}) \\
\leq \udup \mu_{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n} 
\end{equation*}
and 
\begin{align*}
\lefteqn{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}} \\
&= \iint \postmod{m - 1}(\rmd x_{m - 1}) \, \ud{m - 1}(x_{m - 1}, x_m) \ud{m}(x_m, x_{m + 1}) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1}) \, \mu_m \tensprod \mu_{m + 1}(\rmd x_{m:m + 1}) \\ 
&\geq \udlow^2 \mu_{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}, 
\end{align*}
implying the bound
\begin{equation} \label{eq:retrokmod:ratio:bd}
 \frac{\| \retrokmod_{m, n} \1_{\Xset^n} \|_\infty}{\postmod{m - 1} \uk{m - 1} \retrokmod_{m, n} \1_{\Xset^n}} \leq \frac{\udup}{\udlow^2}, 
\end{equation}
we may conclude that 
$| \noshift_{m, n} h - \shiftbwd_{m, n} h| \leq 2 c(\theta,\varphi) \udup \| h \|_\infty / \udlow^2
$. 
Along similar lines, the second difference can be bounded by the same quantity by writing   
\begin{multline*}
\shiftfwd_{m, n} h - \noshift_{m, n} h = \shiftfwd_{m, n} h \left( \frac{\postmod{m} \uk{m} \retrokmod_{m + 1, n} \1_{\Xset^n} - \postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset_n}}{\postmod{m} \uk{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} \right) \\
+ \frac{\postmod{m} (h \times \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}) - \postmod{m} (h \times \uk{m} \retrokmod_{m + 1, n} \1_{\Xset_n})}{\postmod{m} \uk{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} 
\end{multline*}
and reapplying H\ref{assum:bias:bound} and \eqref{eq:retrokmod:ratio:bd}.  We turn to (ii). By definition \eqref{eq:def:retrokmod},  
$$
\retrokmod_{m + 1, n} \addf[n]{m}(x_{m + 1}) = \int \addf{m}(x_m, x_{m + 1}) \, \bkmod{m}(x_{m + 1}, \rmd x_m) \, \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1});
$$
thus,
\begin{align}
\frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{m}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} &= \iint \frac{\postmod{m} \ukmod{m}(\rmd x_{m + 1}) \, \addf{m}(x_m, x_{m + 1}) \, \bkmod{m}(x_{m + 1}, \rmd x_m) \, \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}} \nonumber \\
&= \iint \frac{\postmod{m}(\rmd x_m) \, \ukmod{m}(x_m, \rmd x_{m + 1}) \, \addf{m}(x_m, x_{m + 1}) \retrokmod_{m + 1, n} \1_{\Xset^n}(x_{m + 1})}{\postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}}. \nonumber
\end{align}
We may thus decompose the quantity under consideration as  
\begin{multline*}
\frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{m}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} - \frac{\postmod{m} \retrokmod_{m, n} \addf[n]{m}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}}
= \frac{\postmod{m + 1} \retrokmod_{m + 1, n} \addf[n]{m}}{\postmod{m + 1} \retrokmod_{m + 1, n} \1_{\Xset^n}} \left( \frac{\postmod{m} \uk{m} \retrokmod_{m + 1, n} \1_{\Xset^n} - \postmod{m} \ukmod{m} \retrokmod_{m + 1, n} \1_{\Xset^n}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}} \right) \\
+ \int \frac{\postmod{m}(\rmd x_m) \{ \ukmod{m}(\addf{m} \retrokmod_{m + 1, n} \1_{\Xset^n})(x_m) - \uk{m} (\addf{m} \retrokmod_{m + 1, n} \1_{\Xset^n})(x_m)\}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}} ,  
\end{multline*}
from which (ii) follows, as before, by a combination of H\ref{assum:bias:bound} and \eqref{eq:retrokmod:ratio:bd}.  
\end{proof}



Write 
$$
\postmod{0:n} h_n - \post{0:n} h_n = \sum_{k = 0}^{n - 1} \left( \postmod{0:n} \addf[n]{k} - \post{0:n} \addf[n]{k} \right), 
$$
where each term can be decomposed according to   
\begin{equation*}
\postmod{0:n} \addf[n]{k} - \post{0:n} \addf[n]{k} = 
\sum_{m = 1}^n \left( \frac{\postmod{0:m} \uk{m, n - 1} \addf[n]{k}}{\postmod{0:m} \uk{m, n - 1} \1_{\Xset^n}} - \frac{\postmod{0:m - 1} \uk{m - 1, n - 1} \addf[n]{k}}{\postmod{0:m - 1} \uk{m - 1, n - 1} \1_{\Xset^n}} \right).  
\end{equation*}
In order to bound each term of this decomposition, write, using Lemma~\ref{lem:retro:prospective:id},  
$$
\frac{\postmod{0:m} \uk{m, n - 1} \addf[n]{k}}{\postmod{0:m} \uk{m, n - 1} \1_{\Xset^n}} - \frac{\postmod{0:m - 1} \uk{m - 1, n - 1} \addf[n]{k}}{\postmod{0:m - 1} \uk{m - 1, n - 1} \1_{\Xset^n}} 
= \frac{\postmod{m} \retrokmod_{m, n} \addf[n]{k}}{\postmod{m} \retrokmod_{m, n} \1_{\Xset^n}} - \frac{\postmod{m - 1} \retrokmod_{m - 1, n} \addf[n]{k}}{\postmod{m - 1} \retrokmod_{m - 1, n} \1_{\Xset^n}}. 
$$
Now, for all $m \in \intvect{1}{n}$, pick an arbitrary element $\xarb_m \in \Xset_m$ and define the kernel 
\begin{equation} \label{eq:def:norm:objective:func}
\retrokmodnorm_{m, n} h(x_m) \eqdef \frac{\retrokmod_{m, n} h(x_m)}{\retrokmod_{m, n} \1_{\Xset^n}(x_m)} - \frac{\retrokmod_{m, n} h (\xarb_m)}{\retrokmod_{m, n} \1_{\Xset^n} (\xarb_m)}, \quad x_m \in \Xset_m, \quad h \in \bmf{\Xfd^n}. 
\end{equation}
Combining this definition with Lemma~\ref{lemma:three:identities}, we may express the quantity of interest as 
\begin{multline*}
\postmod{0:n} \addf[n]{k} - \post{0:n} \addf[n]{k} = \sum_{m = 1}^k \left( \noshift_{m, n} \retrokmodnorm_{m, n} \addf[n]{k} - \shiftbwd_{m, n}  \retrokmodnorm_{m, n} \addf[n]{k} \right) \\ 
+ \frac{\postmod{k + 1} \retrokmod_{k + 1, n} \addf[n]{k}}{\postmod{k + 1} \retrokmod_{k + 1, n} \1_{\Xset^n}} - \frac{\postmod{k} \retrokmod_{k, n} \addf[n]{k}}{\postmod{k} \retrokmod_{k, n} \1_{\Xset^n}} + \sum_{m = k + 1}^{n - 1} \left( \shiftfwd_{m, n} \retrokmodnorm_{m, n} \addf[n]{k} - \noshift_{m, n} \retrokmodnorm_{m, n} \addf[n]{k} \right). 
\end{multline*}
Now, applying Lemmas~\ref{lem:geo:bound} and \ref{lem:diff:bound} to the previous decomposition yields
$$
\left| \postmod{0:n} \addf[n]{k} - \post{0:n} \addf[n]{k} \right| \leq 2 c(\theta,\varphi) \frac{\udup }{\udlow^2} \sum_{k = 0}^{n - 1} \| \addf{k} \|_\infty \left( \sum_{m = 1}^{n - 1} \rho^{|k - m| - 1} + 1 \right).  
$$
which was to be established. 


\end{proof}

\clearpage
\newpage

For each $m \in \nset$, define the  backward Markov kernel 
\begin{equation} \label{eq:def:backward:kernel}
    \bkw{m}^\parvec(x_{m + 1}, \rmd x_m) \eqdef \frac{\post{m}^\parvec(\rmd x_m) \, \ud{m,\parvec}(x_m, x_{m + 1})}{\post{m}^\parvec[\ud{m,\parvec}(\cdot, x_{m + 1})]} = \frac{\post{m}^\parvec(\rmd x_m) \, \hd{m;\parvec}(x_{m}, x_{m+1})}{\post{m}^\parvec[\hd{m;\parvec}(\cdot, x_{m+1})]}
\end{equation}
on $\Xset_{m + 1} \times \Xfd_m$. In addition, for each $n \in \nsetpos$, let the Markov kernel   
\begin{equation} \label{eq:def:tstat}
\tstat{n}^\parvec(x_n, \rmd x_{0:n - 1}) \eqdef \prod_{m = 0}^{n - 1} \bkw{m}^\parvec(x_{m + 1}, \rmd x_m)
\end{equation}
on $\Xset_n \times \Xfd^{n - 1}$ denote the joint law of the backward Markov chain induced by the kernels \eqref{eq:def:backward:kernel} when initialised at $x_n \in \Xset_n$. 

\begin{itemize} 
\item[(i)]
For all $n \in \nset$ and $h \in \bmf{\Xfd_n \tensprod \Xfd_{n + 1}}$, 
\begin{equation} \label{eq:reversibility}
\iint \post{n}^\parvec(\rmd x_n) \, \uk{n,\parvec}(x_n, \rmd x_{n + 1}) \, h(x_n, x_{n + 1}) = \iint \post{n}^\parvec \uk{n,\parvec}(\rmd x_{n + 1}) \, \bkw{n}^\parvec(x_{n + 1}, \rmd x_n) \, h(x_n, x_{n + 1}). 
\end{equation}
\item[(ii)]
For all $n \in \nsetpos$ and $h \in \bmf{\Xfd^n}$, 
$
\post{0:n}^\parvec h = \post{n}^\parvec \tstat{n}^\parvec h. 
$
\end{itemize}
In the case of state-space models, the identity (ii) above is a well-known result typically referred to as the \emph{backward decomposition} of the joint-smoothing distribution. Importantly, as noted in \cite{cappe:2009}, the functions $(\tstat{n} h_n)_{n \in \nsetpos}$ can be expressed recursively through  
\begin{equation} \label{eq:forward:smoothing}
\tstat{n + 1}^\parvec h_{n + 1}(x_{n + 1}) = \int (\tstat{n}^\parvec h_n(x_n) + \addf{n}(x_n, x_{n + 1})) \, \bkw{n}^\parvec(x_{n + 1}, \rmd x_n), \quad n \in \nset, 
\end{equation}
with, by convention, $\tstat{0}^\parvec h_0 \equiv 0$. Here the backward kernel $\bkw{n}^\parvec$ depends on the marginal $\post{n}^\parvec$; thus, the recursion is driven by the marginal flow $(\post{n}^\parvec)_{n \in \nset}$, which may again be expressed recursively. However, as these marginals are, as already mentioned, generally intractable, exact computations need typically to be replaced by approximations. 

 For all $n\in\nset$, write $\post{0:n}^{\varphi}$ the joint smoothing distribution provided by the variational family. Then,
$$
\post{0:n}^\parvec h - \post{0:n}^{\varphi} h= \post{n}^\parvec \tstat{n}^\parvec h - \post{0:n}^{\varphi} h =  \post{n}^\parvec \left\{\prod_{m = 0}^{n - 1} \bkw{m}^\parvec\right\} h - q_{\varphi,n}\left\{\prod_{k=0}^{n-1}q_{\varphi,k}\right\} h\eqsp.
$$
Write 
$$
\postmod{0:n} h_n - \post{0:n}^{\varphi} h_n = \sum_{k = 0}^{n - 1} \left( \postmod{0:n} \addf[n]{k} - \post{0:n}^{\varphi} \addf[n]{k} \right), 
$$
where each term can be decomposed according to   
\begin{equation*}
\postmod{0:n} \addf[n]{k} - \post{0:n}^\varphi \addf[n]{k} = 
\sum_{m = 1}^n \left( \frac{\postmod{0:m} \uk{m, n - 1} \addf[n]{k}}{\postmod{0:m} \uk{m, n - 1} \1_{\Xset^n}} - \frac{\postmod{0:m - 1} \uk{m - 1, n - 1} \addf[n]{k}}{\postmod{0:m - 1} \uk{m - 1, n - 1} \1_{\Xset^n}} \right)  
\end{equation*}











\clearpage
\newpage





Define, for each $n \in \nset$ and $0\leq m \leq n$, the kernel  
\begin{equation} \label{eq:def:retrok}
    \retrok_{m, n}^\parvec(x_m', \rmd x_{0:n}) \eqdef \delta_{x_m'}(\rmd x_m) \,  
    \tstat{n}^\parvec(x_m, \rmd x_{0:m - 1})
    \prod_{\ell = m}^{n - 1} \uk{\ell}^\parvec(x_\ell, \rmd x_{\ell + 1})
\end{equation}
on $\Xset_m \times \Xfd^n$ as well as the centered version 
$$
\retroknorm_{m, n}^\parvec h (x_m) \eqdef  \retrok_{m, n}^\parvec(h - \post{0:n}^\parvec h)(x_m) 
$$
on the same space.  For all $0\leq m \leq n$ and $h \in \bmf{\Xfd^n}$, 
\begin{equation} \label{eq:retro:prospective:id}
\postmod{0:m} \uk{m, n - 1}^\parvec h = \postmod{m} \retrokmod_{m, n} h.  
\end{equation}
In addition, for each $0\leq k \leq n - 1$, let 
\begin{equation} \label{eq:def:addf}
\addf[n]{k}: x_{0:n} \mapsto \addf{k}(x_k, x_{k + 1})  
\end{equation}
denote the extension of $\addf{k}$ to $\Xset^n$.  For all $n\in\nset$, write $\post{0:n}^{\varphi}$ the joint smoothing distribution provided by the variational family. Then,
$$
\post{0:n}^\parvec h - \post{0:n}^{\varphi} h= \post{n}^\parvec \tstat{n}^\parvec h - \post{0:n}^{\varphi} h =  \post{n}^\parvec \left\{\prod_{m = 0}^{n - 1} \bkw{m}^\parvec\right\} h - q_{\varphi,n}\left\{\prod_{k=0}^{n-1}q_{\varphi,k}\right\} h\eqsp.
$$
Write 
$$
\postmod{0:n} h_n - \post{0:n}^{\varphi} h_n = \sum_{k = 0}^{n - 1} \left( \postmod{0:n} \addf[n]{k} - \post{0:n}^{\varphi} \addf[n]{k} \right), 
$$
where each term can be decomposed according to   
\begin{equation*}
\postmod{0:n} \addf[n]{k} - \post{0:n}^\varphi \addf[n]{k} = 
\sum_{m = 1}^n \left( \frac{\postmod{0:m} \uk{m, n - 1} \addf[n]{k}}{\postmod{0:m} \uk{m, n - 1} \1_{\Xset^n}} - \frac{\postmod{0:m - 1} \uk{m - 1, n - 1} \addf[n]{k}}{\postmod{0:m - 1} \uk{m - 1, n - 1} \1_{\Xset^n}} \right)  
\end{equation*}
 
\end{document}
